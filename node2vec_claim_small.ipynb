{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import gensim\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pkg_resources\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.auto import tqdm\n",
    "from mumin import MuminDataset\n",
    "import dgl\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def parallel_generate_walks(d_graph: dict, global_walk_length: int, num_walks: int, cpu_num: int,\n",
    "                            sampling_strategy: dict = None, num_walks_key: str = None, walk_length_key: str = None,\n",
    "                            neighbors_key: str = None, probabilities_key: str = None, first_travel_key: str = None,\n",
    "                            quiet: bool = False) -> list:\n",
    "    \"\"\"\n",
    "    Generates the random walks which will be used as the skip-gram input.\n",
    "\n",
    "    :return: List of walks. Each walk is a list of nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    walks = list()\n",
    "\n",
    "    if not quiet:\n",
    "        pbar = tqdm(total=num_walks, desc='Generating walks (CPU: {})'.format(cpu_num))\n",
    "\n",
    "    for n_walk in range(num_walks):\n",
    "\n",
    "        # Update progress bar\n",
    "        if not quiet:\n",
    "            pbar.update(1)\n",
    "\n",
    "        # Shuffle the nodes\n",
    "        shuffled_nodes = list(d_graph.keys())\n",
    "        random.shuffle(shuffled_nodes)\n",
    "\n",
    "        # Start a random walk from every node\n",
    "        for source in shuffled_nodes:\n",
    "\n",
    "            # Skip nodes with specific num_walks\n",
    "            if source in sampling_strategy and \\\n",
    "                    num_walks_key in sampling_strategy[source] and \\\n",
    "                    sampling_strategy[source][num_walks_key] <= n_walk:\n",
    "                continue\n",
    "\n",
    "            # Start walk\n",
    "            walk = [source]\n",
    "\n",
    "            # Calculate walk length\n",
    "            if source in sampling_strategy:\n",
    "                walk_length = sampling_strategy[source].get(walk_length_key, global_walk_length)\n",
    "            else:\n",
    "                walk_length = global_walk_length\n",
    "\n",
    "            # Perform walk\n",
    "            while len(walk) < walk_length:\n",
    "\n",
    "                walk_options = d_graph[walk[-1]].get(neighbors_key, None)\n",
    "\n",
    "                # Skip dead end nodes\n",
    "                if not walk_options:\n",
    "                    break\n",
    "\n",
    "                if len(walk) == 1:  # For the first step\n",
    "                    probabilities = d_graph[walk[-1]][first_travel_key]\n",
    "                    walk_to = random.choices(walk_options, weights=probabilities)[0]\n",
    "                else:\n",
    "                    probabilities = d_graph[walk[-1]][probabilities_key][walk[-2]]\n",
    "                    walk_to = random.choices(walk_options, weights=probabilities)[0]\n",
    "\n",
    "                walk.append(walk_to)\n",
    "\n",
    "            walk = list(map(str, walk))  # Convert all to strings\n",
    "\n",
    "            walks.append(walk)\n",
    "\n",
    "    if not quiet:\n",
    "        pbar.close()\n",
    "\n",
    "    return walks\n",
    "\n",
    "\n",
    "class Node2Vec:\n",
    "    FIRST_TRAVEL_KEY = 'first_travel_key'\n",
    "    PROBABILITIES_KEY = 'probabilities'\n",
    "    NEIGHBORS_KEY = 'neighbors'\n",
    "    WEIGHT_KEY = 'weight'\n",
    "    NUM_WALKS_KEY = 'num_walks'\n",
    "    WALK_LENGTH_KEY = 'walk_length'\n",
    "    P_KEY = 'p'\n",
    "    Q_KEY = 'q'\n",
    "\n",
    "    def __init__(self, graph: nx.Graph, dimensions: int = 128, walk_length: int = 80, num_walks: int = 10, p: float = 1,\n",
    "                 q: float = 1, weight_key: str = 'weight', workers: int = 1, sampling_strategy: dict = None,\n",
    "                 quiet: bool = False, temp_folder: str = None, seed: int = None):\n",
    "        \"\"\"\n",
    "        Initiates the Node2Vec object, precomputes walking probabilities and generates the walks.\n",
    "\n",
    "        :param graph: Input graph\n",
    "        :param dimensions: Embedding dimensions (default: 128)\n",
    "        :param walk_length: Number of nodes in each walk (default: 80)\n",
    "        :param num_walks: Number of walks per node (default: 10)\n",
    "        :param p: Return hyper parameter (default: 1)\n",
    "        :param q: Inout parameter (default: 1)\n",
    "        :param weight_key: On weighted graphs, this is the key for the weight attribute (default: 'weight')\n",
    "        :param workers: Number of workers for parallel execution (default: 1)\n",
    "        :param sampling_strategy: Node specific sampling strategies, supports setting node specific 'q', 'p', 'num_walks' and 'walk_length'.\n",
    "        :param seed: Seed for the random number generator.\n",
    "        Use these keys exactly. If not set, will use the global ones which were passed on the object initialization\n",
    "        :param temp_folder: Path to folder with enough space to hold the memory map of self.d_graph (for big graphs); to be passed joblib.Parallel.temp_folder\n",
    "        \"\"\"\n",
    "\n",
    "        self.graph = graph\n",
    "        self.dimensions = dimensions\n",
    "        self.walk_length = walk_length\n",
    "        self.num_walks = num_walks\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.weight_key = weight_key\n",
    "        self.workers = workers\n",
    "        self.quiet = quiet\n",
    "        self.d_graph = defaultdict(dict)\n",
    "\n",
    "        if sampling_strategy is None:\n",
    "            self.sampling_strategy = {}\n",
    "        else:\n",
    "            self.sampling_strategy = sampling_strategy\n",
    "\n",
    "        self.temp_folder, self.require = None, None\n",
    "        if temp_folder:\n",
    "            if not os.path.isdir(temp_folder):\n",
    "                raise NotADirectoryError(\"temp_folder does not exist or is not a directory. ({})\".format(temp_folder))\n",
    "\n",
    "            self.temp_folder = temp_folder\n",
    "            self.require = \"sharedmem\"\n",
    "\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        self._precompute_probabilities()\n",
    "        self.walks = self._generate_walks()\n",
    "\n",
    "    def _precompute_probabilities(self):\n",
    "        \"\"\"\n",
    "        Precomputes transition probabilities for each node.\n",
    "        \"\"\"\n",
    "\n",
    "        d_graph = self.d_graph\n",
    "\n",
    "        nodes_generator = self.graph.nodes() if self.quiet \\\n",
    "            else tqdm(self.graph.nodes(), desc='Computing transition probabilities')\n",
    "\n",
    "        for source in nodes_generator:\n",
    "\n",
    "            # Init probabilities dict for first travel\n",
    "            if self.PROBABILITIES_KEY not in d_graph[source]:\n",
    "                d_graph[source][self.PROBABILITIES_KEY] = dict()\n",
    "\n",
    "            for current_node in self.graph.neighbors(source):\n",
    "\n",
    "                # Init probabilities dict\n",
    "                if self.PROBABILITIES_KEY not in d_graph[current_node]:\n",
    "                    d_graph[current_node][self.PROBABILITIES_KEY] = dict()\n",
    "\n",
    "                unnormalized_weights = list()\n",
    "                d_neighbors = list()\n",
    "\n",
    "                # Calculate unnormalized weights\n",
    "                for destination in self.graph.neighbors(current_node):\n",
    "\n",
    "                    p = self.sampling_strategy[current_node].get(self.P_KEY,\n",
    "                                                                 self.p) if current_node in self.sampling_strategy else self.p\n",
    "                    q = self.sampling_strategy[current_node].get(self.Q_KEY,\n",
    "                                                                 self.q) if current_node in self.sampling_strategy else self.q\n",
    "\n",
    "                    try:\n",
    "                        if self.graph[current_node][destination].get(self.weight_key):\n",
    "                            weight = self.graph[current_node][destination].get(self.weight_key, 1)\n",
    "                        else:\n",
    "                            ## Example : AtlasView({0: {'type': 1, 'weight':0.1}})- when we have edge weight\n",
    "                            edge = list(self.graph[current_node][destination])[-1]\n",
    "                            weight = self.graph[current_node][destination][edge].get(self.weight_key, 1)\n",
    "\n",
    "                    except:\n",
    "                        weight = 1\n",
    "\n",
    "                    if destination == source:  # Backwards probability\n",
    "                        ss_weight = weight * 1 / p\n",
    "                    elif destination in self.graph[source]:  # If the neighbor is connected to the source\n",
    "                        ss_weight = weight\n",
    "                    else:\n",
    "                        ss_weight = weight * 1 / q\n",
    "\n",
    "                    # Assign the unnormalized sampling strategy weight, normalize during random walk\n",
    "                    unnormalized_weights.append(ss_weight)\n",
    "                    d_neighbors.append(destination)\n",
    "\n",
    "                # Normalize\n",
    "                unnormalized_weights = np.array(unnormalized_weights)\n",
    "                d_graph[current_node][self.PROBABILITIES_KEY][\n",
    "                    source] = unnormalized_weights / unnormalized_weights.sum()\n",
    "\n",
    "            # Calculate first_travel weights for source\n",
    "            first_travel_weights = []\n",
    "\n",
    "            for destination in self.graph.neighbors(source):\n",
    "                first_travel_weights.append(self.graph[source][destination].get(self.weight_key, 1))\n",
    "\n",
    "            first_travel_weights = np.array(first_travel_weights)\n",
    "            d_graph[source][self.FIRST_TRAVEL_KEY] = first_travel_weights / first_travel_weights.sum()\n",
    "\n",
    "            # Save neighbors\n",
    "            d_graph[source][self.NEIGHBORS_KEY] = list(self.graph.neighbors(source))\n",
    "\n",
    "    def _generate_walks(self) -> list:\n",
    "        \"\"\"\n",
    "        Generates the random walks which will be used as the skip-gram input.\n",
    "        :return: List of walks. Each walk is a list of nodes.\n",
    "        \"\"\"\n",
    "\n",
    "        flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "        # Split num_walks for each worker\n",
    "        num_walks_lists = np.array_split(range(self.num_walks), self.workers)\n",
    "\n",
    "        walk_results = Parallel(n_jobs=self.workers, temp_folder=self.temp_folder, require=self.require)(\n",
    "            delayed(parallel_generate_walks)(self.d_graph,\n",
    "                                             self.walk_length,\n",
    "                                             len(num_walks),\n",
    "                                             idx,\n",
    "                                             self.sampling_strategy,\n",
    "                                             self.NUM_WALKS_KEY,\n",
    "                                             self.WALK_LENGTH_KEY,\n",
    "                                             self.NEIGHBORS_KEY,\n",
    "                                             self.PROBABILITIES_KEY,\n",
    "                                             self.FIRST_TRAVEL_KEY,\n",
    "                                             self.quiet) for\n",
    "            idx, num_walks\n",
    "            in enumerate(num_walks_lists, 1))\n",
    "\n",
    "        walks = flatten(walk_results)\n",
    "\n",
    "        return walks\n",
    "\n",
    "    def fit(self, **skip_gram_params) -> gensim.models.Word2Vec:\n",
    "        \"\"\"\n",
    "        Creates the embeddings using gensim's Word2Vec.\n",
    "        :param skip_gram_params: Parameters for gensim.models.Word2Vec - do not supply 'size' / 'vector_size' it is\n",
    "            taken from the Node2Vec 'dimensions' parameter\n",
    "        :type skip_gram_params: dict\n",
    "        :return: A gensim word2vec model\n",
    "        \"\"\"\n",
    "\n",
    "        if 'workers' not in skip_gram_params:\n",
    "            skip_gram_params['workers'] = self.workers\n",
    "\n",
    "        # Figure out gensim version, naming of output dimensions changed from size to vector_size in v4.0.0\n",
    "        gensim_version = pkg_resources.get_distribution(\"gensim\").version\n",
    "        size = 'size' if gensim_version < '4.0.0' else 'vector_size'\n",
    "        if size not in skip_gram_params:\n",
    "            skip_gram_params[size] = self.dimensions\n",
    "\n",
    "        if 'sg' not in skip_gram_params:\n",
    "            skip_gram_params['sg'] = 1\n",
    "\n",
    "        return gensim.models.Word2Vec(self.walks, **skip_gram_params)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 21:59:17,266 [INFO] Loading dataset\n",
      "2022-09-15 21:59:23,561 [INFO] Outputting to DGL\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "data collect\n",
    "\"\"\"\n",
    "twitter_bearer_token = 'AAAAAAAAAAAAAAAAAAAAAOYLagEAAAAA1K8YrEuA8CHQDAqAdjkPsBS2Pig%3DMUmnQgjpzkkXslyJpeNytAwFQ2qgiGE0Ah0rkrjuwH9UnOYSLI'\n",
    "dataset = MuminDataset(twitter_bearer_token=twitter_bearer_token)\n",
    "dataset.compile()\n",
    "dataset.add_embeddings()\n",
    "if 'dgl_graph' not in globals():\n",
    "    dgl_graph = dataset.to_dgl()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "graph1\n",
    "\"\"\"\n",
    "rel = ('tweet', 'discusses', 'claim')\n",
    "g1 = dgl.edge_type_subgraph(dgl_graph, etypes=[rel]).to('cuda')\n",
    "num = g1.edges()[0].shape[0]\n",
    "g_src = g1.edges()[0].tolist()\n",
    "g_tgt = g1.edges()[1].tolist()\n",
    "src = []\n",
    "tgt = []\n",
    "for i in range(num):\n",
    "    for j in range(i,num):\n",
    "        if g_src[i] == g_src[j]:\n",
    "            src.append(g_tgt[i])\n",
    "            tgt.append(g_tgt[j])\n",
    "graph = nx.Graph()\n",
    "for i in range(len(src)):\n",
    "    graph.add_edge(src[i],tgt[i])\n",
    "\n",
    "tgt_node = 'claim'\n",
    "dim = g1.nodes[tgt_node].data['feat'].shape[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 2127/2127 [00:00<00:00, 29554.38it/s]\n",
      "Generating walks (CPU: 4): 100%|██████████| 25/25 [00:02<00:00,  9.33it/s]"
     ]
    }
   ],
   "source": [
    "# Precompute probabilities and generate walks\n",
    "\n",
    "node2vec = Node2Vec(graph, dimensions=dim, walk_length=20, num_walks=100, workers=4)\n",
    "\n",
    "## if d_graph is big enough to fit in the memory, pass temp_folder which has enough disk space\n",
    "# Note: It will trigger \"sharedmem\" in Parallel, which will be slow on smaller graphs\n",
    "#node2vec = Node2Vec(graph, dimensions=64, walk_length=30, num_walks=200, workers=4, temp_folder=\"/mnt/tmp_data\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 21:59:35,549 [INFO] collecting all words and their counts\n",
      "2022-09-15 21:59:35,550 [INFO] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-09-15 21:59:35,566 [INFO] PROGRESS: at sentence #10000, processed 200000 words, keeping 2127 word types\n",
      "2022-09-15 21:59:35,583 [INFO] PROGRESS: at sentence #20000, processed 400000 words, keeping 2127 word types\n",
      "2022-09-15 21:59:35,600 [INFO] PROGRESS: at sentence #30000, processed 600000 words, keeping 2127 word types\n",
      "2022-09-15 21:59:35,619 [INFO] PROGRESS: at sentence #40000, processed 800000 words, keeping 2127 word types\n",
      "2022-09-15 21:59:35,637 [INFO] PROGRESS: at sentence #50000, processed 1000000 words, keeping 2127 word types\n",
      "2022-09-15 21:59:35,656 [INFO] PROGRESS: at sentence #60000, processed 1200000 words, keeping 2127 word types\n",
      "2022-09-15 21:59:35,674 [INFO] PROGRESS: at sentence #70000, processed 1400000 words, keeping 2127 word types\n",
      "2022-09-15 21:59:35,693 [INFO] PROGRESS: at sentence #80000, processed 1600000 words, keeping 2127 word types\n",
      "2022-09-15 21:59:35,712 [INFO] PROGRESS: at sentence #90000, processed 1800000 words, keeping 2127 word types\n",
      "2022-09-15 21:59:35,730 [INFO] PROGRESS: at sentence #100000, processed 2000000 words, keeping 2127 word types\n",
      "2022-09-15 21:59:35,748 [INFO] PROGRESS: at sentence #110000, processed 2200000 words, keeping 2127 word types\n",
      "2022-09-15 21:59:35,767 [INFO] PROGRESS: at sentence #120000, processed 2400000 words, keeping 2127 word types\n",
      "2022-09-15 21:59:35,785 [INFO] PROGRESS: at sentence #130000, processed 2600000 words, keeping 2127 word types\n",
      "2022-09-15 21:59:35,803 [INFO] PROGRESS: at sentence #140000, processed 2800000 words, keeping 2127 word types\n",
      "2022-09-15 21:59:35,822 [INFO] PROGRESS: at sentence #150000, processed 3000000 words, keeping 2127 word types\n",
      "2022-09-15 21:59:35,841 [INFO] PROGRESS: at sentence #160000, processed 3200000 words, keeping 2127 word types\n",
      "2022-09-15 21:59:35,859 [INFO] PROGRESS: at sentence #170000, processed 3400000 words, keeping 2127 word types\n",
      "2022-09-15 21:59:35,877 [INFO] PROGRESS: at sentence #180000, processed 3600000 words, keeping 2127 word types\n",
      "2022-09-15 21:59:35,895 [INFO] PROGRESS: at sentence #190000, processed 3800000 words, keeping 2127 word types\n",
      "2022-09-15 21:59:35,913 [INFO] PROGRESS: at sentence #200000, processed 4000000 words, keeping 2127 word types\n",
      "2022-09-15 21:59:35,931 [INFO] PROGRESS: at sentence #210000, processed 4200000 words, keeping 2127 word types\n",
      "2022-09-15 21:59:35,936 [INFO] collected 2127 word types from a corpus of 4254000 raw words and 212700 sentences\n",
      "2022-09-15 21:59:35,937 [INFO] Creating a fresh vocabulary\n",
      "2022-09-15 21:59:35,942 [INFO] Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 2127 unique words (100.00% of original 2127, drops 0)', 'datetime': '2022-09-15T21:59:35.942575', 'gensim': '4.2.0', 'python': '3.7.13 (default, Mar 29 2022, 02:18:16) \\n[GCC 7.5.0]', 'platform': 'Linux-5.15.0-46-generic-x86_64-with-debian-bookworm-sid', 'event': 'prepare_vocab'}\n",
      "2022-09-15 21:59:35,943 [INFO] Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 4254000 word corpus (100.00% of original 4254000, drops 0)', 'datetime': '2022-09-15T21:59:35.943510', 'gensim': '4.2.0', 'python': '3.7.13 (default, Mar 29 2022, 02:18:16) \\n[GCC 7.5.0]', 'platform': 'Linux-5.15.0-46-generic-x86_64-with-debian-bookworm-sid', 'event': 'prepare_vocab'}\n",
      "2022-09-15 21:59:35,951 [INFO] deleting the raw counts dictionary of 2127 items\n",
      "2022-09-15 21:59:35,958 [INFO] sample=0.001 downsamples 0 most-common words\n",
      "2022-09-15 21:59:35,959 [INFO] Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 4254000 word corpus (100.0%% of prior 4254000)', 'datetime': '2022-09-15T21:59:35.959202', 'gensim': '4.2.0', 'python': '3.7.13 (default, Mar 29 2022, 02:18:16) \\n[GCC 7.5.0]', 'platform': 'Linux-5.15.0-46-generic-x86_64-with-debian-bookworm-sid', 'event': 'prepare_vocab'}\n",
      "2022-09-15 21:59:35,974 [INFO] estimated required memory for 2127 words and 870 dimensions: 15867420 bytes\n",
      "2022-09-15 21:59:35,974 [INFO] resetting layer weights\n",
      "2022-09-15 21:59:35,982 [INFO] Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-09-15T21:59:35.981968', 'gensim': '4.2.0', 'python': '3.7.13 (default, Mar 29 2022, 02:18:16) \\n[GCC 7.5.0]', 'platform': 'Linux-5.15.0-46-generic-x86_64-with-debian-bookworm-sid', 'event': 'build_vocab'}\n",
      "2022-09-15 21:59:35,982 [INFO] Word2Vec lifecycle event {'msg': 'training model with 4 workers on 2127 vocabulary and 870 features, using sg=1 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2022-09-15T21:59:35.982687', 'gensim': '4.2.0', 'python': '3.7.13 (default, Mar 29 2022, 02:18:16) \\n[GCC 7.5.0]', 'platform': 'Linux-5.15.0-46-generic-x86_64-with-debian-bookworm-sid', 'event': 'train'}\n",
      "2022-09-15 21:59:36,986 [INFO] EPOCH 0 - PROGRESS: at 2.72% examples, 115702 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 21:59:37,986 [INFO] EPOCH 0 - PROGRESS: at 5.65% examples, 120036 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 21:59:38,986 [INFO] EPOCH 0 - PROGRESS: at 8.77% examples, 124354 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 21:59:39,986 [INFO] EPOCH 0 - PROGRESS: at 11.30% examples, 120206 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 21:59:40,986 [INFO] EPOCH 0 - PROGRESS: at 13.38% examples, 113801 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-15 21:59:41,986 [INFO] EPOCH 0 - PROGRESS: at 16.86% examples, 119497 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 21:59:42,986 [INFO] EPOCH 0 - PROGRESS: at 20.40% examples, 123950 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 21:59:43,986 [INFO] EPOCH 0 - PROGRESS: at 22.78% examples, 121141 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 21:59:44,986 [INFO] EPOCH 0 - PROGRESS: at 26.23% examples, 123953 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 21:59:45,986 [INFO] EPOCH 0 - PROGRESS: at 28.50% examples, 121208 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 21:59:46,987 [INFO] EPOCH 0 - PROGRESS: at 32.14% examples, 124269 words/s, in_qsize 8, out_qsize 3\n",
      "2022-09-15 21:59:47,987 [INFO] EPOCH 0 - PROGRESS: at 35.60% examples, 126181 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 21:59:48,987 [INFO] EPOCH 0 - PROGRESS: at 39.34% examples, 128712 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-15 21:59:50,005 [INFO] EPOCH 0 - PROGRESS: at 42.15% examples, 127894 words/s, in_qsize 5, out_qsize 2\n",
      "2022-09-15 21:59:51,005 [INFO] EPOCH 0 - PROGRESS: at 45.05% examples, 127580 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 21:59:52,005 [INFO] EPOCH 0 - PROGRESS: at 47.63% examples, 126471 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-15 21:59:53,007 [INFO] EPOCH 0 - PROGRESS: at 51.01% examples, 127468 words/s, in_qsize 0, out_qsize 11\n",
      "2022-09-15 21:59:54,007 [INFO] EPOCH 0 - PROGRESS: at 53.39% examples, 126013 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-15 21:59:55,007 [INFO] EPOCH 0 - PROGRESS: at 56.97% examples, 127398 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 21:59:56,008 [INFO] EPOCH 0 - PROGRESS: at 60.65% examples, 128866 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 21:59:57,008 [INFO] EPOCH 0 - PROGRESS: at 64.18% examples, 129876 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 21:59:58,008 [INFO] EPOCH 0 - PROGRESS: at 67.82% examples, 131008 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 21:59:59,008 [INFO] EPOCH 0 - PROGRESS: at 70.49% examples, 130255 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-15 22:00:00,012 [INFO] EPOCH 0 - PROGRESS: at 73.82% examples, 130695 words/s, in_qsize 0, out_qsize 9\n",
      "2022-09-15 22:00:01,012 [INFO] EPOCH 0 - PROGRESS: at 76.52% examples, 130074 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:02,012 [INFO] EPOCH 0 - PROGRESS: at 78.88% examples, 128934 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:03,012 [INFO] EPOCH 0 - PROGRESS: at 82.43% examples, 129742 words/s, in_qsize 7, out_qsize 1\n",
      "2022-09-15 22:00:04,012 [INFO] EPOCH 0 - PROGRESS: at 85.25% examples, 129391 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:05,012 [INFO] EPOCH 0 - PROGRESS: at 88.70% examples, 129991 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:06,012 [INFO] EPOCH 0 - PROGRESS: at 92.20% examples, 130628 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:07,012 [INFO] EPOCH 0 - PROGRESS: at 94.92% examples, 130136 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:08,012 [INFO] EPOCH 0 - PROGRESS: at 98.55% examples, 130904 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:08,404 [INFO] EPOCH 0: training on 4254000 raw words (4254000 effective words) took 32.4s, 131220 effective words/s\n",
      "2022-09-15 22:00:09,406 [INFO] EPOCH 1 - PROGRESS: at 3.49% examples, 148346 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:10,406 [INFO] EPOCH 1 - PROGRESS: at 6.91% examples, 146973 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:11,406 [INFO] EPOCH 1 - PROGRESS: at 10.32% examples, 146372 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:12,406 [INFO] EPOCH 1 - PROGRESS: at 13.79% examples, 146659 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:13,406 [INFO] EPOCH 1 - PROGRESS: at 17.25% examples, 146790 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:14,406 [INFO] EPOCH 1 - PROGRESS: at 20.52% examples, 145453 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:15,406 [INFO] EPOCH 1 - PROGRESS: at 24.08% examples, 146301 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:16,406 [INFO] EPOCH 1 - PROGRESS: at 27.69% examples, 147237 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:17,407 [INFO] EPOCH 1 - PROGRESS: at 31.12% examples, 147099 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:18,407 [INFO] EPOCH 1 - PROGRESS: at 34.63% examples, 147296 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:19,407 [INFO] EPOCH 1 - PROGRESS: at 37.81% examples, 146194 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:20,408 [INFO] EPOCH 1 - PROGRESS: at 41.01% examples, 145342 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:21,408 [INFO] EPOCH 1 - PROGRESS: at 43.61% examples, 142681 words/s, in_qsize 5, out_qsize 2\n",
      "2022-09-15 22:00:22,408 [INFO] EPOCH 1 - PROGRESS: at 47.26% examples, 143587 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:23,408 [INFO] EPOCH 1 - PROGRESS: at 50.82% examples, 144101 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:24,408 [INFO] EPOCH 1 - PROGRESS: at 54.40% examples, 144611 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:25,408 [INFO] EPOCH 1 - PROGRESS: at 57.88% examples, 144821 words/s, in_qsize 6, out_qsize 0\n",
      "2022-09-15 22:00:26,408 [INFO] EPOCH 1 - PROGRESS: at 60.23% examples, 142311 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:00:27,408 [INFO] EPOCH 1 - PROGRESS: at 63.93% examples, 143123 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:28,409 [INFO] EPOCH 1 - PROGRESS: at 67.46% examples, 143467 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:29,409 [INFO] EPOCH 1 - PROGRESS: at 70.20% examples, 142187 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:30,409 [INFO] EPOCH 1 - PROGRESS: at 73.89% examples, 142858 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:31,409 [INFO] EPOCH 1 - PROGRESS: at 77.65% examples, 143602 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:00:32,409 [INFO] EPOCH 1 - PROGRESS: at 80.52% examples, 142696 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:00:33,409 [INFO] EPOCH 1 - PROGRESS: at 83.75% examples, 142489 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-15 22:00:34,409 [INFO] EPOCH 1 - PROGRESS: at 87.03% examples, 142379 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-15 22:00:35,410 [INFO] EPOCH 1 - PROGRESS: at 90.66% examples, 142820 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:36,410 [INFO] EPOCH 1 - PROGRESS: at 93.39% examples, 141873 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:37,410 [INFO] EPOCH 1 - PROGRESS: at 97.07% examples, 142366 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:38,233 [INFO] EPOCH 1: training on 4254000 raw words (4254000 effective words) took 29.8s, 142620 effective words/s\n",
      "2022-09-15 22:00:39,235 [INFO] EPOCH 2 - PROGRESS: at 3.67% examples, 156033 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:40,235 [INFO] EPOCH 2 - PROGRESS: at 7.31% examples, 155533 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:41,235 [INFO] EPOCH 2 - PROGRESS: at 10.73% examples, 152173 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:42,236 [INFO] EPOCH 2 - PROGRESS: at 14.21% examples, 151155 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-15 22:00:43,236 [INFO] EPOCH 2 - PROGRESS: at 17.48% examples, 148674 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:44,236 [INFO] EPOCH 2 - PROGRESS: at 21.01% examples, 148926 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:45,236 [INFO] EPOCH 2 - PROGRESS: at 24.58% examples, 149344 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:46,236 [INFO] EPOCH 2 - PROGRESS: at 28.13% examples, 149571 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:00:47,236 [INFO] EPOCH 2 - PROGRESS: at 31.66% examples, 149620 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:00:48,236 [INFO] EPOCH 2 - PROGRESS: at 35.20% examples, 149715 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:49,236 [INFO] EPOCH 2 - PROGRESS: at 37.88% examples, 146465 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:50,236 [INFO] EPOCH 2 - PROGRESS: at 41.37% examples, 146629 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:51,236 [INFO] EPOCH 2 - PROGRESS: at 44.99% examples, 147206 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:52,237 [INFO] EPOCH 2 - PROGRESS: at 48.68% examples, 147890 words/s, in_qsize 7, out_qsize 1\n",
      "2022-09-15 22:00:53,237 [INFO] EPOCH 2 - PROGRESS: at 52.36% examples, 148466 words/s, in_qsize 7, out_qsize 2\n",
      "2022-09-15 22:00:54,237 [INFO] EPOCH 2 - PROGRESS: at 55.18% examples, 146701 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:55,237 [INFO] EPOCH 2 - PROGRESS: at 58.52% examples, 146412 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:56,237 [INFO] EPOCH 2 - PROGRESS: at 61.99% examples, 146495 words/s, in_qsize 7, out_qsize 2\n",
      "2022-09-15 22:00:57,242 [INFO] EPOCH 2 - PROGRESS: at 64.66% examples, 144716 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:58,242 [INFO] EPOCH 2 - PROGRESS: at 67.71% examples, 143961 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:00:59,243 [INFO] EPOCH 2 - PROGRESS: at 71.28% examples, 144345 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:00,243 [INFO] EPOCH 2 - PROGRESS: at 74.76% examples, 144518 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-15 22:01:01,243 [INFO] EPOCH 2 - PROGRESS: at 78.18% examples, 144560 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:02,243 [INFO] EPOCH 2 - PROGRESS: at 81.67% examples, 144717 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:03,243 [INFO] EPOCH 2 - PROGRESS: at 85.20% examples, 144931 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:04,243 [INFO] EPOCH 2 - PROGRESS: at 88.68% examples, 145048 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:05,243 [INFO] EPOCH 2 - PROGRESS: at 92.22% examples, 145255 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:01:06,243 [INFO] EPOCH 2 - PROGRESS: at 95.85% examples, 145590 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:07,243 [INFO] EPOCH 2 - PROGRESS: at 99.48% examples, 145890 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:07,386 [INFO] EPOCH 2: training on 4254000 raw words (4254000 effective words) took 29.2s, 145930 effective words/s\n",
      "2022-09-15 22:01:08,388 [INFO] EPOCH 3 - PROGRESS: at 2.40% examples, 102027 words/s, in_qsize 7, out_qsize 1\n",
      "2022-09-15 22:01:09,388 [INFO] EPOCH 3 - PROGRESS: at 5.87% examples, 124820 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:10,388 [INFO] EPOCH 3 - PROGRESS: at 9.30% examples, 131911 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:11,389 [INFO] EPOCH 3 - PROGRESS: at 12.18% examples, 129491 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:12,389 [INFO] EPOCH 3 - PROGRESS: at 15.35% examples, 130551 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:13,389 [INFO] EPOCH 3 - PROGRESS: at 18.82% examples, 133439 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:14,389 [INFO] EPOCH 3 - PROGRESS: at 22.26% examples, 135242 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:15,389 [INFO] EPOCH 3 - PROGRESS: at 25.70% examples, 136635 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:16,389 [INFO] EPOCH 3 - PROGRESS: at 29.19% examples, 137967 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:17,389 [INFO] EPOCH 3 - PROGRESS: at 32.71% examples, 139127 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:18,389 [INFO] EPOCH 3 - PROGRESS: at 36.09% examples, 139569 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:01:19,389 [INFO] EPOCH 3 - PROGRESS: at 39.66% examples, 140585 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:20,390 [INFO] EPOCH 3 - PROGRESS: at 43.08% examples, 140952 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:21,390 [INFO] EPOCH 3 - PROGRESS: at 46.49% examples, 141233 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:01:22,390 [INFO] EPOCH 3 - PROGRESS: at 49.95% examples, 141630 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:23,390 [INFO] EPOCH 3 - PROGRESS: at 53.38% examples, 141914 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:24,390 [INFO] EPOCH 3 - PROGRESS: at 56.85% examples, 142244 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:25,390 [INFO] EPOCH 3 - PROGRESS: at 60.39% examples, 142694 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:26,390 [INFO] EPOCH 3 - PROGRESS: at 63.78% examples, 142774 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:27,390 [INFO] EPOCH 3 - PROGRESS: at 67.17% examples, 142864 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:28,390 [INFO] EPOCH 3 - PROGRESS: at 70.65% examples, 143104 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-15 22:01:29,390 [INFO] EPOCH 3 - PROGRESS: at 74.13% examples, 143324 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:01:30,391 [INFO] EPOCH 3 - PROGRESS: at 77.63% examples, 143560 words/s, in_qsize 7, out_qsize 1\n",
      "2022-09-15 22:01:31,391 [INFO] EPOCH 3 - PROGRESS: at 81.03% examples, 143607 words/s, in_qsize 5, out_qsize 1\n",
      "2022-09-15 22:01:32,391 [INFO] EPOCH 3 - PROGRESS: at 84.45% examples, 143693 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:33,391 [INFO] EPOCH 3 - PROGRESS: at 87.87% examples, 143758 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:34,391 [INFO] EPOCH 3 - PROGRESS: at 91.29% examples, 143814 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:35,391 [INFO] EPOCH 3 - PROGRESS: at 94.78% examples, 143979 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:36,391 [INFO] EPOCH 3 - PROGRESS: at 98.24% examples, 144091 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-15 22:01:36,895 [INFO] EPOCH 3: training on 4254000 raw words (4254000 effective words) took 29.5s, 144170 effective words/s\n",
      "2022-09-15 22:01:37,897 [INFO] EPOCH 4 - PROGRESS: at 3.51% examples, 149262 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:38,897 [INFO] EPOCH 4 - PROGRESS: at 7.14% examples, 151767 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:39,897 [INFO] EPOCH 4 - PROGRESS: at 10.72% examples, 152054 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-15 22:01:40,897 [INFO] EPOCH 4 - PROGRESS: at 14.41% examples, 153223 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:41,897 [INFO] EPOCH 4 - PROGRESS: at 18.08% examples, 153788 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:42,898 [INFO] EPOCH 4 - PROGRESS: at 21.22% examples, 150402 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-15 22:01:43,898 [INFO] EPOCH 4 - PROGRESS: at 24.64% examples, 149734 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:01:44,898 [INFO] EPOCH 4 - PROGRESS: at 28.25% examples, 150213 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:45,898 [INFO] EPOCH 4 - PROGRESS: at 31.57% examples, 149182 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:46,898 [INFO] EPOCH 4 - PROGRESS: at 33.74% examples, 143500 words/s, in_qsize 5, out_qsize 0\n",
      "2022-09-15 22:01:47,898 [INFO] EPOCH 4 - PROGRESS: at 35.81% examples, 138470 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:48,898 [INFO] EPOCH 4 - PROGRESS: at 37.91% examples, 134365 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:49,898 [INFO] EPOCH 4 - PROGRESS: at 39.94% examples, 130674 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:01:50,898 [INFO] EPOCH 4 - PROGRESS: at 42.63% examples, 129513 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:51,898 [INFO] EPOCH 4 - PROGRESS: at 46.23% examples, 131087 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-15 22:01:52,899 [INFO] EPOCH 4 - PROGRESS: at 49.76% examples, 132278 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:01:53,899 [INFO] EPOCH 4 - PROGRESS: at 53.36% examples, 133501 words/s, in_qsize 7, out_qsize 1\n",
      "2022-09-15 22:01:54,899 [INFO] EPOCH 4 - PROGRESS: at 56.96% examples, 134600 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:55,899 [INFO] EPOCH 4 - PROGRESS: at 60.52% examples, 135478 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:56,899 [INFO] EPOCH 4 - PROGRESS: at 64.09% examples, 136315 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:57,899 [INFO] EPOCH 4 - PROGRESS: at 67.56% examples, 136842 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:01:58,899 [INFO] EPOCH 4 - PROGRESS: at 71.10% examples, 137471 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:01:59,899 [INFO] EPOCH 4 - PROGRESS: at 74.71% examples, 138175 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:02:00,899 [INFO] EPOCH 4 - PROGRESS: at 78.33% examples, 138821 words/s, in_qsize 8, out_qsize 2\n",
      "2022-09-15 22:02:01,899 [INFO] EPOCH 4 - PROGRESS: at 81.80% examples, 139171 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:02:02,899 [INFO] EPOCH 4 - PROGRESS: at 85.42% examples, 139740 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:02:03,900 [INFO] EPOCH 4 - PROGRESS: at 88.84% examples, 139957 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-15 22:02:04,900 [INFO] EPOCH 4 - PROGRESS: at 92.39% examples, 140349 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:02:05,900 [INFO] EPOCH 4 - PROGRESS: at 95.96% examples, 140754 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:02:06,900 [INFO] EPOCH 4 - PROGRESS: at 99.56% examples, 141166 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:02:07,028 [INFO] EPOCH 4: training on 4254000 raw words (4254000 effective words) took 30.1s, 141184 effective words/s\n",
      "2022-09-15 22:02:07,028 [INFO] Word2Vec lifecycle event {'msg': 'training on 21270000 raw words (21270000 effective words) took 151.0s, 140818 effective words/s', 'datetime': '2022-09-15T22:02:07.028951', 'gensim': '4.2.0', 'python': '3.7.13 (default, Mar 29 2022, 02:18:16) \\n[GCC 7.5.0]', 'platform': 'Linux-5.15.0-46-generic-x86_64-with-debian-bookworm-sid', 'event': 'train'}\n",
      "2022-09-15 22:02:07,029 [INFO] Word2Vec lifecycle event {'params': 'Word2Vec<vocab=2127, vector_size=870, alpha=0.025>', 'datetime': '2022-09-15T22:02:07.029414', 'gensim': '4.2.0', 'python': '3.7.13 (default, Mar 29 2022, 02:18:16) \\n[GCC 7.5.0]', 'platform': 'Linux-5.15.0-46-generic-x86_64-with-debian-bookworm-sid', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "# Embed\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)  # Any keywords acceptable by gensim.Word2Vec can be passed, `dimensions` and `workers` are automatically passed (from the Node2Vec constructor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "nodes = list(graph.nodes())\n",
    "emb = []\n",
    "for i in nodes:\n",
    "    emb.append(model.wv[str(i)])\n",
    "\n",
    "import pandas as pd\n",
    "dataframe = pd.DataFrame(nodes,columns=['node'])\n",
    "dataframe['feats'] = emb\n",
    "\n",
    "sub_g = g1\n",
    "feats = sub_g.nodes[tgt_node].data['feat'].cpu().detach().numpy()\n",
    "text_feats= []\n",
    "for i in range(feats.shape[0]):\n",
    "    text_feats.append(feats[i,:])\n",
    "\n",
    "label = sub_g.nodes[tgt_node].data['label'].cpu().detach().numpy()\n",
    "train_mask = sub_g.nodes[tgt_node].data['train_mask'].cpu().detach().numpy()\n",
    "val_mask = sub_g.nodes[tgt_node].data['val_mask'].cpu().detach().numpy()\n",
    "test_mask = sub_g.nodes[tgt_node].data['test_mask'].cpu().detach().numpy()\n",
    "\n",
    "import pandas as pd\n",
    "dataframe['label'] = label.tolist()\n",
    "dataframe['train_mask'] = train_mask.tolist()\n",
    "dataframe['val_mask'] = val_mask.tolist()\n",
    "dataframe['test_mask'] = test_mask.tolist()\n",
    "dataframe['text_feats'] = text_feats\n",
    "\n",
    "dc = nx.degree_centrality(graph)\n",
    "bc = nx.betweenness_centrality(graph)\n",
    "dc_list = []\n",
    "for key in sorted(dc.keys()):\n",
    "    dc_list.append(dc[key])\n",
    "\n",
    "bc_list = []\n",
    "for key in sorted(bc.keys()):\n",
    "    bc_list.append(bc[key])\n",
    "\n",
    "dc_feats = []\n",
    "for i in range(len(dataframe)):\n",
    "    dc_feats.append((dc_list[i]*dataframe['feats'][i]+(1-dc_list[i])*dataframe['text_feats'][i]))\n",
    "\n",
    "bc_feats = []\n",
    "for i in range(len(dataframe)):\n",
    "    bc_feats.append((bc_list[i]*dataframe['feats'][i]+(1-bc_list[i])*dataframe['text_feats'][i]))\n",
    "\n",
    "dataframe['dc_feats'] = dc_feats\n",
    "dataframe['bc_feats'] = bc_feats\n",
    "\n",
    "mix_feats = []\n",
    "for i in range(len(dataframe)):\n",
    "    mix_feats.append(np.hstack((dataframe['feats'][i],dataframe['text_feats'][i])))\n",
    "dataframe['mix_feats'] = mix_feats\n",
    "\n",
    "# Split up the data\n",
    "train = dataframe.query('train_mask == True')\n",
    "val = dataframe.query('val_mask == True')\n",
    "test = dataframe.query('test_mask == True')\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "val.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def getFeatsMatrix(df):\n",
    "    m_temp = np.zeros([len(df['feats'].values),dim])\n",
    "    for i in range(len(df['feats'].values)):\n",
    "        m_temp[i,:] = df['feats'][i]\n",
    "    return m_temp\n",
    "\n",
    "def getTextFeatsMatrix(df):\n",
    "    m_temp = np.zeros([len(df['text_feats'].values),dim])\n",
    "    for i in range(len(df['text_feats'].values)):\n",
    "        m_temp[i,:] = df['text_feats'][i]\n",
    "    return m_temp\n",
    "\n",
    "def getMixFeatsMatrix(df):\n",
    "    m_temp = np.zeros([len(df['mix_feats'].values),2*dim])\n",
    "    for i in range(len(df['mix_feats'].values)):\n",
    "        m_temp[i,:] = df['mix_feats'][i]\n",
    "    return m_temp\n",
    "\n",
    "def getDcFeatsMatrix(df):\n",
    "    m_temp = np.zeros([len(df['dc_feats'].values),dim])\n",
    "    for i in range(len(df['dc_feats'].values)):\n",
    "        m_temp[i,:] = df['dc_feats'][i]\n",
    "    return m_temp\n",
    "\n",
    "def getBcFeatsMatrix(df):\n",
    "    m_temp = np.zeros([len(df['bc_feats'].values),dim])\n",
    "    for i in range(len(df['bc_feats'].values)):\n",
    "        m_temp[i,:] = df['bc_feats'][i]\n",
    "    return m_temp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "def convert_to_data_loader(dataset,m, num_classes):\n",
    "    # convert from list to tensor\n",
    "    input_tensor = torch.from_numpy(m).float()\n",
    "    label_tensor = torch.from_numpy(np.array(dataset['label'])).long()\n",
    "    tensor_dataset = TensorDataset(input_tensor, label_tensor)\n",
    "    loader = DataLoader(tensor_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    return loader\n",
    "\n",
    "num_classes = 2   # number of possible labels in the sentiment analysis task\n",
    "\n",
    "\n",
    "# train_loader = convert_to_data_loader(train,getMixFeatsMatrix(train), num_classes)\n",
    "# dev_loader = convert_to_data_loader(val, getMixFeatsMatrix(val),num_classes)\n",
    "# test_loader = convert_to_data_loader(test,getMixFeatsMatrix(test), num_classes)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 2)\n",
    "\n",
    "\n",
    "        # 构造Dropout方法，在每次训练过程中都随机“掐死”百分之二十的神经元，防止过拟合。\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 确保输入的tensor是展开的单列数据，把每张图片的通道、长度、宽度三个维度都压缩为一列\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # 在训练过程中对隐含层神经元的正向推断使用Dropout方法\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "\n",
    "        # 在输出单元不需要使用Dropout方法\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def train_nn():\n",
    "    # 对上面定义的Classifier类进行实例化\n",
    "    model = Classifier()\n",
    "\n",
    "    # 定义损失函数为负对数损失函数\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    # 优化方法为Adam梯度下降方法，学习率为0.003\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "    # 对训练集的全部数据学习15遍，这个数字越大，训练时间越长\n",
    "    epochs = 20\n",
    "\n",
    "    # 将每次训练的训练误差和测试误差存储在这两个列表里，后面绘制误差变化折线图用\n",
    "    train_losses, test_losses = [], []\n",
    "\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "\n",
    "        # 对训练集中的所有图片都过一遍\n",
    "        for images, labels in train_loader:\n",
    "            # 将优化器中的求导结果都设为0，否则会在每次反向传播之后叠加之前的\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 对64张图片进行推断，计算损失函数，反向传播优化权重，将损失求和\n",
    "            log_ps = model(images)\n",
    "            loss = criterion(log_ps, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # 每次学完一遍数据集，都进行以下测试操作\n",
    "        else:\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            # 测试的时候不需要开自动求导和反向传播\n",
    "            with torch.no_grad():\n",
    "                # 关闭Dropout\n",
    "                model.eval()\n",
    "\n",
    "                # 对测试集中的所有图片都过一遍\n",
    "                for images, labels in dev_loader:\n",
    "                    # 对传入的测试集图片进行正向推断、计算损失，accuracy为测试集一万张图片中模型预测正确率\n",
    "                    log_ps = model(images)\n",
    "                    test_loss += criterion(log_ps, labels)\n",
    "                    ps = torch.exp(log_ps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "\n",
    "                    # 等号右边为每一批64张测试图片中预测正确的占比\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "            # 恢复Dropout\n",
    "            model.train()\n",
    "            # 将训练误差和测试误差存在两个列表里，后面绘制误差变化折线图用\n",
    "            train_losses.append(running_loss/len(train_loader))\n",
    "            test_losses.append(test_loss/len(dev_loader))\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def predict_nn(trained_model, test_loader):\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0  # count the number of correct classification labels\n",
    "\n",
    "    gold_labs = []  # gold labels to return\n",
    "    pred_labs = []  # predicted labels to return\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "        test_output = trained_model(inputs)\n",
    "        predicted_labels = test_output.argmax(1)\n",
    "\n",
    "        gold_labs.extend(labels.tolist())\n",
    "        pred_labs.extend(predicted_labels.tolist())\n",
    "\n",
    "\n",
    "    f1 = f1_score(gold_labs, pred_labs, average='macro')\n",
    "\n",
    "    return f1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (macro average) = 0.5973502304147466\n"
     ]
    }
   ],
   "source": [
    "train_loader = convert_to_data_loader(train,getBcFeatsMatrix(train), num_classes)\n",
    "dev_loader = convert_to_data_loader(val, getBcFeatsMatrix(val),num_classes)\n",
    "test_loader = convert_to_data_loader(test,getBcFeatsMatrix(test), num_classes)\n",
    "score = []\n",
    "\n",
    "model = train_nn()\n",
    "score=model,test_loader))=\n",
    "\n",
    "print(f'F1 score (macro average) = {score}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (macro average) = 0.5984580498866213\n"
     ]
    }
   ],
   "source": [
    "train_loader = convert_to_data_loader(train,getDcFeatsMatrix(train), num_classes)\n",
    "dev_loader = convert_to_data_loader(val, getDcFeatsMatrix(val),num_classes)\n",
    "test_loader = convert_to_data_loader(test,getDcFeatsMatrix(test), num_classes)\n",
    "\n",
    "score = []\n",
    "\n",
    "model = train_nn()\n",
    "score=model,test_loader))=\n",
    "\n",
    "print(f'F1 score (macro average) = {score}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (macro average) = 0.5984580498866213\n"
     ]
    }
   ],
   "source": [
    "train_loader = convert_to_data_loader(train,getFeatsMatrix(train), num_classes)\n",
    "dev_loader = convert_to_data_loader(val, getFeatsMatrix(val),num_classes)\n",
    "test_loader = convert_to_data_loader(test,getFeatsMatrix(test), num_classes)\n",
    "\n",
    "model = train_nn()\n",
    "score=model,test_loader))=\n",
    "\n",
    "print(f'F1 score (macro average) = {score}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (macro average) = 0.6741258741258741\n"
     ]
    }
   ],
   "source": [
    "train_loader = convert_to_data_loader(train,getMixFeatsMatrix(train), num_classes)\n",
    "dev_loader = convert_to_data_loader(val, getMixFeatsMatrix(val),num_classes)\n",
    "test_loader = convert_to_data_loader(test,getMixFeatsMatrix(test), num_classes)\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2*dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 2)\n",
    "\n",
    "\n",
    "        # 构造Dropout方法，在每次训练过程中都随机“掐死”百分之二十的神经元，防止过拟合。\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 确保输入的tensor是展开的单列数据，把每张图片的通道、长度、宽度三个维度都压缩为一列\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # 在训练过程中对隐含层神经元的正向推断使用Dropout方法\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "\n",
    "        # 在输出单元不需要使用Dropout方法\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "model = train_nn()\n",
    "score=model,test_loader))=\n",
    "\n",
    "print(f'F1 score (macro average) = {score}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}