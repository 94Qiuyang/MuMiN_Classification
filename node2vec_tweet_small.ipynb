{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import gensim\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pkg_resources\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.auto import tqdm\n",
    "from mumin import MuminDataset\n",
    "import dgl\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def parallel_generate_walks(d_graph: dict, global_walk_length: int, num_walks: int, cpu_num: int,\n",
    "                            sampling_strategy: dict = None, num_walks_key: str = None, walk_length_key: str = None,\n",
    "                            neighbors_key: str = None, probabilities_key: str = None, first_travel_key: str = None,\n",
    "                            quiet: bool = False) -> list:\n",
    "    \"\"\"\n",
    "    Generates the random walks which will be used as the skip-gram input.\n",
    "\n",
    "    :return: List of walks. Each walk is a list of nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    walks = list()\n",
    "\n",
    "    if not quiet:\n",
    "        pbar = tqdm(total=num_walks, desc='Generating walks (CPU: {})'.format(cpu_num))\n",
    "\n",
    "    for n_walk in range(num_walks):\n",
    "\n",
    "        # Update progress bar\n",
    "        if not quiet:\n",
    "            pbar.update(1)\n",
    "\n",
    "        # Shuffle the nodes\n",
    "        shuffled_nodes = list(d_graph.keys())\n",
    "        random.shuffle(shuffled_nodes)\n",
    "\n",
    "        # Start a random walk from every node\n",
    "        for source in shuffled_nodes:\n",
    "\n",
    "            # Skip nodes with specific num_walks\n",
    "            if source in sampling_strategy and \\\n",
    "                    num_walks_key in sampling_strategy[source] and \\\n",
    "                    sampling_strategy[source][num_walks_key] <= n_walk:\n",
    "                continue\n",
    "\n",
    "            # Start walk\n",
    "            walk = [source]\n",
    "\n",
    "            # Calculate walk length\n",
    "            if source in sampling_strategy:\n",
    "                walk_length = sampling_strategy[source].get(walk_length_key, global_walk_length)\n",
    "            else:\n",
    "                walk_length = global_walk_length\n",
    "\n",
    "            # Perform walk\n",
    "            while len(walk) < walk_length:\n",
    "\n",
    "                walk_options = d_graph[walk[-1]].get(neighbors_key, None)\n",
    "\n",
    "                # Skip dead end nodes\n",
    "                if not walk_options:\n",
    "                    break\n",
    "\n",
    "                if len(walk) == 1:  # For the first step\n",
    "                    probabilities = d_graph[walk[-1]][first_travel_key]\n",
    "                    walk_to = random.choices(walk_options, weights=probabilities)[0]\n",
    "                else:\n",
    "                    probabilities = d_graph[walk[-1]][probabilities_key][walk[-2]]\n",
    "                    walk_to = random.choices(walk_options, weights=probabilities)[0]\n",
    "\n",
    "                walk.append(walk_to)\n",
    "\n",
    "            walk = list(map(str, walk))  # Convert all to strings\n",
    "\n",
    "            walks.append(walk)\n",
    "\n",
    "    if not quiet:\n",
    "        pbar.close()\n",
    "\n",
    "    return walks\n",
    "\n",
    "\n",
    "class Node2Vec:\n",
    "    FIRST_TRAVEL_KEY = 'first_travel_key'\n",
    "    PROBABILITIES_KEY = 'probabilities'\n",
    "    NEIGHBORS_KEY = 'neighbors'\n",
    "    WEIGHT_KEY = 'weight'\n",
    "    NUM_WALKS_KEY = 'num_walks'\n",
    "    WALK_LENGTH_KEY = 'walk_length'\n",
    "    P_KEY = 'p'\n",
    "    Q_KEY = 'q'\n",
    "\n",
    "    def __init__(self, graph: nx.Graph, dimensions: int = 128, walk_length: int = 80, num_walks: int = 10, p: float = 1,\n",
    "                 q: float = 1, weight_key: str = 'weight', workers: int = 1, sampling_strategy: dict = None,\n",
    "                 quiet: bool = False, temp_folder: str = None, seed: int = None):\n",
    "        \"\"\"\n",
    "        Initiates the Node2Vec object, precomputes walking probabilities and generates the walks.\n",
    "\n",
    "        :param graph: Input graph\n",
    "        :param dimensions: Embedding dimensions (default: 128)\n",
    "        :param walk_length: Number of nodes in each walk (default: 80)\n",
    "        :param num_walks: Number of walks per node (default: 10)\n",
    "        :param p: Return hyper parameter (default: 1)\n",
    "        :param q: Inout parameter (default: 1)\n",
    "        :param weight_key: On weighted graphs, this is the key for the weight attribute (default: 'weight')\n",
    "        :param workers: Number of workers for parallel execution (default: 1)\n",
    "        :param sampling_strategy: Node specific sampling strategies, supports setting node specific 'q', 'p', 'num_walks' and 'walk_length'.\n",
    "        :param seed: Seed for the random number generator.\n",
    "        Use these keys exactly. If not set, will use the global ones which were passed on the object initialization\n",
    "        :param temp_folder: Path to folder with enough space to hold the memory map of self.d_graph (for big graphs); to be passed joblib.Parallel.temp_folder\n",
    "        \"\"\"\n",
    "\n",
    "        self.graph = graph\n",
    "        self.dimensions = dimensions\n",
    "        self.walk_length = walk_length\n",
    "        self.num_walks = num_walks\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.weight_key = weight_key\n",
    "        self.workers = workers\n",
    "        self.quiet = quiet\n",
    "        self.d_graph = defaultdict(dict)\n",
    "\n",
    "        if sampling_strategy is None:\n",
    "            self.sampling_strategy = {}\n",
    "        else:\n",
    "            self.sampling_strategy = sampling_strategy\n",
    "\n",
    "        self.temp_folder, self.require = None, None\n",
    "        if temp_folder:\n",
    "            if not os.path.isdir(temp_folder):\n",
    "                raise NotADirectoryError(\"temp_folder does not exist or is not a directory. ({})\".format(temp_folder))\n",
    "\n",
    "            self.temp_folder = temp_folder\n",
    "            self.require = \"sharedmem\"\n",
    "\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        self._precompute_probabilities()\n",
    "        self.walks = self._generate_walks()\n",
    "\n",
    "    def _precompute_probabilities(self):\n",
    "        \"\"\"\n",
    "        Precomputes transition probabilities for each node.\n",
    "        \"\"\"\n",
    "\n",
    "        d_graph = self.d_graph\n",
    "\n",
    "        nodes_generator = self.graph.nodes() if self.quiet \\\n",
    "            else tqdm(self.graph.nodes(), desc='Computing transition probabilities')\n",
    "\n",
    "        for source in nodes_generator:\n",
    "\n",
    "            # Init probabilities dict for first travel\n",
    "            if self.PROBABILITIES_KEY not in d_graph[source]:\n",
    "                d_graph[source][self.PROBABILITIES_KEY] = dict()\n",
    "\n",
    "            for current_node in self.graph.neighbors(source):\n",
    "\n",
    "                # Init probabilities dict\n",
    "                if self.PROBABILITIES_KEY not in d_graph[current_node]:\n",
    "                    d_graph[current_node][self.PROBABILITIES_KEY] = dict()\n",
    "\n",
    "                unnormalized_weights = list()\n",
    "                d_neighbors = list()\n",
    "\n",
    "                # Calculate unnormalized weights\n",
    "                for destination in self.graph.neighbors(current_node):\n",
    "\n",
    "                    p = self.sampling_strategy[current_node].get(self.P_KEY,\n",
    "                                                                 self.p) if current_node in self.sampling_strategy else self.p\n",
    "                    q = self.sampling_strategy[current_node].get(self.Q_KEY,\n",
    "                                                                 self.q) if current_node in self.sampling_strategy else self.q\n",
    "\n",
    "                    try:\n",
    "                        if self.graph[current_node][destination].get(self.weight_key):\n",
    "                            weight = self.graph[current_node][destination].get(self.weight_key, 1)\n",
    "                        else:\n",
    "                            ## Example : AtlasView({0: {'type': 1, 'weight':0.1}})- when we have edge weight\n",
    "                            edge = list(self.graph[current_node][destination])[-1]\n",
    "                            weight = self.graph[current_node][destination][edge].get(self.weight_key, 1)\n",
    "\n",
    "                    except:\n",
    "                        weight = 1\n",
    "\n",
    "                    if destination == source:  # Backwards probability\n",
    "                        ss_weight = weight * 1 / p\n",
    "                    elif destination in self.graph[source]:  # If the neighbor is connected to the source\n",
    "                        ss_weight = weight\n",
    "                    else:\n",
    "                        ss_weight = weight * 1 / q\n",
    "\n",
    "                    # Assign the unnormalized sampling strategy weight, normalize during random walk\n",
    "                    unnormalized_weights.append(ss_weight)\n",
    "                    d_neighbors.append(destination)\n",
    "\n",
    "                # Normalize\n",
    "                unnormalized_weights = np.array(unnormalized_weights)\n",
    "                d_graph[current_node][self.PROBABILITIES_KEY][\n",
    "                    source] = unnormalized_weights / unnormalized_weights.sum()\n",
    "\n",
    "            # Calculate first_travel weights for source\n",
    "            first_travel_weights = []\n",
    "\n",
    "            for destination in self.graph.neighbors(source):\n",
    "                first_travel_weights.append(self.graph[source][destination].get(self.weight_key, 1))\n",
    "\n",
    "            first_travel_weights = np.array(first_travel_weights)\n",
    "            d_graph[source][self.FIRST_TRAVEL_KEY] = first_travel_weights / first_travel_weights.sum()\n",
    "\n",
    "            # Save neighbors\n",
    "            d_graph[source][self.NEIGHBORS_KEY] = list(self.graph.neighbors(source))\n",
    "\n",
    "    def _generate_walks(self) -> list:\n",
    "        \"\"\"\n",
    "        Generates the random walks which will be used as the skip-gram input.\n",
    "        :return: List of walks. Each walk is a list of nodes.\n",
    "        \"\"\"\n",
    "\n",
    "        flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "        # Split num_walks for each worker\n",
    "        num_walks_lists = np.array_split(range(self.num_walks), self.workers)\n",
    "\n",
    "        walk_results = Parallel(n_jobs=self.workers, temp_folder=self.temp_folder, require=self.require)(\n",
    "            delayed(parallel_generate_walks)(self.d_graph,\n",
    "                                             self.walk_length,\n",
    "                                             len(num_walks),\n",
    "                                             idx,\n",
    "                                             self.sampling_strategy,\n",
    "                                             self.NUM_WALKS_KEY,\n",
    "                                             self.WALK_LENGTH_KEY,\n",
    "                                             self.NEIGHBORS_KEY,\n",
    "                                             self.PROBABILITIES_KEY,\n",
    "                                             self.FIRST_TRAVEL_KEY,\n",
    "                                             self.quiet) for\n",
    "            idx, num_walks\n",
    "            in enumerate(num_walks_lists, 1))\n",
    "\n",
    "        walks = flatten(walk_results)\n",
    "\n",
    "        return walks\n",
    "\n",
    "    def fit(self, **skip_gram_params) -> gensim.models.Word2Vec:\n",
    "        \"\"\"\n",
    "        Creates the embeddings using gensim's Word2Vec.\n",
    "        :param skip_gram_params: Parameters for gensim.models.Word2Vec - do not supply 'size' / 'vector_size' it is\n",
    "            taken from the Node2Vec 'dimensions' parameter\n",
    "        :type skip_gram_params: dict\n",
    "        :return: A gensim word2vec model\n",
    "        \"\"\"\n",
    "\n",
    "        if 'workers' not in skip_gram_params:\n",
    "            skip_gram_params['workers'] = self.workers\n",
    "\n",
    "        # Figure out gensim version, naming of output dimensions changed from size to vector_size in v4.0.0\n",
    "        gensim_version = pkg_resources.get_distribution(\"gensim\").version\n",
    "        size = 'size' if gensim_version < '4.0.0' else 'vector_size'\n",
    "        if size not in skip_gram_params:\n",
    "            skip_gram_params[size] = self.dimensions\n",
    "\n",
    "        if 'sg' not in skip_gram_params:\n",
    "            skip_gram_params['sg'] = 1\n",
    "\n",
    "        return gensim.models.Word2Vec(self.walks, **skip_gram_params)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 22:10:14,450 [INFO] Loading dataset\n",
      "2022-09-15 22:10:19,933 [INFO] Outputting to DGL\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "data collect\n",
    "\"\"\"\n",
    "twitter_bearer_token = 'AAAAAAAAAAAAAAAAAAAAAOYLagEAAAAA1K8YrEuA8CHQDAqAdjkPsBS2Pig%3DMUmnQgjpzkkXslyJpeNytAwFQ2qgiGE0Ah0rkrjuwH9UnOYSLI'\n",
    "dataset = MuminDataset(twitter_bearer_token=twitter_bearer_token)\n",
    "dataset.compile()\n",
    "dataset.add_embeddings()\n",
    "if 'dgl_graph' not in globals():\n",
    "    dgl_graph = dataset.to_dgl()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "graph1\n",
    "\"\"\"\n",
    "rel = ('tweet', 'posted_inv', 'user')\n",
    "g1 = dgl.edge_type_subgraph(dgl_graph, etypes=[rel]).to('cuda')\n",
    "num = g1.edges()[0].shape[0]\n",
    "g_src = g1.edges()[0].tolist()\n",
    "g_tgt = g1.edges()[1].tolist()\n",
    "src = []\n",
    "tgt = []\n",
    "for i in range(num):\n",
    "    for j in range(i,num):\n",
    "        if g_tgt[i] == g_tgt[j]:\n",
    "            src.append(g_src[i])\n",
    "            tgt.append(g_src[j])\n",
    "graph = nx.Graph()\n",
    "for i in range(len(src)):\n",
    "    graph.add_edge(src[i],tgt[i])\n",
    "\n",
    "tgt_node = 'tweet'\n",
    "dim = g1.nodes[tgt_node].data['feat'].shape[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 4178/4178 [00:05<00:00, 817.18it/s] \n",
      "Generating walks (CPU: 4): 100%|██████████| 25/25 [00:06<00:00,  4.15it/s]"
     ]
    }
   ],
   "source": [
    "# Precompute probabilities and generate walks\n",
    "\n",
    "node2vec = Node2Vec(graph, dimensions=dim, walk_length=20, num_walks=100, workers=4)\n",
    "\n",
    "## if d_graph is big enough to fit in the memory, pass temp_folder which has enough disk space\n",
    "# Note: It will trigger \"sharedmem\" in Parallel, which will be slow on smaller graphs\n",
    "#node2vec = Node2Vec(graph, dimensions=64, walk_length=30, num_walks=200, workers=4, temp_folder=\"/mnt/tmp_data\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 22:11:43,110 [INFO] collecting all words and their counts\n",
      "2022-09-15 22:11:43,111 [INFO] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-09-15 22:11:43,128 [INFO] PROGRESS: at sentence #10000, processed 200000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,144 [INFO] PROGRESS: at sentence #20000, processed 400000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,160 [INFO] PROGRESS: at sentence #30000, processed 600000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,175 [INFO] PROGRESS: at sentence #40000, processed 800000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,191 [INFO] PROGRESS: at sentence #50000, processed 1000000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,207 [INFO] PROGRESS: at sentence #60000, processed 1200000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,224 [INFO] PROGRESS: at sentence #70000, processed 1400000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,242 [INFO] PROGRESS: at sentence #80000, processed 1600000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,259 [INFO] PROGRESS: at sentence #90000, processed 1800000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,276 [INFO] PROGRESS: at sentence #100000, processed 2000000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,294 [INFO] PROGRESS: at sentence #110000, processed 2200000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,313 [INFO] PROGRESS: at sentence #120000, processed 2400000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,330 [INFO] PROGRESS: at sentence #130000, processed 2600000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,347 [INFO] PROGRESS: at sentence #140000, processed 2800000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,365 [INFO] PROGRESS: at sentence #150000, processed 3000000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,383 [INFO] PROGRESS: at sentence #160000, processed 3200000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,400 [INFO] PROGRESS: at sentence #170000, processed 3400000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,419 [INFO] PROGRESS: at sentence #180000, processed 3600000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,437 [INFO] PROGRESS: at sentence #190000, processed 3800000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,454 [INFO] PROGRESS: at sentence #200000, processed 4000000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,472 [INFO] PROGRESS: at sentence #210000, processed 4200000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,489 [INFO] PROGRESS: at sentence #220000, processed 4400000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,507 [INFO] PROGRESS: at sentence #230000, processed 4600000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,524 [INFO] PROGRESS: at sentence #240000, processed 4800000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,542 [INFO] PROGRESS: at sentence #250000, processed 5000000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,559 [INFO] PROGRESS: at sentence #260000, processed 5200000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,577 [INFO] PROGRESS: at sentence #270000, processed 5400000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,595 [INFO] PROGRESS: at sentence #280000, processed 5600000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,613 [INFO] PROGRESS: at sentence #290000, processed 5800000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,631 [INFO] PROGRESS: at sentence #300000, processed 6000000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,649 [INFO] PROGRESS: at sentence #310000, processed 6200000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,666 [INFO] PROGRESS: at sentence #320000, processed 6400000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,684 [INFO] PROGRESS: at sentence #330000, processed 6600000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,702 [INFO] PROGRESS: at sentence #340000, processed 6800000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,719 [INFO] PROGRESS: at sentence #350000, processed 7000000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,736 [INFO] PROGRESS: at sentence #360000, processed 7200000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,754 [INFO] PROGRESS: at sentence #370000, processed 7400000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,771 [INFO] PROGRESS: at sentence #380000, processed 7600000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,789 [INFO] PROGRESS: at sentence #390000, processed 7800000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,807 [INFO] PROGRESS: at sentence #400000, processed 8000000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,824 [INFO] PROGRESS: at sentence #410000, processed 8200000 words, keeping 4178 word types\n",
      "2022-09-15 22:11:43,838 [INFO] collected 4178 word types from a corpus of 8356000 raw words and 417800 sentences\n",
      "2022-09-15 22:11:43,838 [INFO] Creating a fresh vocabulary\n",
      "2022-09-15 22:11:43,848 [INFO] Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 4178 unique words (100.00% of original 4178, drops 0)', 'datetime': '2022-09-15T22:11:43.848145', 'gensim': '4.2.0', 'python': '3.7.13 (default, Mar 29 2022, 02:18:16) \\n[GCC 7.5.0]', 'platform': 'Linux-5.15.0-46-generic-x86_64-with-debian-bookworm-sid', 'event': 'prepare_vocab'}\n",
      "2022-09-15 22:11:43,849 [INFO] Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 8356000 word corpus (100.00% of original 8356000, drops 0)', 'datetime': '2022-09-15T22:11:43.849171', 'gensim': '4.2.0', 'python': '3.7.13 (default, Mar 29 2022, 02:18:16) \\n[GCC 7.5.0]', 'platform': 'Linux-5.15.0-46-generic-x86_64-with-debian-bookworm-sid', 'event': 'prepare_vocab'}\n",
      "2022-09-15 22:11:43,864 [INFO] deleting the raw counts dictionary of 4178 items\n",
      "2022-09-15 22:11:43,865 [INFO] sample=0.001 downsamples 0 most-common words\n",
      "2022-09-15 22:11:43,865 [INFO] Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 8356000 word corpus (100.0%% of prior 8356000)', 'datetime': '2022-09-15T22:11:43.865701', 'gensim': '4.2.0', 'python': '3.7.13 (default, Mar 29 2022, 02:18:16) \\n[GCC 7.5.0]', 'platform': 'Linux-5.15.0-46-generic-x86_64-with-debian-bookworm-sid', 'event': 'prepare_vocab'}\n",
      "2022-09-15 22:11:43,893 [INFO] estimated required memory for 4178 words and 812 dimensions: 29229288 bytes\n",
      "2022-09-15 22:11:43,893 [INFO] resetting layer weights\n",
      "2022-09-15 22:11:43,905 [INFO] Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-09-15T22:11:43.905769', 'gensim': '4.2.0', 'python': '3.7.13 (default, Mar 29 2022, 02:18:16) \\n[GCC 7.5.0]', 'platform': 'Linux-5.15.0-46-generic-x86_64-with-debian-bookworm-sid', 'event': 'build_vocab'}\n",
      "2022-09-15 22:11:43,906 [INFO] Word2Vec lifecycle event {'msg': 'training model with 4 workers on 4178 vocabulary and 812 features, using sg=1 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2022-09-15T22:11:43.906509', 'gensim': '4.2.0', 'python': '3.7.13 (default, Mar 29 2022, 02:18:16) \\n[GCC 7.5.0]', 'platform': 'Linux-5.15.0-46-generic-x86_64-with-debian-bookworm-sid', 'event': 'train'}\n",
      "2022-09-15 22:11:44,908 [INFO] EPOCH 0 - PROGRESS: at 2.20% examples, 183642 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:11:45,908 [INFO] EPOCH 0 - PROGRESS: at 4.67% examples, 195259 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:11:46,908 [INFO] EPOCH 0 - PROGRESS: at 7.20% examples, 200603 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:11:47,908 [INFO] EPOCH 0 - PROGRESS: at 10.27% examples, 214449 words/s, in_qsize 7, out_qsize 1\n",
      "2022-09-15 22:11:48,908 [INFO] EPOCH 0 - PROGRESS: at 13.44% examples, 224537 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:11:49,908 [INFO] EPOCH 0 - PROGRESS: at 16.74% examples, 233149 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-15 22:11:50,908 [INFO] EPOCH 0 - PROGRESS: at 20.28% examples, 242092 words/s, in_qsize 4, out_qsize 3\n",
      "2022-09-15 22:11:51,908 [INFO] EPOCH 0 - PROGRESS: at 23.92% examples, 249832 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:11:52,908 [INFO] EPOCH 0 - PROGRESS: at 27.58% examples, 256027 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:11:53,908 [INFO] EPOCH 0 - PROGRESS: at 31.06% examples, 259552 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:11:54,908 [INFO] EPOCH 0 - PROGRESS: at 34.52% examples, 262208 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:11:55,908 [INFO] EPOCH 0 - PROGRESS: at 38.06% examples, 264987 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:11:56,909 [INFO] EPOCH 0 - PROGRESS: at 41.64% examples, 267628 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-15 22:11:57,909 [INFO] EPOCH 0 - PROGRESS: at 45.09% examples, 269135 words/s, in_qsize 7, out_qsize 1\n",
      "2022-09-15 22:11:58,909 [INFO] EPOCH 0 - PROGRESS: at 48.77% examples, 271671 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:11:59,909 [INFO] EPOCH 0 - PROGRESS: at 52.39% examples, 273601 words/s, in_qsize 7, out_qsize 3\n",
      "2022-09-15 22:12:00,909 [INFO] EPOCH 0 - PROGRESS: at 55.95% examples, 274985 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:01,909 [INFO] EPOCH 0 - PROGRESS: at 59.41% examples, 275799 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:02,909 [INFO] EPOCH 0 - PROGRESS: at 62.86% examples, 276443 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-15 22:12:03,909 [INFO] EPOCH 0 - PROGRESS: at 66.31% examples, 277033 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:04,909 [INFO] EPOCH 0 - PROGRESS: at 69.87% examples, 277993 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:05,909 [INFO] EPOCH 0 - PROGRESS: at 73.29% examples, 278349 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-15 22:12:06,909 [INFO] EPOCH 0 - PROGRESS: at 76.72% examples, 278710 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:07,909 [INFO] EPOCH 0 - PROGRESS: at 80.27% examples, 279462 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:08,909 [INFO] EPOCH 0 - PROGRESS: at 83.92% examples, 280467 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:09,909 [INFO] EPOCH 0 - PROGRESS: at 87.46% examples, 281057 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-15 22:12:10,909 [INFO] EPOCH 0 - PROGRESS: at 90.80% examples, 280979 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:12:11,909 [INFO] EPOCH 0 - PROGRESS: at 94.32% examples, 281457 words/s, in_qsize 5, out_qsize 2\n",
      "2022-09-15 22:12:12,909 [INFO] EPOCH 0 - PROGRESS: at 97.95% examples, 282203 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:13,475 [INFO] EPOCH 0: training on 8356000 raw words (8356000 effective words) took 29.6s, 282611 effective words/s\n",
      "2022-09-15 22:12:14,479 [INFO] EPOCH 1 - PROGRESS: at 3.60% examples, 300640 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-15 22:12:15,479 [INFO] EPOCH 1 - PROGRESS: at 7.19% examples, 300327 words/s, in_qsize 5, out_qsize 2\n",
      "2022-09-15 22:12:16,479 [INFO] EPOCH 1 - PROGRESS: at 10.86% examples, 302551 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-15 22:12:17,479 [INFO] EPOCH 1 - PROGRESS: at 14.65% examples, 306071 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:18,479 [INFO] EPOCH 1 - PROGRESS: at 18.46% examples, 308403 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-15 22:12:19,479 [INFO] EPOCH 1 - PROGRESS: at 22.17% examples, 308677 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:20,479 [INFO] EPOCH 1 - PROGRESS: at 25.83% examples, 308359 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:21,479 [INFO] EPOCH 1 - PROGRESS: at 29.54% examples, 308484 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:22,479 [INFO] EPOCH 1 - PROGRESS: at 33.25% examples, 308724 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:23,479 [INFO] EPOCH 1 - PROGRESS: at 36.99% examples, 309045 words/s, in_qsize 7, out_qsize 1\n",
      "2022-09-15 22:12:24,479 [INFO] EPOCH 1 - PROGRESS: at 40.53% examples, 307844 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-15 22:12:25,480 [INFO] EPOCH 1 - PROGRESS: at 44.14% examples, 307335 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:26,480 [INFO] EPOCH 1 - PROGRESS: at 47.68% examples, 306460 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:27,480 [INFO] EPOCH 1 - PROGRESS: at 51.28% examples, 306048 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:28,480 [INFO] EPOCH 1 - PROGRESS: at 54.87% examples, 305632 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:29,480 [INFO] EPOCH 1 - PROGRESS: at 58.52% examples, 305627 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:30,480 [INFO] EPOCH 1 - PROGRESS: at 62.26% examples, 306034 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:31,480 [INFO] EPOCH 1 - PROGRESS: at 65.90% examples, 305909 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-15 22:12:32,480 [INFO] EPOCH 1 - PROGRESS: at 69.65% examples, 306299 words/s, in_qsize 8, out_qsize 2\n",
      "2022-09-15 22:12:33,480 [INFO] EPOCH 1 - PROGRESS: at 73.39% examples, 306616 words/s, in_qsize 7, out_qsize 3\n",
      "2022-09-15 22:12:34,480 [INFO] EPOCH 1 - PROGRESS: at 77.04% examples, 306515 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:35,480 [INFO] EPOCH 1 - PROGRESS: at 80.62% examples, 306191 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:36,480 [INFO] EPOCH 1 - PROGRESS: at 84.31% examples, 306302 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:37,480 [INFO] EPOCH 1 - PROGRESS: at 87.89% examples, 305999 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:12:38,480 [INFO] EPOCH 1 - PROGRESS: at 91.62% examples, 306220 words/s, in_qsize 7, out_qsize 1\n",
      "2022-09-15 22:12:39,480 [INFO] EPOCH 1 - PROGRESS: at 95.25% examples, 306119 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-15 22:12:40,480 [INFO] EPOCH 1 - PROGRESS: at 98.85% examples, 305907 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:12:40,812 [INFO] EPOCH 1: training on 8356000 raw words (8356000 effective words) took 27.3s, 305716 effective words/s\n",
      "2022-09-15 22:12:41,813 [INFO] EPOCH 2 - PROGRESS: at 3.42% examples, 286039 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:42,813 [INFO] EPOCH 2 - PROGRESS: at 6.85% examples, 286210 words/s, in_qsize 7, out_qsize 1\n",
      "2022-09-15 22:12:43,813 [INFO] EPOCH 2 - PROGRESS: at 10.36% examples, 288477 words/s, in_qsize 7, out_qsize 1\n",
      "2022-09-15 22:12:44,813 [INFO] EPOCH 2 - PROGRESS: at 13.90% examples, 290272 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:45,813 [INFO] EPOCH 2 - PROGRESS: at 17.52% examples, 292760 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:46,813 [INFO] EPOCH 2 - PROGRESS: at 21.28% examples, 296298 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:47,813 [INFO] EPOCH 2 - PROGRESS: at 24.99% examples, 298253 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-15 22:12:48,813 [INFO] EPOCH 2 - PROGRESS: at 28.67% examples, 299474 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:12:49,813 [INFO] EPOCH 2 - PROGRESS: at 32.42% examples, 300973 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:50,813 [INFO] EPOCH 2 - PROGRESS: at 36.18% examples, 302292 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:51,814 [INFO] EPOCH 2 - PROGRESS: at 39.92% examples, 303208 words/s, in_qsize 7, out_qsize 1\n",
      "2022-09-15 22:12:52,814 [INFO] EPOCH 2 - PROGRESS: at 43.67% examples, 304092 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:53,814 [INFO] EPOCH 2 - PROGRESS: at 47.30% examples, 304051 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:54,814 [INFO] EPOCH 2 - PROGRESS: at 51.09% examples, 304898 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-15 22:12:55,814 [INFO] EPOCH 2 - PROGRESS: at 54.82% examples, 305374 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:12:56,814 [INFO] EPOCH 2 - PROGRESS: at 58.60% examples, 306036 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:12:57,814 [INFO] EPOCH 2 - PROGRESS: at 62.28% examples, 306108 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-15 22:12:58,814 [INFO] EPOCH 2 - PROGRESS: at 65.84% examples, 305654 words/s, in_qsize 7, out_qsize 2\n",
      "2022-09-15 22:12:59,814 [INFO] EPOCH 2 - PROGRESS: at 69.38% examples, 305119 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:13:00,814 [INFO] EPOCH 2 - PROGRESS: at 73.07% examples, 305277 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-15 22:13:01,814 [INFO] EPOCH 2 - PROGRESS: at 76.84% examples, 305741 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:13:02,814 [INFO] EPOCH 2 - PROGRESS: at 80.62% examples, 306187 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:13:03,814 [INFO] EPOCH 2 - PROGRESS: at 84.35% examples, 306434 words/s, in_qsize 6, out_qsize 3\n",
      "2022-09-15 22:13:04,814 [INFO] EPOCH 2 - PROGRESS: at 88.04% examples, 306531 words/s, in_qsize 7, out_qsize 2\n",
      "2022-09-15 22:13:05,814 [INFO] EPOCH 2 - PROGRESS: at 91.65% examples, 306313 words/s, in_qsize 7, out_qsize 1\n",
      "2022-09-15 22:13:06,814 [INFO] EPOCH 2 - PROGRESS: at 95.28% examples, 306194 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-15 22:13:07,814 [INFO] EPOCH 2 - PROGRESS: at 98.96% examples, 306248 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:13:08,099 [INFO] EPOCH 2: training on 8356000 raw words (8356000 effective words) took 27.3s, 306237 effective words/s\n",
      "2022-09-15 22:13:09,101 [INFO] EPOCH 3 - PROGRESS: at 3.58% examples, 298948 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-15 22:13:10,101 [INFO] EPOCH 3 - PROGRESS: at 7.33% examples, 306172 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:13:11,101 [INFO] EPOCH 3 - PROGRESS: at 10.88% examples, 303070 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:13:12,101 [INFO] EPOCH 3 - PROGRESS: at 14.52% examples, 303400 words/s, in_qsize 8, out_qsize 2\n",
      "2022-09-15 22:13:13,101 [INFO] EPOCH 3 - PROGRESS: at 18.19% examples, 303963 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:13:14,101 [INFO] EPOCH 3 - PROGRESS: at 21.87% examples, 304539 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-15 22:13:15,101 [INFO] EPOCH 3 - PROGRESS: at 25.52% examples, 304585 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:13:16,101 [INFO] EPOCH 3 - PROGRESS: at 29.28% examples, 305810 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:13:17,101 [INFO] EPOCH 3 - PROGRESS: at 32.97% examples, 306124 words/s, in_qsize 8, out_qsize 2\n",
      "2022-09-15 22:13:18,101 [INFO] EPOCH 3 - PROGRESS: at 36.64% examples, 306180 words/s, in_qsize 8, out_qsize 2\n",
      "2022-09-15 22:13:19,101 [INFO] EPOCH 3 - PROGRESS: at 40.30% examples, 306144 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:13:20,101 [INFO] EPOCH 3 - PROGRESS: at 43.68% examples, 304143 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:13:21,102 [INFO] EPOCH 3 - PROGRESS: at 47.21% examples, 303415 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:13:22,102 [INFO] EPOCH 3 - PROGRESS: at 50.75% examples, 302891 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:13:23,102 [INFO] EPOCH 3 - PROGRESS: at 54.35% examples, 302752 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:13:24,102 [INFO] EPOCH 3 - PROGRESS: at 58.03% examples, 303027 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-15 22:13:25,102 [INFO] EPOCH 3 - PROGRESS: at 61.50% examples, 302256 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:13:26,102 [INFO] EPOCH 3 - PROGRESS: at 65.07% examples, 302037 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:13:27,102 [INFO] EPOCH 3 - PROGRESS: at 68.74% examples, 302287 words/s, in_qsize 8, out_qsize 3\n",
      "2022-09-15 22:13:28,102 [INFO] EPOCH 3 - PROGRESS: at 72.43% examples, 302609 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:13:29,102 [INFO] EPOCH 3 - PROGRESS: at 75.95% examples, 302185 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:13:30,102 [INFO] EPOCH 3 - PROGRESS: at 79.36% examples, 301406 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:13:31,102 [INFO] EPOCH 3 - PROGRESS: at 82.83% examples, 300910 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:13:32,102 [INFO] EPOCH 3 - PROGRESS: at 86.39% examples, 300763 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:13:33,102 [INFO] EPOCH 3 - PROGRESS: at 90.02% examples, 300855 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-15 22:13:34,102 [INFO] EPOCH 3 - PROGRESS: at 93.50% examples, 300487 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:13:35,102 [INFO] EPOCH 3 - PROGRESS: at 97.08% examples, 300413 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:13:35,910 [INFO] EPOCH 3: training on 8356000 raw words (8356000 effective words) took 27.8s, 300473 effective words/s\n",
      "2022-09-15 22:13:36,912 [INFO] EPOCH 4 - PROGRESS: at 3.63% examples, 303158 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:13:37,912 [INFO] EPOCH 4 - PROGRESS: at 7.22% examples, 301629 words/s, in_qsize 6, out_qsize 0\n",
      "2022-09-15 22:13:38,912 [INFO] EPOCH 4 - PROGRESS: at 10.81% examples, 301128 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-15 22:13:39,912 [INFO] EPOCH 4 - PROGRESS: at 14.42% examples, 301212 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:13:40,912 [INFO] EPOCH 4 - PROGRESS: at 18.01% examples, 301010 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-15 22:13:41,912 [INFO] EPOCH 4 - PROGRESS: at 21.64% examples, 301365 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:13:42,912 [INFO] EPOCH 4 - PROGRESS: at 25.26% examples, 301473 words/s, in_qsize 7, out_qsize 2\n",
      "2022-09-15 22:13:43,912 [INFO] EPOCH 4 - PROGRESS: at 28.87% examples, 301524 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-15 22:13:44,912 [INFO] EPOCH 4 - PROGRESS: at 32.48% examples, 301563 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:13:45,912 [INFO] EPOCH 4 - PROGRESS: at 36.15% examples, 302080 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:13:46,912 [INFO] EPOCH 4 - PROGRESS: at 39.66% examples, 301241 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:13:47,912 [INFO] EPOCH 4 - PROGRESS: at 43.28% examples, 301367 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-15 22:13:48,912 [INFO] EPOCH 4 - PROGRESS: at 46.90% examples, 301459 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:13:49,912 [INFO] EPOCH 4 - PROGRESS: at 50.53% examples, 301587 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:13:50,912 [INFO] EPOCH 4 - PROGRESS: at 54.15% examples, 301622 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-15 22:13:51,912 [INFO] EPOCH 4 - PROGRESS: at 57.75% examples, 301590 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:13:52,912 [INFO] EPOCH 4 - PROGRESS: at 61.35% examples, 301555 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:13:53,912 [INFO] EPOCH 4 - PROGRESS: at 64.89% examples, 301220 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:13:54,913 [INFO] EPOCH 4 - PROGRESS: at 68.47% examples, 301125 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-15 22:13:55,913 [INFO] EPOCH 4 - PROGRESS: at 71.99% examples, 300744 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-15 22:13:56,913 [INFO] EPOCH 4 - PROGRESS: at 75.61% examples, 300855 words/s, in_qsize 5, out_qsize 2\n",
      "2022-09-15 22:13:57,913 [INFO] EPOCH 4 - PROGRESS: at 79.11% examples, 300471 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:13:58,913 [INFO] EPOCH 4 - PROGRESS: at 82.69% examples, 300392 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:13:59,913 [INFO] EPOCH 4 - PROGRESS: at 86.34% examples, 300598 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:14:00,913 [INFO] EPOCH 4 - PROGRESS: at 89.86% examples, 300333 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:14:01,913 [INFO] EPOCH 4 - PROGRESS: at 93.35% examples, 300016 words/s, in_qsize 6, out_qsize 0\n",
      "2022-09-15 22:14:02,913 [INFO] EPOCH 4 - PROGRESS: at 96.89% examples, 299837 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-15 22:14:03,801 [INFO] EPOCH 4: training on 8356000 raw words (8356000 effective words) took 27.9s, 299614 effective words/s\n",
      "2022-09-15 22:14:03,802 [INFO] Word2Vec lifecycle event {'msg': 'training on 41780000 raw words (41780000 effective words) took 139.9s, 298652 effective words/s', 'datetime': '2022-09-15T22:14:03.802013', 'gensim': '4.2.0', 'python': '3.7.13 (default, Mar 29 2022, 02:18:16) \\n[GCC 7.5.0]', 'platform': 'Linux-5.15.0-46-generic-x86_64-with-debian-bookworm-sid', 'event': 'train'}\n",
      "2022-09-15 22:14:03,802 [INFO] Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4178, vector_size=812, alpha=0.025>', 'datetime': '2022-09-15T22:14:03.802389', 'gensim': '4.2.0', 'python': '3.7.13 (default, Mar 29 2022, 02:18:16) \\n[GCC 7.5.0]', 'platform': 'Linux-5.15.0-46-generic-x86_64-with-debian-bookworm-sid', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "# Embed\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)  # Any keywords acceptable by gensim.Word2Vec can be passed, `dimensions` and `workers` are automatically passed (from the Node2Vec constructor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "nodes = list(graph.nodes())\n",
    "emb = []\n",
    "for i in nodes:\n",
    "    emb.append(model.wv[str(i)])\n",
    "\n",
    "import pandas as pd\n",
    "dataframe = pd.DataFrame(nodes,columns=['node'])\n",
    "dataframe['feats'] = emb\n",
    "\n",
    "sub_g = g1\n",
    "feats = sub_g.nodes[tgt_node].data['feat'].cpu().detach().numpy()\n",
    "text_feats= []\n",
    "for i in range(feats.shape[0]):\n",
    "    text_feats.append(feats[i,:])\n",
    "\n",
    "label = sub_g.nodes[tgt_node].data['label'].cpu().detach().numpy()\n",
    "train_mask = sub_g.nodes[tgt_node].data['train_mask'].cpu().detach().numpy()\n",
    "val_mask = sub_g.nodes[tgt_node].data['val_mask'].cpu().detach().numpy()\n",
    "test_mask = sub_g.nodes[tgt_node].data['test_mask'].cpu().detach().numpy()\n",
    "\n",
    "import pandas as pd\n",
    "dataframe['label'] = label.tolist()\n",
    "dataframe['train_mask'] = train_mask.tolist()\n",
    "dataframe['val_mask'] = val_mask.tolist()\n",
    "dataframe['test_mask'] = test_mask.tolist()\n",
    "dataframe['text_feats'] = text_feats\n",
    "\n",
    "dc = nx.degree_centrality(graph)\n",
    "bc = nx.betweenness_centrality(graph)\n",
    "dc_list = []\n",
    "for key in sorted(dc.keys()):\n",
    "    dc_list.append(dc[key])\n",
    "\n",
    "bc_list = []\n",
    "for key in sorted(bc.keys()):\n",
    "    bc_list.append(bc[key])\n",
    "\n",
    "dc_feats = []\n",
    "for i in range(len(dataframe)):\n",
    "    dc_feats.append((dc_list[i]*dataframe['feats'][i]+(1-dc_list[i])*dataframe['text_feats'][i]))\n",
    "\n",
    "bc_feats = []\n",
    "for i in range(len(dataframe)):\n",
    "    bc_feats.append((bc_list[i]*dataframe['feats'][i]+(1-bc_list[i])*dataframe['text_feats'][i]))\n",
    "\n",
    "dataframe['dc_feats'] = dc_feats\n",
    "dataframe['bc_feats'] = bc_feats\n",
    "\n",
    "mix_feats = []\n",
    "for i in range(len(dataframe)):\n",
    "    mix_feats.append(np.hstack((dataframe['feats'][i],dataframe['text_feats'][i])))\n",
    "dataframe['mix_feats'] = mix_feats\n",
    "\n",
    "# Split up the data\n",
    "train = dataframe.query('train_mask == True')\n",
    "val = dataframe.query('val_mask == True')\n",
    "test = dataframe.query('test_mask == True')\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "val.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def getFeatsMatrix(df):\n",
    "    m_temp = np.zeros([len(df['feats'].values),dim])\n",
    "    for i in range(len(df['feats'].values)):\n",
    "        m_temp[i,:] = df['feats'][i]\n",
    "    return m_temp\n",
    "\n",
    "def getTextFeatsMatrix(df):\n",
    "    m_temp = np.zeros([len(df['text_feats'].values),dim])\n",
    "    for i in range(len(df['text_feats'].values)):\n",
    "        m_temp[i,:] = df['text_feats'][i]\n",
    "    return m_temp\n",
    "\n",
    "def getMixFeatsMatrix(df):\n",
    "    m_temp = np.zeros([len(df['mix_feats'].values),2*dim])\n",
    "    for i in range(len(df['mix_feats'].values)):\n",
    "        m_temp[i,:] = df['mix_feats'][i]\n",
    "    return m_temp\n",
    "\n",
    "def getDcFeatsMatrix(df):\n",
    "    m_temp = np.zeros([len(df['dc_feats'].values),dim])\n",
    "    for i in range(len(df['dc_feats'].values)):\n",
    "        m_temp[i,:] = df['dc_feats'][i]\n",
    "    return m_temp\n",
    "\n",
    "def getBcFeatsMatrix(df):\n",
    "    m_temp = np.zeros([len(df['bc_feats'].values),dim])\n",
    "    for i in range(len(df['bc_feats'].values)):\n",
    "        m_temp[i,:] = df['bc_feats'][i]\n",
    "    return m_temp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "def convert_to_data_loader(dataset,m, num_classes):\n",
    "    # convert from list to tensor\n",
    "    input_tensor = torch.from_numpy(m).float()\n",
    "    label_tensor = torch.from_numpy(np.array(dataset['label'])).long()\n",
    "    tensor_dataset = TensorDataset(input_tensor, label_tensor)\n",
    "    loader = DataLoader(tensor_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    return loader\n",
    "\n",
    "num_classes = 2   # number of possible labels in the sentiment analysis task\n",
    "\n",
    "\n",
    "# train_loader = convert_to_data_loader(train,getMixFeatsMatrix(train), num_classes)\n",
    "# dev_loader = convert_to_data_loader(val, getMixFeatsMatrix(val),num_classes)\n",
    "# test_loader = convert_to_data_loader(test,getMixFeatsMatrix(test), num_classes)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 2)\n",
    "\n",
    "\n",
    "        # 构造Dropout方法，在每次训练过程中都随机“掐死”百分之二十的神经元，防止过拟合。\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 确保输入的tensor是展开的单列数据，把每张图片的通道、长度、宽度三个维度都压缩为一列\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # 在训练过程中对隐含层神经元的正向推断使用Dropout方法\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "\n",
    "        # 在输出单元不需要使用Dropout方法\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def train_nn():\n",
    "    # 对上面定义的Classifier类进行实例化\n",
    "    model = Classifier()\n",
    "\n",
    "    # 定义损失函数为负对数损失函数\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    # 优化方法为Adam梯度下降方法，学习率为0.003\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "    # 对训练集的全部数据学习15遍，这个数字越大，训练时间越长\n",
    "    epochs = 20\n",
    "\n",
    "    # 将每次训练的训练误差和测试误差存储在这两个列表里，后面绘制误差变化折线图用\n",
    "    train_losses, test_losses = [], []\n",
    "\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "\n",
    "        # 对训练集中的所有图片都过一遍\n",
    "        for images, labels in train_loader:\n",
    "            # 将优化器中的求导结果都设为0，否则会在每次反向传播之后叠加之前的\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 对64张图片进行推断，计算损失函数，反向传播优化权重，将损失求和\n",
    "            log_ps = model(images)\n",
    "            loss = criterion(log_ps, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # 每次学完一遍数据集，都进行以下测试操作\n",
    "        else:\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            # 测试的时候不需要开自动求导和反向传播\n",
    "            with torch.no_grad():\n",
    "                # 关闭Dropout\n",
    "                model.eval()\n",
    "\n",
    "                # 对测试集中的所有图片都过一遍\n",
    "                for images, labels in dev_loader:\n",
    "                    # 对传入的测试集图片进行正向推断、计算损失，accuracy为测试集一万张图片中模型预测正确率\n",
    "                    log_ps = model(images)\n",
    "                    test_loss += criterion(log_ps, labels)\n",
    "                    ps = torch.exp(log_ps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "\n",
    "                    # 等号右边为每一批64张测试图片中预测正确的占比\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "            # 恢复Dropout\n",
    "            model.train()\n",
    "            # 将训练误差和测试误差存在两个列表里，后面绘制误差变化折线图用\n",
    "            train_losses.append(running_loss/len(train_loader))\n",
    "            test_losses.append(test_loss/len(dev_loader))\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def predict_nn(trained_model, test_loader):\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0  # count the number of correct classification labels\n",
    "\n",
    "    gold_labs = []  # gold labels to return\n",
    "    pred_labs = []  # predicted labels to return\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "        test_output = trained_model(inputs)\n",
    "        predicted_labels = test_output.argmax(1)\n",
    "\n",
    "        gold_labs.extend(labels.tolist())\n",
    "        pred_labs.extend(predicted_labels.tolist())\n",
    "\n",
    "\n",
    "    f1 = f1_score(gold_labs, pred_labs, average='macro')\n",
    "\n",
    "    return f1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (macro average) = 0.48775055679287305\n"
     ]
    }
   ],
   "source": [
    "train_loader = convert_to_data_loader(train,getBcFeatsMatrix(train), num_classes)\n",
    "dev_loader = convert_to_data_loader(val, getBcFeatsMatrix(val),num_classes)\n",
    "test_loader = convert_to_data_loader(test,getBcFeatsMatrix(test), num_classes)\n",
    "score = []\n",
    "\n",
    "model = train_nn()\n",
    "score=model,test_loader))=\n",
    "\n",
    "print(f'F1 score (macro average) = {score}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (macro average) = 0.48775055679287305\n"
     ]
    }
   ],
   "source": [
    "train_loader = convert_to_data_loader(train,getDcFeatsMatrix(train), num_classes)\n",
    "dev_loader = convert_to_data_loader(val, getDcFeatsMatrix(val),num_classes)\n",
    "test_loader = convert_to_data_loader(test,getDcFeatsMatrix(test), num_classes)\n",
    "\n",
    "score = []\n",
    "\n",
    "model = train_nn()\n",
    "score=model,test_loader))=\n",
    "\n",
    "print(f'F1 score (macro average) = {score}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (macro average) = 0.5314182601356268\n"
     ]
    }
   ],
   "source": [
    "train_loader = convert_to_data_loader(train,getFeatsMatrix(train), num_classes)\n",
    "dev_loader = convert_to_data_loader(val, getFeatsMatrix(val),num_classes)\n",
    "test_loader = convert_to_data_loader(test,getFeatsMatrix(test), num_classes)\n",
    "\n",
    "model = train_nn()\n",
    "score=model,test_loader))=\n",
    "\n",
    "print(f'F1 score (macro average) = {score}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (macro average) = 0.5467342342342343\n"
     ]
    }
   ],
   "source": [
    "train_loader = convert_to_data_loader(train,getMixFeatsMatrix(train), num_classes)\n",
    "dev_loader = convert_to_data_loader(val, getMixFeatsMatrix(val),num_classes)\n",
    "test_loader = convert_to_data_loader(test,getMixFeatsMatrix(test), num_classes)\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2*dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 2)\n",
    "\n",
    "\n",
    "        # 构造Dropout方法，在每次训练过程中都随机“掐死”百分之二十的神经元，防止过拟合。\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 确保输入的tensor是展开的单列数据，把每张图片的通道、长度、宽度三个维度都压缩为一列\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # 在训练过程中对隐含层神经元的正向推断使用Dropout方法\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "\n",
    "        # 在输出单元不需要使用Dropout方法\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = train_nn()\n",
    "score=model,test_loader))=\n",
    "\n",
    "print(f'F1 score (macro average) = {score}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}