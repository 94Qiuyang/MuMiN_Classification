{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mumin[all]==1.6.2 torchmetrics==0.7.2 --quiet\n",
        "!pip install dgl-cu111==0.7.2 -f https://data.dgl.ai/wheels/repo.html --quiet\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tDRC5DOT1ih",
        "outputId": "8085c92a-c48a-46ab-ff6c-706f8b3fd3f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 397 kB 3.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 211 kB 56.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.2 MB 57.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 74.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 281 kB 84.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 93 kB 2.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 81 kB 9.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.4 MB 27.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 43.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 120 kB 63.7 MB/s \n",
            "\u001b[?25h  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for timeout-decorator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 165.0 MB 30 kB/s \n",
            "\u001b[?25hMon Sep 12 10:48:41 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "\n",
        "import os\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "import gensim\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pkg_resources\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm.auto import tqdm\n",
        "from mumin import MuminDataset\n",
        "import dgl\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import argparse\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "from getpass import getpass\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import re\n",
        "import random\n",
        "import time\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJ175mC0kgNQ",
        "outputId": "668fe065-4013-4764-9966-49a0c7f26ba2",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using backend: pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "twitter_bearer_token = 'AAAAAAAAAAAAAAAAAAAAAOYLagEAAAAA1K8YrEuA8CHQDAqAdjkPsBS2Pig%3DMUmnQgjpzkkXslyJpeNytAwFQ2qgiGE0Ah0rkrjuwH9UnOYSLI'\n",
        "dataset = MuminDataset(twitter_bearer_token=twitter_bearer_token,size='small')\n",
        "drive_dir = Path('drive')\n",
        "drive.mount(str(drive_dir.resolve()), force_remount=True)\n",
        "drive_content_dir = [child for child in drive_dir.iterdir() \n",
        "                     if re.search(r'My ?Drive', str(child.stem)) is not None][0]\n",
        "shutil.copy(drive_content_dir / 'mumin-small.zip', 'mumin-small.zip')\n",
        "dataset.compile()\n",
        "dataset.add_embeddings()\n",
        "dgl_g = dataset.to_dgl()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wSsq1qqkhUZ",
        "outputId": "ed614f2b-7922-435b-dfa6-3b98293341d3",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:mumin.dataset:Loading dataset\n",
            "INFO:mumin.dataset:Outputting to DGL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rel1 = ('user', 'posted', 'tweet')\n",
        "rel2 = ('tweet', 'posted_inv', 'user')\n",
        "rel3 = ('tweet', 'discusses', 'claim')\n",
        "rel4 = ('claim', 'discusses_inv', 'tweet')\n",
        "rel5 = ('user', 'follows', 'user')\n",
        "\n",
        "sub_g = dgl.edge_type_subgraph(dgl_g,etypes=[rel1,rel2,rel3,rel4,rel5])\n",
        "\n",
        "\n",
        "# sub_g = dgl.edge_type_subgraph(dgl_g,etypes=[rel3,rel4])\n",
        "\n",
        "claim_id = []\n",
        "for i in range(sub_g.num_nodes('claim')):\n",
        "    claim_id.append('c'+str(i))\n",
        "claim_id.__len__()\n",
        "\n",
        "tweet_id = []\n",
        "for i in range(sub_g.num_nodes('tweet')):\n",
        "    tweet_id.append('t'+str(i))\n",
        "tweet_id.__len__()\n",
        "\n",
        "user_id = []\n",
        "for i in range(sub_g.num_nodes('user')):\n",
        "    user_id.append('u'+str(i))\n",
        "\n",
        "tgt_node = 'claim'\n",
        "dim = sub_g.nodes[tgt_node].data['feat'].shape[1]"
      ],
      "metadata": {
        "id": "OiOXh7G9kinx",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def generate_metapath(tgt_n,result_name,meta_path):\n",
        "    # 拼接路径，输出路径\n",
        "    import tqdm\n",
        "    output_path = open(os.path.join(path, result_name), \"w\")\n",
        "    count = 0\n",
        "\n",
        "    # 看上一个cell实现的construct_graph()函数\n",
        "    # hg: dgl里面定义的heterogeneous graph\n",
        "    # author_names: author id: author name\n",
        "    # hg, author_names, conf_names, paper_names = construct_graph()\n",
        "    hg = sub_g\n",
        "\n",
        "\n",
        "    # 关键：产生metapaths\n",
        "    for conf_idx in tqdm.trange(hg.number_of_nodes(tgt_n)):\n",
        "        # 产生\"conference - paper - Author - paper - conference\"类型的metapath\n",
        "        traces, _ = dgl.sampling.random_walk(\n",
        "            # 这里要改metapath！！！！！！！！！\n",
        "                hg, [conf_idx] * num_walks_per_node, metapath=meta_path * walk_length)\n",
        "        # 将paper都drop掉，只保留conference和Author\n",
        "        # 注意conf_names和author_names这里使得输出的是name而不是id\n",
        "        for tr in traces:\n",
        "            outline = ' '.join(\n",
        "                # 这里要改！！！！！！！！！！！！！！\n",
        "                    claim_id[tr[i]]\n",
        "                    for i in range(0, len(tr),5)\n",
        "            )  # skip paper\n",
        "            # outline = ' '\n",
        "            # for i in range(0,len(tr)):\n",
        "            #     if i%3 == 0:\n",
        "            #         # print(tweet_id[tr[i]])\n",
        "            #         outline = outline+' '+tweet_id[tr[i]]\n",
        "            print(outline, file=output_path)\n",
        "    output_path.close()\n",
        "\n",
        "\n",
        "path = ''\n",
        "\n",
        "# generate_metapath()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "oiJ9T8N9Tta3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGramModel(nn.Module):\n",
        "\n",
        "    def __init__(self, emb_size, emb_dimension):\n",
        "        super(SkipGramModel, self).__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.u_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
        "        self.v_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
        "\n",
        "        initrange = 1.0 / self.emb_dimension\n",
        "        init.uniform_(self.u_embeddings.weight.data, -initrange, initrange)\n",
        "        init.constant_(self.v_embeddings.weight.data, 0)\n",
        "\n",
        "    def forward(self, pos_u, pos_v, neg_v):\n",
        "        emb_u = self.u_embeddings(pos_u)\n",
        "        emb_v = self.v_embeddings(pos_v)\n",
        "        emb_neg_v = self.v_embeddings(neg_v)\n",
        "\n",
        "        score = torch.sum(torch.mul(emb_u, emb_v), dim=1)\n",
        "        score = torch.clamp(score, max=10, min=-10)\n",
        "        score = -F.logsigmoid(score)\n",
        "\n",
        "        neg_score = torch.bmm(emb_neg_v, emb_u.unsqueeze(2)).squeeze()\n",
        "        neg_score = torch.clamp(neg_score, max=10, min=-10)\n",
        "        neg_score = -torch.sum(F.logsigmoid(-neg_score), dim=1)\n",
        "\n",
        "        return torch.mean(score + neg_score)\n",
        "\n",
        "    def save_embedding(self, id2word, file_name):\n",
        "        embedding = self.u_embeddings.weight.cpu().data.numpy()\n",
        "        with open(file_name, 'w') as f:\n",
        "            f.write('%d %d\\n' % (len(id2word), self.emb_dimension))\n",
        "            for wid, w in id2word.items():\n",
        "                e = ' '.join(map(lambda x: str(x), embedding[wid]))\n",
        "                f.write('%s %s\\n' % (w, e))\n",
        "\n",
        "\n",
        "class CustomDataset(object):\n",
        "    \"\"\"\n",
        "    Custom dataset generated by sampler.py (e.g. NetDBIS)\n",
        "    \"\"\"\n",
        "    def __init__(self, path):\n",
        "        self.fn = path\n",
        "\n",
        "class DataReader:\n",
        "    NEGATIVE_TABLE_SIZE = 1e8\n",
        "\n",
        "    def __init__(self, dataset, min_count, care_type):\n",
        "\n",
        "        self.negatives = []\n",
        "        self.discards = []\n",
        "        self.negpos = 0\n",
        "        self.care_type = care_type\n",
        "        self.word2id = dict()\n",
        "        self.id2word = dict()\n",
        "        self.sentences_count = 0\n",
        "        self.token_count = 0\n",
        "        self.word_frequency = dict()\n",
        "        self.inputFileName = dataset.fn\n",
        "        self.read_words(min_count)\n",
        "        self.initTableNegatives()\n",
        "        self.initTableDiscards()\n",
        "\n",
        "    def read_words(self, min_count):\n",
        "        word_frequency = dict()\n",
        "        for line in open(self.inputFileName, encoding=\"ISO-8859-1\"):\n",
        "            line = line.split()\n",
        "            if len(line) > 1:\n",
        "                self.sentences_count += 1\n",
        "                for word in line:\n",
        "                    if len(word) > 0:\n",
        "                        self.token_count += 1\n",
        "                        word_frequency[word] = word_frequency.get(word, 0) + 1\n",
        "\n",
        "                        if self.token_count % 1000000 == 0:\n",
        "                            print(\"Read \" + str(int(self.token_count / 1000000)) + \"M words.\")\n",
        "\n",
        "        wid = 0\n",
        "        for w, c in word_frequency.items():\n",
        "            if c < min_count:\n",
        "                continue\n",
        "            self.word2id[w] = wid\n",
        "            self.id2word[wid] = w\n",
        "            self.word_frequency[wid] = c\n",
        "            wid += 1\n",
        "\n",
        "        self.word_count = len(self.word2id)\n",
        "        print(\"Total embeddings: \" + str(len(self.word2id)))\n",
        "\n",
        "    def initTableDiscards(self):\n",
        "        # get a frequency table for sub-sampling. Note that the frequency is adjusted by\n",
        "        # sub-sampling tricks.\n",
        "        t = 0.0001\n",
        "        f = np.array(list(self.word_frequency.values())) / self.token_count\n",
        "        self.discards = np.sqrt(t / f) + (t / f)\n",
        "\n",
        "    def initTableNegatives(self):\n",
        "        # get a table for negative sampling, if word with index 2 appears twice, then 2 will be listed\n",
        "        # in the table twice.\n",
        "        pow_frequency = np.array(list(self.word_frequency.values())) ** 0.75\n",
        "        words_pow = sum(pow_frequency)\n",
        "        ratio = pow_frequency / words_pow\n",
        "        count = np.round(ratio * DataReader.NEGATIVE_TABLE_SIZE)\n",
        "        for wid, c in enumerate(count):\n",
        "            self.negatives += [wid] * int(c)\n",
        "        self.negatives = np.array(self.negatives)\n",
        "        np.random.shuffle(self.negatives)\n",
        "        self.sampling_prob = ratio\n",
        "\n",
        "    def getNegatives(self, target, size):  # TODO check equality with target\n",
        "        if self.care_type == 0:\n",
        "            response = self.negatives[self.negpos:self.negpos + size]\n",
        "            self.negpos = (self.negpos + size) % len(self.negatives)\n",
        "            if len(response) != size:\n",
        "                return np.concatenate((response, self.negatives[0:self.negpos]))\n",
        "        return response\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "class Metapath2vecDataset(Dataset):\n",
        "    def __init__(self, data, window_size):\n",
        "        # read in data, window_size and input filename\n",
        "        self.data = data\n",
        "        self.window_size = window_size\n",
        "        self.input_file = open(data.inputFileName, encoding=\"ISO-8859-1\")\n",
        "\n",
        "    def __len__(self):\n",
        "        # return the number of walks\n",
        "        return self.data.sentences_count\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # return the list of pairs (center, context, 5 negatives)\n",
        "        while True:\n",
        "            line = self.input_file.readline()\n",
        "            if not line:\n",
        "                self.input_file.seek(0, 0)\n",
        "                line = self.input_file.readline()\n",
        "\n",
        "            if len(line) > 1:\n",
        "                words = line.split()\n",
        "\n",
        "                if len(words) > 1:\n",
        "                    word_ids = [self.data.word2id[w] for w in words if\n",
        "                                w in self.data.word2id and np.random.rand() < self.data.discards[self.data.word2id[w]]]\n",
        "\n",
        "                    pair_catch = []\n",
        "                    for i, u in enumerate(word_ids):\n",
        "                        for j, v in enumerate(\n",
        "                                word_ids[max(i - self.window_size, 0):i + self.window_size]):\n",
        "                            assert u < self.data.word_count\n",
        "                            assert v < self.data.word_count\n",
        "                            if i == j:\n",
        "                                continue\n",
        "                            pair_catch.append((u, v, self.data.getNegatives(v,5)))\n",
        "                    return pair_catch\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def collate(batches):\n",
        "        all_u = [u for batch in batches for u, _, _ in batch if len(batch) > 0]\n",
        "        all_v = [v for batch in batches for _, v, _ in batch if len(batch) > 0]\n",
        "        all_neg_v = [neg_v for batch in batches for _, _, neg_v in batch if len(batch) > 0]\n",
        "\n",
        "        return torch.LongTensor(np.array(all_u)), torch.LongTensor(np.array(all_v)), torch.LongTensor(np.array(all_neg_v))\n",
        "\n",
        "class Metapath2VecTrainer:\n",
        "    def __init__(self, args):\n",
        "        if args.aminer:\n",
        "            dataset = AminerDataset(args.path)\n",
        "        else:\n",
        "            # 我们这里使用这种情况\n",
        "            # 指定path: NetDBIS\n",
        "            dataset = CustomDataset(args.path)\n",
        "        # 读入metapath文件并处理\n",
        "        self.data = DataReader(dataset, args.min_count, args.care_type)\n",
        "        # 实现封装\n",
        "        dataset = Metapath2vecDataset(self.data, args.window_size)\n",
        "        # https://zhuanlan.zhihu.com/p/30385675\n",
        "        self.dataloader = DataLoader(dataset, batch_size=args.batch_size,\n",
        "                                     shuffle=True, num_workers=args.num_workers, collate_fn=dataset.collate)\n",
        "        \n",
        "        # 模型参数用args赋值\n",
        "        self.output_file_name = args.output_file\n",
        "        # 一共多少个节点N\n",
        "        self.emb_size = len(self.data.word2id)\n",
        "        self.emb_dimension = args.dim\n",
        "        self.batch_size = args.batch_size\n",
        "        self.iterations = args.iterations\n",
        "        self.initial_lr = args.initial_lr\n",
        "        # emb_size: 一共多少个节点N; emb_dimension: 维度如128\n",
        "        self.skip_gram_model = SkipGramModel(self.emb_size, self.emb_dimension)\n",
        "\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
        "        if self.use_cuda:\n",
        "            self.skip_gram_model.cuda()\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        for iteration in range(self.iterations):\n",
        "            # print(\"\\n\\n\\nIteration: \" + str(iteration + 1))\n",
        "            # 优化方式\n",
        "            optimizer = optim.SparseAdam(self.skip_gram_model.parameters(), lr=self.initial_lr)\n",
        "            # 优化方式和学习率等\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(self.dataloader))\n",
        "\n",
        "            running_loss = 0.0\n",
        "            # 按batch训练\n",
        "            for i, sample_batched in enumerate(tqdm(self.dataloader)):\n",
        "\n",
        "                if len(sample_batched[0]) > 1:\n",
        "                    pos_u = sample_batched[0].to(self.device)\n",
        "                    pos_v = sample_batched[1].to(self.device)\n",
        "                    neg_v = sample_batched[2].to(self.device)\n",
        "\n",
        "                    \n",
        "                    optimizer.zero_grad()\n",
        "                    # 损失函数值\n",
        "                    loss = self.skip_gram_model.forward(pos_u, pos_v, neg_v)\n",
        "                    # 反向传播，更新参数\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    #有改动@@@@@@@@@@@@@@@@@@@@@@@\n",
        "                    scheduler.step()\n",
        "\n",
        "                    running_loss = running_loss * 0.9 + loss.item() * 0.1\n",
        "                    # if i > 0 and i % 500 == 0:\n",
        "                    #     print(\" Loss: \" + str(running_loss))\n",
        "\n",
        "            self.skip_gram_model.save_embedding(self.data.id2word, self.output_file_name)"
      ],
      "metadata": {
        "id": "gJRX4USZpr9g",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def getData():\n",
        "    feats = sub_g.nodes[tgt_node].data['feat'].cpu().detach().numpy()\n",
        "    text_feats= []\n",
        "    for i in range(feats.shape[0]):\n",
        "        text_feats.append(feats[i,:])\n",
        "\n",
        "    label = sub_g.nodes[tgt_node].data['label'].cpu().detach().numpy()\n",
        "    train_mask = sub_g.nodes[tgt_node].data['train_mask'].cpu().detach().numpy()\n",
        "    val_mask = sub_g.nodes[tgt_node].data['val_mask'].cpu().detach().numpy()\n",
        "    test_mask = sub_g.nodes[tgt_node].data['test_mask'].cpu().detach().numpy()\n",
        "\n",
        "    import pandas as pd\n",
        "    dataframe = pd.DataFrame(label,columns=['label'])\n",
        "    dataframe['train_mask'] = train_mask.tolist()\n",
        "    dataframe['val_mask'] = val_mask.tolist()\n",
        "    dataframe['test_mask'] = test_mask.tolist()\n",
        "    dataframe['text_feats'] = text_feats\n",
        "\n",
        "    feats_list = []\n",
        "    id_list = []\n",
        "    with open('result_'+name, \"r\") as f:\n",
        "        for line in f.readlines():\n",
        "            line = line.strip('\\n')\n",
        "            #去掉列表中每一个元素的换行符\n",
        "            feats = []\n",
        "            for idx,word in enumerate(line.split()):\n",
        "                if idx == 0:\n",
        "                    id_list.append(word)\n",
        "                else:\n",
        "                    feats.append(word)\n",
        "            feats_list.append(np.array(feats,dtype=float))\n",
        "\n",
        "    feat_df = pd.DataFrame({'feats':feats_list})\n",
        "    feat_df['id'] = id_list\n",
        "    feat_df = feat_df.drop(index=[0])\n",
        "\n",
        "    mask = [ True if i.startswith('c') else False for i in feat_df.id ]\n",
        "    feat_df = feat_df[mask]\n",
        "\n",
        "    num_id = [int(word[1:]) for word in feat_df.id]\n",
        "    feat_df['num_id'] = num_id\n",
        "\n",
        "    feat_df = feat_df.sort_values(by = 'num_id')\n",
        "    feat_df.reset_index(drop=True, inplace=True)\n",
        "    tweet_claim_df = dataframe.join(feat_df)\n",
        "    tweet_claim_df = tweet_claim_df.dropna()\n",
        "\n",
        "    mix_feats = []\n",
        "    for i in range(len(tweet_claim_df)):\n",
        "        mix_feats.append(np.hstack((tweet_claim_df['feats'][i],tweet_claim_df['text_feats'][i])))\n",
        "    tweet_claim_df['mix_feats'] = mix_feats\n",
        "\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    # Split up the data\n",
        "    train = tweet_claim_df.query('train_mask == True')\n",
        "    val = tweet_claim_df.query('val_mask == True')\n",
        "    test = tweet_claim_df.query('test_mask == True')\n",
        "\n",
        "    train.reset_index(drop=True, inplace=True)\n",
        "    val.reset_index(drop=True, inplace=True)\n",
        "    test.reset_index(drop=True, inplace=True)\n",
        "    return train,val,test"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "1v1DVQ6aTta5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def getFeatsMatrix(df):\n",
        "    m_temp = np.zeros([len(df['feats'].values),dim])\n",
        "    for i in range(len(df['feats'].values)):\n",
        "        m_temp[i,:] = df['feats'][i]\n",
        "    return m_temp\n",
        "\n",
        "def getTextFeatsMatrix(df):\n",
        "    m_temp = np.zeros([len(df['text_feats'].values),dim])\n",
        "    for i in range(len(df['text_feats'].values)):\n",
        "        m_temp[i,:] = df['text_feats'][i]\n",
        "    return m_temp\n",
        "\n",
        "def getMixFeatsMatrix(df):\n",
        "    m_temp = np.zeros([len(df['mix_feats'].values),2*dim])\n",
        "    for i in range(len(df['mix_feats'].values)):\n",
        "        m_temp[i,:] = df['mix_feats'][i]\n",
        "    return m_temp"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "cm0v7jqFTta6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "def convert_to_data_loader(dataset,m, num_classes):\n",
        "    # convert from list to tensor\n",
        "    input_tensor = torch.from_numpy(m).float()\n",
        "    label_tensor = torch.from_numpy(np.array(dataset['label'])).long()\n",
        "    tensor_dataset = TensorDataset(input_tensor, label_tensor)\n",
        "    loader = DataLoader(tensor_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    return loader\n",
        "\n",
        "num_classes = 2   # number of possible labels in the sentiment analysis task"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "_K6U3SKTTta6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.fc4 = nn.Linear(32, 2)\n",
        "\n",
        "\n",
        "        # 构造Dropout方法，在每次训练过程中都随机“掐死”百分之二十的神经元，防止过拟合。\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 确保输入的tensor是展开的单列数据，把每张图片的通道、长度、宽度三个维度都压缩为一列\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "        # 在训练过程中对隐含层神经元的正向推断使用Dropout方法\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.dropout(F.relu(self.fc3(x)))\n",
        "\n",
        "        # 在输出单元不需要使用Dropout方法\n",
        "        x = F.log_softmax(self.fc4(x), dim=1)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Lm7KNGDRTta6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def train_nn():\n",
        "    # 对上面定义的Classifier类进行实例化\n",
        "    model = Classifier()\n",
        "\n",
        "    # 定义损失函数为负对数损失函数\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    # 优化方法为Adam梯度下降方法，学习率为0.003\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "    # 对训练集的全部数据学习15遍，这个数字越大，训练时间越长\n",
        "    epochs = 15\n",
        "\n",
        "    # 将每次训练的训练误差和测试误差存储在这两个列表里，后面绘制误差变化折线图用\n",
        "    train_losses, test_losses = [], []\n",
        "\n",
        "    for e in range(epochs):\n",
        "        running_loss = 0\n",
        "\n",
        "        # 对训练集中的所有图片都过一遍\n",
        "        for images, labels in train_loader:\n",
        "            # 将优化器中的求导结果都设为0，否则会在每次反向传播之后叠加之前的\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 对64张图片进行推断，计算损失函数，反向传播优化权重，将损失求和\n",
        "            log_ps = model(images)\n",
        "            loss = criterion(log_ps, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # 每次学完一遍数据集，都进行以下测试操作\n",
        "        else:\n",
        "            test_loss = 0\n",
        "            accuracy = 0\n",
        "            # 测试的时候不需要开自动求导和反向传播\n",
        "            with torch.no_grad():\n",
        "                # 关闭Dropout\n",
        "                model.eval()\n",
        "\n",
        "                # 对测试集中的所有图片都过一遍\n",
        "                for images, labels in dev_loader:\n",
        "                    # 对传入的测试集图片进行正向推断、计算损失，accuracy为测试集一万张图片中模型预测正确率\n",
        "                    log_ps = model(images)\n",
        "                    test_loss += criterion(log_ps, labels)\n",
        "                    ps = torch.exp(log_ps)\n",
        "                    top_p, top_class = ps.topk(1, dim=1)\n",
        "                    equals = top_class == labels.view(*top_class.shape)\n",
        "\n",
        "                    # 等号右边为每一批64张测试图片中预测正确的占比\n",
        "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "            # 恢复Dropout\n",
        "            model.train()\n",
        "            # 将训练误差和测试误差存在两个列表里，后面绘制误差变化折线图用\n",
        "            train_losses.append(running_loss/len(train_loader))\n",
        "            test_losses.append(test_loss/len(dev_loader))\n",
        "\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bPXeWSIUTta7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def predict_nn(trained_model, test_loader):\n",
        "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0  # count the number of correct classification labels\n",
        "\n",
        "    gold_labs = []  # gold labels to return\n",
        "    pred_labs = []  # predicted labels to return\n",
        "\n",
        "    for inputs, labels in test_loader:\n",
        "        test_output = trained_model(inputs)\n",
        "        predicted_labels = test_output.argmax(1)\n",
        "\n",
        "        gold_labs.extend(labels.tolist())\n",
        "        pred_labs.extend(predicted_labels.tolist())\n",
        "\n",
        "\n",
        "\n",
        "    f1 = f1_score(gold_labs, pred_labs, average='macro')\n",
        "    return f1"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "QUvUvyuqTta7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "f1_wl = []\n",
        "for wl in range(5,106,10):\n",
        "    num_walks_per_node = 50\n",
        "    # random walk的长度\n",
        "    walk_length = wl\n",
        "    metapath = ['discusses_inv','posted_inv','follows','posted','discusses']\n",
        "    name = 'ctuutc'+str(num_walks_per_node)+'_'+str(walk_length)+'.txt'\n",
        "    generate_metapath(tgt_node,name,metapath)\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Metapath2vec\")\n",
        "    #parser.add_argument('--input_file', type=str, help=\"input_file\")\n",
        "    parser.add_argument('--aminer', action='store_true', help='Use AMiner dataset')\n",
        "    # 输入文件\n",
        "    parser.add_argument('--path', type=str, help=\"input_path\", default=name)\n",
        "    # 输出文件\n",
        "    parser.add_argument('--output_file', type=str, help='output_file', default='result_'+name)\n",
        "    # embedding维度\n",
        "    parser.add_argument('--dim', default=dim, type=int, help=\"embedding dimensions\")\n",
        "    # 窗口大小\n",
        "    parser.add_argument('--window_size', default=7, type=int, help=\"context window size\")\n",
        "    # 迭代次数\n",
        "    parser.add_argument('--iterations', default=5, type=int, help=\"iterations\")\n",
        "    # batch size\n",
        "    parser.add_argument('--batch_size', default=50, type=int, help=\"batch size\")\n",
        "    # 0: metapath2vec; 1: metapath2vec++\n",
        "    parser.add_argument('--care_type', default=0, type=int, help=\"if 1, heterogeneous negative sampling, else normal negative sampling\")\n",
        "    # 学习率\n",
        "    parser.add_argument('--initial_lr', default=0.025, type=float, help=\"learning rate\")\n",
        "    # skip2gram模型的词频\n",
        "    parser.add_argument('--min_count', default=5, type=int, help=\"min count\")\n",
        "    # 资源核数\n",
        "    parser.add_argument('--num_workers', default=16, type=int, help=\"number of workers\")\n",
        "    args = parser.parse_args(args=[])\n",
        "    from tqdm import tqdm\n",
        "    m2v = Metapath2VecTrainer(args)\n",
        "    m2v.train()\n",
        "\n",
        "    train,val,test = getData()\n",
        "\n",
        "    train_loader = convert_to_data_loader(train,getFeatsMatrix(train), num_classes)\n",
        "    dev_loader = convert_to_data_loader(val, getFeatsMatrix(val),num_classes)\n",
        "    test_loader = convert_to_data_loader(test,getFeatsMatrix(test), num_classes)\n",
        "\n",
        "    score = []\n",
        "\n",
        "    for i in range(20):\n",
        "        model = train_nn()\n",
        "        score.append(predict_nn(model,test_loader))\n",
        "\n",
        "    print(f'F1 score (macro average) = {sorted(score)[-1]}')\n",
        "    f1_wl.append(sorted(score)[-1])"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "iyKRdrx8Tta7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1_wl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYUOvMzKV3Cp",
        "outputId": "74671575-73de-4272-961d-1ee96cbfb295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6565920273119792,\n",
              " 0.5476190476190477,\n",
              " 0.625615763546798,\n",
              " 0.5777777777777777,\n",
              " 0.5650165679285069,\n",
              " 0.7223296797764883,\n",
              " 0.592128801431127,\n",
              " 0.5966981132075472,\n",
              " 0.507899507899508,\n",
              " 0.626003626003626,\n",
              " 0.5396915125575386]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1_nw = []\n",
        "for nw in range(20,201,20):\n",
        "    num_walks_per_node = nw\n",
        "    # random walk的长度\n",
        "    walk_length = 50\n",
        "    metapath = ['discusses_inv','posted_inv','follows','posted','discusses']\n",
        "    name = 'ctuutc'+str(num_walks_per_node)+'_'+str(walk_length)+'.txt'\n",
        "    generate_metapath(tgt_node,name,metapath)\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Metapath2vec\")\n",
        "    #parser.add_argument('--input_file', type=str, help=\"input_file\")\n",
        "    parser.add_argument('--aminer', action='store_true', help='Use AMiner dataset')\n",
        "    # 输入文件\n",
        "    parser.add_argument('--path', type=str, help=\"input_path\", default=name)\n",
        "    # 输出文件\n",
        "    parser.add_argument('--output_file', type=str, help='output_file', default='result_'+name)\n",
        "    # embedding维度\n",
        "    parser.add_argument('--dim', default=dim, type=int, help=\"embedding dimensions\")\n",
        "    # 窗口大小\n",
        "    parser.add_argument('--window_size', default=7, type=int, help=\"context window size\")\n",
        "    # 迭代次数\n",
        "    parser.add_argument('--iterations', default=5, type=int, help=\"iterations\")\n",
        "    # batch size\n",
        "    parser.add_argument('--batch_size', default=50, type=int, help=\"batch size\")\n",
        "    # 0: metapath2vec; 1: metapath2vec++\n",
        "    parser.add_argument('--care_type', default=0, type=int, help=\"if 1, heterogeneous negative sampling, else normal negative sampling\")\n",
        "    # 学习率\n",
        "    parser.add_argument('--initial_lr', default=0.025, type=float, help=\"learning rate\")\n",
        "    # skip2gram模型的词频\n",
        "    parser.add_argument('--min_count', default=5, type=int, help=\"min count\")\n",
        "    # 资源核数\n",
        "    parser.add_argument('--num_workers', default=16, type=int, help=\"number of workers\")\n",
        "    args = parser.parse_args(args=[])\n",
        "    from tqdm import tqdm\n",
        "    m2v = Metapath2VecTrainer(args)\n",
        "    m2v.train()\n",
        "\n",
        "    train,val,test = getData()\n",
        "\n",
        "    train_loader = convert_to_data_loader(train,getFeatsMatrix(train), num_classes)\n",
        "    dev_loader = convert_to_data_loader(val, getFeatsMatrix(val),num_classes)\n",
        "    test_loader = convert_to_data_loader(test,getFeatsMatrix(test), num_classes)\n",
        "\n",
        "    score = []\n",
        "\n",
        "    for i in range(20):\n",
        "        model = train_nn()\n",
        "        score.append(predict_nn(model,test_loader))\n",
        "\n",
        "    print(f'F1 score (macro average) = {sorted(score)[-1]}, num_walks = {nw}')\n",
        "    f1_nw.append(sorted(score)[-1])"
      ],
      "metadata": {
        "id": "UqwwLAGnV5gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_nw1 = []\n",
        "for nw in range(200,501,100):\n",
        "    num_walks_per_node = nw\n",
        "    # random walk的长度\n",
        "    walk_length = 50\n",
        "    metapath = ['discusses_inv','posted_inv','follows','posted','discusses']\n",
        "    name = 'ctuutc'+str(num_walks_per_node)+'_'+str(walk_length)+'.txt'\n",
        "    generate_metapath(tgt_node,name,metapath)\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Metapath2vec\")\n",
        "    #parser.add_argument('--input_file', type=str, help=\"input_file\")\n",
        "    parser.add_argument('--aminer', action='store_true', help='Use AMiner dataset')\n",
        "    # 输入文件\n",
        "    parser.add_argument('--path', type=str, help=\"input_path\", default=name)\n",
        "    # 输出文件\n",
        "    parser.add_argument('--output_file', type=str, help='output_file', default='result_'+name)\n",
        "    # embedding维度\n",
        "    parser.add_argument('--dim', default=dim, type=int, help=\"embedding dimensions\")\n",
        "    # 窗口大小\n",
        "    parser.add_argument('--window_size', default=7, type=int, help=\"context window size\")\n",
        "    # 迭代次数\n",
        "    parser.add_argument('--iterations', default=5, type=int, help=\"iterations\")\n",
        "    # batch size\n",
        "    parser.add_argument('--batch_size', default=50, type=int, help=\"batch size\")\n",
        "    # 0: metapath2vec; 1: metapath2vec++\n",
        "    parser.add_argument('--care_type', default=0, type=int, help=\"if 1, heterogeneous negative sampling, else normal negative sampling\")\n",
        "    # 学习率\n",
        "    parser.add_argument('--initial_lr', default=0.025, type=float, help=\"learning rate\")\n",
        "    # skip2gram模型的词频\n",
        "    parser.add_argument('--min_count', default=5, type=int, help=\"min count\")\n",
        "    # 资源核数\n",
        "    parser.add_argument('--num_workers', default=16, type=int, help=\"number of workers\")\n",
        "    args = parser.parse_args(args=[])\n",
        "    from tqdm import tqdm\n",
        "    m2v = Metapath2VecTrainer(args)\n",
        "    m2v.train()\n",
        "\n",
        "    train,val,test = getData()\n",
        "\n",
        "    train_loader = convert_to_data_loader(train,getFeatsMatrix(train), num_classes)\n",
        "    dev_loader = convert_to_data_loader(val, getFeatsMatrix(val),num_classes)\n",
        "    test_loader = convert_to_data_loader(test,getFeatsMatrix(test), num_classes)\n",
        "\n",
        "    model = train_nn()\n",
        "    score = predict_nn(model,test_loader)\n",
        "    print(f'F1 score (macro average) = {score}')\n",
        "\n",
        "    print(f'F1 score (macro average) = {score}, num_walks = {nw}')\n",
        "    f1_nw1.append(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b7sxAyObxNb",
        "outputId": "457878f5-af97-482c-abc7-cb812635322f"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2100/2100 [00:53<00:00, 38.96it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read 1M words.\n",
            "Read 2M words.\n",
            "Read 3M words.\n",
            "Read 4M words.\n",
            "Read 5M words.\n",
            "Read 6M words.\n",
            "Read 7M words.\n",
            "Read 8M words.\n",
            "Read 9M words.\n",
            "Read 10M words.\n",
            "Read 11M words.\n",
            "Read 12M words.\n",
            "Read 13M words.\n",
            "Read 14M words.\n",
            "Read 15M words.\n",
            "Read 16M words.\n",
            "Read 17M words.\n",
            "Read 18M words.\n",
            "Read 19M words.\n",
            "Read 20M words.\n",
            "Read 21M words.\n",
            "Total embeddings: 2100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 8400/8400 [00:45<00:00, 186.10it/s]\n",
            "100%|██████████| 8400/8400 [00:44<00:00, 189.74it/s]\n",
            "100%|██████████| 8400/8400 [00:44<00:00, 187.02it/s]\n",
            "100%|██████████| 8400/8400 [00:45<00:00, 185.07it/s]\n",
            "100%|██████████| 8400/8400 [00:44<00:00, 187.95it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 score (macro average) = 0.6431924882629108, num_walks = 200\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2100/2100 [01:13<00:00, 28.71it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read 1M words.\n",
            "Read 2M words.\n",
            "Read 3M words.\n",
            "Read 4M words.\n",
            "Read 5M words.\n",
            "Read 6M words.\n",
            "Read 7M words.\n",
            "Read 8M words.\n",
            "Read 9M words.\n",
            "Read 10M words.\n",
            "Read 11M words.\n",
            "Read 12M words.\n",
            "Read 13M words.\n",
            "Read 14M words.\n",
            "Read 15M words.\n",
            "Read 16M words.\n",
            "Read 17M words.\n",
            "Read 18M words.\n",
            "Read 19M words.\n",
            "Read 20M words.\n",
            "Read 21M words.\n",
            "Read 22M words.\n",
            "Read 23M words.\n",
            "Read 24M words.\n",
            "Read 25M words.\n",
            "Read 26M words.\n",
            "Read 27M words.\n",
            "Read 28M words.\n",
            "Read 29M words.\n",
            "Read 30M words.\n",
            "Read 31M words.\n",
            "Read 32M words.\n",
            "Total embeddings: 2100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 12600/12600 [01:04<00:00, 195.08it/s]\n",
            "100%|██████████| 12600/12600 [01:05<00:00, 191.65it/s]\n",
            "100%|██████████| 12600/12600 [01:05<00:00, 192.84it/s]\n",
            "100%|██████████| 12600/12600 [01:04<00:00, 194.53it/s]\n",
            "100%|██████████| 12600/12600 [01:06<00:00, 189.33it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 score (macro average) = 0.6755336617405583, num_walks = 300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2100/2100 [01:35<00:00, 22.04it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read 1M words.\n",
            "Read 2M words.\n",
            "Read 3M words.\n",
            "Read 4M words.\n",
            "Read 5M words.\n",
            "Read 6M words.\n",
            "Read 7M words.\n",
            "Read 8M words.\n",
            "Read 9M words.\n",
            "Read 10M words.\n",
            "Read 11M words.\n",
            "Read 12M words.\n",
            "Read 13M words.\n",
            "Read 14M words.\n",
            "Read 15M words.\n",
            "Read 16M words.\n",
            "Read 17M words.\n",
            "Read 18M words.\n",
            "Read 19M words.\n",
            "Read 20M words.\n",
            "Read 21M words.\n",
            "Read 22M words.\n",
            "Read 23M words.\n",
            "Read 24M words.\n",
            "Read 25M words.\n",
            "Read 26M words.\n",
            "Read 27M words.\n",
            "Read 28M words.\n",
            "Read 29M words.\n",
            "Read 30M words.\n",
            "Read 31M words.\n",
            "Read 32M words.\n",
            "Read 33M words.\n",
            "Read 34M words.\n",
            "Read 35M words.\n",
            "Read 36M words.\n",
            "Read 37M words.\n",
            "Read 38M words.\n",
            "Read 39M words.\n",
            "Read 40M words.\n",
            "Read 41M words.\n",
            "Read 42M words.\n",
            "Total embeddings: 2100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 16800/16800 [01:25<00:00, 197.09it/s]\n",
            "100%|██████████| 16800/16800 [01:26<00:00, 194.56it/s]\n",
            "100%|██████████| 16800/16800 [01:26<00:00, 193.52it/s]\n",
            "100%|██████████| 16800/16800 [01:26<00:00, 193.94it/s]\n",
            "100%|██████████| 16800/16800 [01:26<00:00, 194.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score (macro average) = 0.617930456640134, num_walks = 400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2100/2100 [01:59<00:00, 17.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 1M words.\n",
            "Read 2M words.\n",
            "Read 3M words.\n",
            "Read 4M words.\n",
            "Read 5M words.\n",
            "Read 6M words.\n",
            "Read 7M words.\n",
            "Read 8M words.\n",
            "Read 9M words.\n",
            "Read 10M words.\n",
            "Read 11M words.\n",
            "Read 12M words.\n",
            "Read 13M words.\n",
            "Read 14M words.\n",
            "Read 15M words.\n",
            "Read 16M words.\n",
            "Read 17M words.\n",
            "Read 18M words.\n",
            "Read 19M words.\n",
            "Read 20M words.\n",
            "Read 21M words.\n",
            "Read 22M words.\n",
            "Read 23M words.\n",
            "Read 24M words.\n",
            "Read 25M words.\n",
            "Read 26M words.\n",
            "Read 27M words.\n",
            "Read 28M words.\n",
            "Read 29M words.\n",
            "Read 30M words.\n",
            "Read 31M words.\n",
            "Read 32M words.\n",
            "Read 33M words.\n",
            "Read 34M words.\n",
            "Read 35M words.\n",
            "Read 36M words.\n",
            "Read 37M words.\n",
            "Read 38M words.\n",
            "Read 39M words.\n",
            "Read 40M words.\n",
            "Read 41M words.\n",
            "Read 42M words.\n",
            "Read 43M words.\n",
            "Read 44M words.\n",
            "Read 45M words.\n",
            "Read 46M words.\n",
            "Read 47M words.\n",
            "Read 48M words.\n",
            "Read 49M words.\n",
            "Read 50M words.\n",
            "Read 51M words.\n",
            "Read 52M words.\n",
            "Read 53M words.\n",
            "Total embeddings: 2100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 21000/21000 [01:46<00:00, 197.29it/s]\n",
            "100%|██████████| 21000/21000 [01:46<00:00, 197.36it/s]\n",
            "100%|██████████| 21000/21000 [01:46<00:00, 196.60it/s]\n",
            "100%|██████████| 21000/21000 [01:47<00:00, 194.80it/s]\n",
            "100%|██████████| 21000/21000 [01:48<00:00, 193.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score (macro average) = 0.625615763546798, num_walks = 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1_nw1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VueD2xrGnng8",
        "outputId": "c1f6962b-82c5-404d-b926-7ac994b9076d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6431924882629108, 0.6755336617405583, 0.617930456640134, 0.625615763546798]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1_wl = []\n",
        "for wl in range(55,56):\n",
        "    num_walks_per_node = 50\n",
        "    # random walk的长度\n",
        "    walk_length = wl\n",
        "    metapath = ['discusses_inv','posted_inv','follows','posted','discusses']\n",
        "    name = 'ctuutc'+str(num_walks_per_node)+'_'+str(walk_length)+'.txt'\n",
        "    generate_metapath(tgt_node,name,metapath)\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Metapath2vec\")\n",
        "    #parser.add_argument('--input_file', type=str, help=\"input_file\")\n",
        "    parser.add_argument('--aminer', action='store_true', help='Use AMiner dataset')\n",
        "    # 输入文件\n",
        "    parser.add_argument('--path', type=str, help=\"input_path\", default=name)\n",
        "    # 输出文件\n",
        "    parser.add_argument('--output_file', type=str, help='output_file', default='result_'+name)\n",
        "    # embedding维度\n",
        "    parser.add_argument('--dim', default=dim, type=int, help=\"embedding dimensions\")\n",
        "    # 窗口大小\n",
        "    parser.add_argument('--window_size', default=7, type=int, help=\"context window size\")\n",
        "    # 迭代次数\n",
        "    parser.add_argument('--iterations', default=5, type=int, help=\"iterations\")\n",
        "    # batch size\n",
        "    parser.add_argument('--batch_size', default=50, type=int, help=\"batch size\")\n",
        "    # 0: metapath2vec; 1: metapath2vec++\n",
        "    parser.add_argument('--care_type', default=0, type=int, help=\"if 1, heterogeneous negative sampling, else normal negative sampling\")\n",
        "    # 学习率\n",
        "    parser.add_argument('--initial_lr', default=0.025, type=float, help=\"learning rate\")\n",
        "    # skip2gram模型的词频\n",
        "    parser.add_argument('--min_count', default=5, type=int, help=\"min count\")\n",
        "    # 资源核数\n",
        "    parser.add_argument('--num_workers', default=16, type=int, help=\"number of workers\")\n",
        "    args = parser.parse_args(args=[])\n",
        "    from tqdm import tqdm\n",
        "    m2v = Metapath2VecTrainer(args)\n",
        "    m2v.train()\n",
        "\n",
        "    train,val,test = getData()\n",
        "\n",
        "    train_loader = convert_to_data_loader(train,getFeatsMatrix(train), num_classes)\n",
        "    dev_loader = convert_to_data_loader(val, getFeatsMatrix(val),num_classes)\n",
        "    test_loader = convert_to_data_loader(test,getFeatsMatrix(test), num_classes)\n",
        "\n",
        "    model = train_nn()\n",
        "    score = predict_nn(model,test_loader)\n",
        "    print(f'F1 score (macro average) = {score}')\n",
        "\n",
        "    print(f'F1 score (macro average) = {score}, num_walks = {nw}')\n",
        "    f1_nw1.append(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AeeXKwFn7Qq",
        "outputId": "bb55910b-6db4-4bd7-b148-ca4ed9eee53e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2100/2100 [00:15<00:00, 137.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 1M words.\n",
            "Read 2M words.\n",
            "Read 3M words.\n",
            "Read 4M words.\n",
            "Read 5M words.\n",
            "Total embeddings: 2100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 2100/2100 [00:13<00:00, 151.57it/s]\n",
            "100%|██████████| 2100/2100 [00:13<00:00, 157.10it/s]\n",
            "100%|██████████| 2100/2100 [00:13<00:00, 155.23it/s]\n",
            "100%|██████████| 2100/2100 [00:13<00:00, 156.25it/s]\n",
            "100%|██████████| 2100/2100 [00:13<00:00, 154.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score (macro average) = 0.6402784222737818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1_nw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8-AnJN4yzBJ",
        "outputId": "280a9896-e0d5-4be5-dc3c-7213e90dc3d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5866355866355867,\n",
              " 0.5465909090909091,\n",
              " 0.5615384615384615,\n",
              " 0.625615763546798,\n",
              " 0.5756978653530377,\n",
              " 0.643020594965675,\n",
              " 0.602840680691411,\n",
              " 0.5765161915529355,\n",
              " 0.592128801431127,\n",
              " 0.657543391188251]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}