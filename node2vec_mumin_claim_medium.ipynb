{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import gensim\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pkg_resources\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.auto import tqdm\n",
    "from mumin import MuminDataset\n",
    "import dgl\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def parallel_generate_walks(d_graph: dict, global_walk_length: int, num_walks: int, cpu_num: int,\n",
    "                            sampling_strategy: dict = None, num_walks_key: str = None, walk_length_key: str = None,\n",
    "                            neighbors_key: str = None, probabilities_key: str = None, first_travel_key: str = None,\n",
    "                            quiet: bool = False) -> list:\n",
    "    \"\"\"\n",
    "    Generates the random walks which will be used as the skip-gram input.\n",
    "\n",
    "    :return: List of walks. Each walk is a list of nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    walks = list()\n",
    "\n",
    "    if not quiet:\n",
    "        pbar = tqdm(total=num_walks, desc='Generating walks (CPU: {})'.format(cpu_num))\n",
    "\n",
    "    for n_walk in range(num_walks):\n",
    "\n",
    "        # Update progress bar\n",
    "        if not quiet:\n",
    "            pbar.update(1)\n",
    "\n",
    "        # Shuffle the nodes\n",
    "        shuffled_nodes = list(d_graph.keys())\n",
    "        random.shuffle(shuffled_nodes)\n",
    "\n",
    "        # Start a random walk from every node\n",
    "        for source in shuffled_nodes:\n",
    "\n",
    "            # Skip nodes with specific num_walks\n",
    "            if source in sampling_strategy and \\\n",
    "                    num_walks_key in sampling_strategy[source] and \\\n",
    "                    sampling_strategy[source][num_walks_key] <= n_walk:\n",
    "                continue\n",
    "\n",
    "            # Start walk\n",
    "            walk = [source]\n",
    "\n",
    "            # Calculate walk length\n",
    "            if source in sampling_strategy:\n",
    "                walk_length = sampling_strategy[source].get(walk_length_key, global_walk_length)\n",
    "            else:\n",
    "                walk_length = global_walk_length\n",
    "\n",
    "            # Perform walk\n",
    "            while len(walk) < walk_length:\n",
    "\n",
    "                walk_options = d_graph[walk[-1]].get(neighbors_key, None)\n",
    "\n",
    "                # Skip dead end nodes\n",
    "                if not walk_options:\n",
    "                    break\n",
    "\n",
    "                if len(walk) == 1:  # For the first step\n",
    "                    probabilities = d_graph[walk[-1]][first_travel_key]\n",
    "                    walk_to = random.choices(walk_options, weights=probabilities)[0]\n",
    "                else:\n",
    "                    probabilities = d_graph[walk[-1]][probabilities_key][walk[-2]]\n",
    "                    walk_to = random.choices(walk_options, weights=probabilities)[0]\n",
    "\n",
    "                walk.append(walk_to)\n",
    "\n",
    "            walk = list(map(str, walk))  # Convert all to strings\n",
    "\n",
    "            walks.append(walk)\n",
    "\n",
    "    if not quiet:\n",
    "        pbar.close()\n",
    "\n",
    "    return walks\n",
    "\n",
    "\n",
    "class Node2Vec:\n",
    "    FIRST_TRAVEL_KEY = 'first_travel_key'\n",
    "    PROBABILITIES_KEY = 'probabilities'\n",
    "    NEIGHBORS_KEY = 'neighbors'\n",
    "    WEIGHT_KEY = 'weight'\n",
    "    NUM_WALKS_KEY = 'num_walks'\n",
    "    WALK_LENGTH_KEY = 'walk_length'\n",
    "    P_KEY = 'p'\n",
    "    Q_KEY = 'q'\n",
    "\n",
    "    def __init__(self, graph: nx.Graph, dimensions: int = 128, walk_length: int = 80, num_walks: int = 10, p: float = 1,\n",
    "                 q: float = 1, weight_key: str = 'weight', workers: int = 1, sampling_strategy: dict = None,\n",
    "                 quiet: bool = False, temp_folder: str = None, seed: int = None):\n",
    "        \"\"\"\n",
    "        Initiates the Node2Vec object, precomputes walking probabilities and generates the walks.\n",
    "\n",
    "        :param graph: Input graph\n",
    "        :param dimensions: Embedding dimensions (default: 128)\n",
    "        :param walk_length: Number of nodes in each walk (default: 80)\n",
    "        :param num_walks: Number of walks per node (default: 10)\n",
    "        :param p: Return hyper parameter (default: 1)\n",
    "        :param q: Inout parameter (default: 1)\n",
    "        :param weight_key: On weighted graphs, this is the key for the weight attribute (default: 'weight')\n",
    "        :param workers: Number of workers for parallel execution (default: 1)\n",
    "        :param sampling_strategy: Node specific sampling strategies, supports setting node specific 'q', 'p', 'num_walks' and 'walk_length'.\n",
    "        :param seed: Seed for the random number generator.\n",
    "        Use these keys exactly. If not set, will use the global ones which were passed on the object initialization\n",
    "        :param temp_folder: Path to folder with enough space to hold the memory map of self.d_graph (for big graphs); to be passed joblib.Parallel.temp_folder\n",
    "        \"\"\"\n",
    "\n",
    "        self.graph = graph\n",
    "        self.dimensions = dimensions\n",
    "        self.walk_length = walk_length\n",
    "        self.num_walks = num_walks\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.weight_key = weight_key\n",
    "        self.workers = workers\n",
    "        self.quiet = quiet\n",
    "        self.d_graph = defaultdict(dict)\n",
    "\n",
    "        if sampling_strategy is None:\n",
    "            self.sampling_strategy = {}\n",
    "        else:\n",
    "            self.sampling_strategy = sampling_strategy\n",
    "\n",
    "        self.temp_folder, self.require = None, None\n",
    "        if temp_folder:\n",
    "            if not os.path.isdir(temp_folder):\n",
    "                raise NotADirectoryError(\"temp_folder does not exist or is not a directory. ({})\".format(temp_folder))\n",
    "\n",
    "            self.temp_folder = temp_folder\n",
    "            self.require = \"sharedmem\"\n",
    "\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        self._precompute_probabilities()\n",
    "        self.walks = self._generate_walks()\n",
    "\n",
    "    def _precompute_probabilities(self):\n",
    "        \"\"\"\n",
    "        Precomputes transition probabilities for each node.\n",
    "        \"\"\"\n",
    "\n",
    "        d_graph = self.d_graph\n",
    "\n",
    "        nodes_generator = self.graph.nodes() if self.quiet \\\n",
    "            else tqdm(self.graph.nodes(), desc='Computing transition probabilities')\n",
    "\n",
    "        for source in nodes_generator:\n",
    "\n",
    "            # Init probabilities dict for first travel\n",
    "            if self.PROBABILITIES_KEY not in d_graph[source]:\n",
    "                d_graph[source][self.PROBABILITIES_KEY] = dict()\n",
    "\n",
    "            for current_node in self.graph.neighbors(source):\n",
    "\n",
    "                # Init probabilities dict\n",
    "                if self.PROBABILITIES_KEY not in d_graph[current_node]:\n",
    "                    d_graph[current_node][self.PROBABILITIES_KEY] = dict()\n",
    "\n",
    "                unnormalized_weights = list()\n",
    "                d_neighbors = list()\n",
    "\n",
    "                # Calculate unnormalized weights\n",
    "                for destination in self.graph.neighbors(current_node):\n",
    "\n",
    "                    p = self.sampling_strategy[current_node].get(self.P_KEY,\n",
    "                                                                 self.p) if current_node in self.sampling_strategy else self.p\n",
    "                    q = self.sampling_strategy[current_node].get(self.Q_KEY,\n",
    "                                                                 self.q) if current_node in self.sampling_strategy else self.q\n",
    "\n",
    "                    try:\n",
    "                        if self.graph[current_node][destination].get(self.weight_key):\n",
    "                            weight = self.graph[current_node][destination].get(self.weight_key, 1)\n",
    "                        else:\n",
    "                            ## Example : AtlasView({0: {'type': 1, 'weight':0.1}})- when we have edge weight\n",
    "                            edge = list(self.graph[current_node][destination])[-1]\n",
    "                            weight = self.graph[current_node][destination][edge].get(self.weight_key, 1)\n",
    "\n",
    "                    except:\n",
    "                        weight = 1\n",
    "\n",
    "                    if destination == source:  # Backwards probability\n",
    "                        ss_weight = weight * 1 / p\n",
    "                    elif destination in self.graph[source]:  # If the neighbor is connected to the source\n",
    "                        ss_weight = weight\n",
    "                    else:\n",
    "                        ss_weight = weight * 1 / q\n",
    "\n",
    "                    # Assign the unnormalized sampling strategy weight, normalize during random walk\n",
    "                    unnormalized_weights.append(ss_weight)\n",
    "                    d_neighbors.append(destination)\n",
    "\n",
    "                # Normalize\n",
    "                unnormalized_weights = np.array(unnormalized_weights)\n",
    "                d_graph[current_node][self.PROBABILITIES_KEY][\n",
    "                    source] = unnormalized_weights / unnormalized_weights.sum()\n",
    "\n",
    "            # Calculate first_travel weights for source\n",
    "            first_travel_weights = []\n",
    "\n",
    "            for destination in self.graph.neighbors(source):\n",
    "                first_travel_weights.append(self.graph[source][destination].get(self.weight_key, 1))\n",
    "\n",
    "            first_travel_weights = np.array(first_travel_weights)\n",
    "            d_graph[source][self.FIRST_TRAVEL_KEY] = first_travel_weights / first_travel_weights.sum()\n",
    "\n",
    "            # Save neighbors\n",
    "            d_graph[source][self.NEIGHBORS_KEY] = list(self.graph.neighbors(source))\n",
    "\n",
    "    def _generate_walks(self) -> list:\n",
    "        \"\"\"\n",
    "        Generates the random walks which will be used as the skip-gram input.\n",
    "        :return: List of walks. Each walk is a list of nodes.\n",
    "        \"\"\"\n",
    "\n",
    "        flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "        # Split num_walks for each worker\n",
    "        num_walks_lists = np.array_split(range(self.num_walks), self.workers)\n",
    "\n",
    "        walk_results = Parallel(n_jobs=self.workers, temp_folder=self.temp_folder, require=self.require)(\n",
    "            delayed(parallel_generate_walks)(self.d_graph,\n",
    "                                             self.walk_length,\n",
    "                                             len(num_walks),\n",
    "                                             idx,\n",
    "                                             self.sampling_strategy,\n",
    "                                             self.NUM_WALKS_KEY,\n",
    "                                             self.WALK_LENGTH_KEY,\n",
    "                                             self.NEIGHBORS_KEY,\n",
    "                                             self.PROBABILITIES_KEY,\n",
    "                                             self.FIRST_TRAVEL_KEY,\n",
    "                                             self.quiet) for\n",
    "            idx, num_walks\n",
    "            in enumerate(num_walks_lists, 1))\n",
    "\n",
    "        walks = flatten(walk_results)\n",
    "\n",
    "        return walks\n",
    "\n",
    "    def fit(self, **skip_gram_params) -> gensim.models.Word2Vec:\n",
    "        \"\"\"\n",
    "        Creates the embeddings using gensim's Word2Vec.\n",
    "        :param skip_gram_params: Parameters for gensim.models.Word2Vec - do not supply 'size' / 'vector_size' it is\n",
    "            taken from the Node2Vec 'dimensions' parameter\n",
    "        :type skip_gram_params: dict\n",
    "        :return: A gensim word2vec model\n",
    "        \"\"\"\n",
    "\n",
    "        if 'workers' not in skip_gram_params:\n",
    "            skip_gram_params['workers'] = self.workers\n",
    "\n",
    "        # Figure out gensim version, naming of output dimensions changed from size to vector_size in v4.0.0\n",
    "        gensim_version = pkg_resources.get_distribution(\"gensim\").version\n",
    "        size = 'size' if gensim_version < '4.0.0' else 'vector_size'\n",
    "        if size not in skip_gram_params:\n",
    "            skip_gram_params[size] = self.dimensions\n",
    "\n",
    "        if 'sg' not in skip_gram_params:\n",
    "            skip_gram_params['sg'] = 1\n",
    "\n",
    "        return gensim.models.Word2Vec(self.walks, **skip_gram_params)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 10:52:48,495 [INFO] Loading dataset\n",
      "2022-09-16 10:53:19,110 [INFO] Outputting to DGL\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "data collect\n",
    "\"\"\"\n",
    "twitter_bearer_token = 'AAAAAAAAAAAAAAAAAAAAAOYLagEAAAAA1K8YrEuA8CHQDAqAdjkPsBS2Pig%3DMUmnQgjpzkkXslyJpeNytAwFQ2qgiGE0Ah0rkrjuwH9UnOYSLI'\n",
    "dataset = MuminDataset(twitter_bearer_token=twitter_bearer_token,size='medium')\n",
    "dataset.compile()\n",
    "dataset.add_embeddings()\n",
    "if 'dgl_graph' not in globals():\n",
    "    dgl_graph = dataset.to_dgl()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "graph1\n",
    "\"\"\"\n",
    "rel = ('tweet', 'discusses', 'claim')\n",
    "g1 = dgl.edge_type_subgraph(dgl_graph, etypes=[rel]).to('cuda')\n",
    "num = g1.edges()[0].shape[0]\n",
    "g_src = g1.edges()[0].tolist()\n",
    "g_tgt = g1.edges()[1].tolist()\n",
    "src = []\n",
    "tgt = []\n",
    "for i in range(num):\n",
    "    for j in range(i,num):\n",
    "        if g_src[i] == g_src[j]:\n",
    "            src.append(g_tgt[i])\n",
    "            tgt.append(g_tgt[j])\n",
    "graph = nx.Graph()\n",
    "for i in range(len(src)):\n",
    "    graph.add_edge(src[i],tgt[i])\n",
    "\n",
    "tgt_node = 'claim'\n",
    "dim = g1.nodes[tgt_node].data['feat'].shape[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 5449/5449 [00:00<00:00, 7525.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# Precompute probabilities and generate walks\n",
    "\n",
    "node2vec = Node2Vec(graph, dimensions=dim, walk_length=20, num_walks=100, workers=4)\n",
    "\n",
    "## if d_graph is big enough to fit in the memory, pass temp_folder which has enough disk space\n",
    "# Note: It will trigger \"sharedmem\" in Parallel, which will be slow on smaller graphs\n",
    "#node2vec = Node2Vec(graph, dimensions=64, walk_length=30, num_walks=200, workers=4, temp_folder=\"/mnt/tmp_data\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 10:54:21,217 [INFO] collecting all words and their counts\n",
      "2022-09-16 10:54:21,217 [INFO] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-09-16 10:54:21,239 [INFO] PROGRESS: at sentence #10000, processed 200000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,260 [INFO] PROGRESS: at sentence #20000, processed 400000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,282 [INFO] PROGRESS: at sentence #30000, processed 600000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,303 [INFO] PROGRESS: at sentence #40000, processed 800000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,325 [INFO] PROGRESS: at sentence #50000, processed 1000000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,347 [INFO] PROGRESS: at sentence #60000, processed 1200000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,369 [INFO] PROGRESS: at sentence #70000, processed 1400000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,393 [INFO] PROGRESS: at sentence #80000, processed 1600000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,417 [INFO] PROGRESS: at sentence #90000, processed 1800000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,441 [INFO] PROGRESS: at sentence #100000, processed 2000000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,464 [INFO] PROGRESS: at sentence #110000, processed 2200000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,488 [INFO] PROGRESS: at sentence #120000, processed 2400000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,512 [INFO] PROGRESS: at sentence #130000, processed 2600000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,536 [INFO] PROGRESS: at sentence #140000, processed 2800000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,560 [INFO] PROGRESS: at sentence #150000, processed 3000000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,584 [INFO] PROGRESS: at sentence #160000, processed 3200000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,608 [INFO] PROGRESS: at sentence #170000, processed 3400000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,631 [INFO] PROGRESS: at sentence #180000, processed 3600000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,655 [INFO] PROGRESS: at sentence #190000, processed 3800000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,679 [INFO] PROGRESS: at sentence #200000, processed 4000000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,703 [INFO] PROGRESS: at sentence #210000, processed 4200000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,727 [INFO] PROGRESS: at sentence #220000, processed 4400000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,751 [INFO] PROGRESS: at sentence #230000, processed 4600000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,775 [INFO] PROGRESS: at sentence #240000, processed 4800000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,799 [INFO] PROGRESS: at sentence #250000, processed 5000000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,823 [INFO] PROGRESS: at sentence #260000, processed 5200000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,846 [INFO] PROGRESS: at sentence #270000, processed 5400000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,870 [INFO] PROGRESS: at sentence #280000, processed 5600000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,896 [INFO] PROGRESS: at sentence #290000, processed 5800000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,923 [INFO] PROGRESS: at sentence #300000, processed 6000000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,947 [INFO] PROGRESS: at sentence #310000, processed 6200000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,972 [INFO] PROGRESS: at sentence #320000, processed 6400000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:21,995 [INFO] PROGRESS: at sentence #330000, processed 6600000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:22,019 [INFO] PROGRESS: at sentence #340000, processed 6800000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:22,043 [INFO] PROGRESS: at sentence #350000, processed 7000000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:22,067 [INFO] PROGRESS: at sentence #360000, processed 7200000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:22,092 [INFO] PROGRESS: at sentence #370000, processed 7400000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:22,117 [INFO] PROGRESS: at sentence #380000, processed 7600000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:22,140 [INFO] PROGRESS: at sentence #390000, processed 7800000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:22,164 [INFO] PROGRESS: at sentence #400000, processed 8000000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:22,188 [INFO] PROGRESS: at sentence #410000, processed 8200000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:22,211 [INFO] PROGRESS: at sentence #420000, processed 8400000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:22,235 [INFO] PROGRESS: at sentence #430000, processed 8600000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:22,258 [INFO] PROGRESS: at sentence #440000, processed 8800000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:22,281 [INFO] PROGRESS: at sentence #450000, processed 9000000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:22,305 [INFO] PROGRESS: at sentence #460000, processed 9200000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:22,329 [INFO] PROGRESS: at sentence #470000, processed 9400000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:22,353 [INFO] PROGRESS: at sentence #480000, processed 9600000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:22,376 [INFO] PROGRESS: at sentence #490000, processed 9800000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:22,400 [INFO] PROGRESS: at sentence #500000, processed 10000000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:22,423 [INFO] PROGRESS: at sentence #510000, processed 10200000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:22,446 [INFO] PROGRESS: at sentence #520000, processed 10400000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:22,470 [INFO] PROGRESS: at sentence #530000, processed 10600000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:22,494 [INFO] PROGRESS: at sentence #540000, processed 10800000 words, keeping 5449 word types\n",
      "2022-09-16 10:54:22,506 [INFO] collected 5449 word types from a corpus of 10898000 raw words and 544900 sentences\n",
      "2022-09-16 10:54:22,507 [INFO] Creating a fresh vocabulary\n",
      "2022-09-16 10:54:22,521 [INFO] Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 5449 unique words (100.00% of original 5449, drops 0)', 'datetime': '2022-09-16T10:54:22.521878', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "2022-09-16 10:54:22,522 [INFO] Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 10898000 word corpus (100.00% of original 10898000, drops 0)', 'datetime': '2022-09-16T10:54:22.522877', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "2022-09-16 10:54:22,544 [INFO] deleting the raw counts dictionary of 5449 items\n",
      "2022-09-16 10:54:22,568 [INFO] sample=0.001 downsamples 0 most-common words\n",
      "2022-09-16 10:54:22,568 [INFO] Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 10898000 word corpus (100.0%% of prior 10898000)', 'datetime': '2022-09-16T10:54:22.568887', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "2022-09-16 10:54:22,606 [INFO] estimated required memory for 5449 words and 878 dimensions: 40998276 bytes\n",
      "2022-09-16 10:54:22,607 [INFO] resetting layer weights\n",
      "2022-09-16 10:54:22,629 [INFO] Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-09-16T10:54:22.629902', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'build_vocab'}\n",
      "2022-09-16 10:54:22,629 [INFO] Word2Vec lifecycle event {'msg': 'training model with 4 workers on 5449 vocabulary and 878 features, using sg=1 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2022-09-16T10:54:22.629902', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "2022-09-16 10:54:23,633 [INFO] EPOCH 0 - PROGRESS: at 0.77% examples, 83589 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:54:24,633 [INFO] EPOCH 0 - PROGRESS: at 1.54% examples, 84161 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:54:25,633 [INFO] EPOCH 0 - PROGRESS: at 2.37% examples, 86002 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:54:26,634 [INFO] EPOCH 0 - PROGRESS: at 3.17% examples, 86312 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:54:27,634 [INFO] EPOCH 0 - PROGRESS: at 4.04% examples, 88043 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:54:28,634 [INFO] EPOCH 0 - PROGRESS: at 4.94% examples, 89746 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:54:29,634 [INFO] EPOCH 0 - PROGRESS: at 5.90% examples, 91772 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:54:30,634 [INFO] EPOCH 0 - PROGRESS: at 6.86% examples, 93429 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:54:31,635 [INFO] EPOCH 0 - PROGRESS: at 7.85% examples, 95063 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:54:32,635 [INFO] EPOCH 0 - PROGRESS: at 8.90% examples, 96929 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:54:33,635 [INFO] EPOCH 0 - PROGRESS: at 9.98% examples, 98831 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:54:34,634 [INFO] EPOCH 0 - PROGRESS: at 11.08% examples, 100573 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:54:35,635 [INFO] EPOCH 0 - PROGRESS: at 12.19% examples, 102153 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:54:36,635 [INFO] EPOCH 0 - PROGRESS: at 13.29% examples, 103453 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:54:37,635 [INFO] EPOCH 0 - PROGRESS: at 14.44% examples, 104858 words/s, in_qsize 5, out_qsize 3\n",
      "2022-09-16 10:54:38,635 [INFO] EPOCH 0 - PROGRESS: at 15.59% examples, 106160 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:54:39,635 [INFO] EPOCH 0 - PROGRESS: at 16.74% examples, 107303 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:54:40,636 [INFO] EPOCH 0 - PROGRESS: at 17.88% examples, 108240 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:54:41,636 [INFO] EPOCH 0 - PROGRESS: at 19.03% examples, 109118 words/s, in_qsize 6, out_qsize 0\n",
      "2022-09-16 10:54:42,636 [INFO] EPOCH 0 - PROGRESS: at 20.18% examples, 109921 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:54:43,636 [INFO] EPOCH 0 - PROGRESS: at 21.31% examples, 110576 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:54:44,636 [INFO] EPOCH 0 - PROGRESS: at 22.46% examples, 111231 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:54:45,637 [INFO] EPOCH 0 - PROGRESS: at 23.62% examples, 111918 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-16 10:54:46,636 [INFO] EPOCH 0 - PROGRESS: at 24.80% examples, 112577 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:54:47,637 [INFO] EPOCH 0 - PROGRESS: at 25.97% examples, 113194 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:54:48,637 [INFO] EPOCH 0 - PROGRESS: at 27.12% examples, 113654 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:54:49,638 [INFO] EPOCH 0 - PROGRESS: at 28.25% examples, 113999 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:54:50,637 [INFO] EPOCH 0 - PROGRESS: at 29.42% examples, 114482 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:54:51,637 [INFO] EPOCH 0 - PROGRESS: at 30.63% examples, 115079 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:54:52,638 [INFO] EPOCH 0 - PROGRESS: at 31.84% examples, 115631 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:54:53,638 [INFO] EPOCH 0 - PROGRESS: at 33.03% examples, 116090 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:54:54,638 [INFO] EPOCH 0 - PROGRESS: at 34.23% examples, 116567 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:54:55,638 [INFO] EPOCH 0 - PROGRESS: at 35.43% examples, 117003 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:54:56,638 [INFO] EPOCH 0 - PROGRESS: at 36.65% examples, 117468 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:54:57,639 [INFO] EPOCH 0 - PROGRESS: at 37.84% examples, 117797 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:54:58,638 [INFO] EPOCH 0 - PROGRESS: at 39.04% examples, 118155 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:54:59,638 [INFO] EPOCH 0 - PROGRESS: at 40.24% examples, 118496 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:00,638 [INFO] EPOCH 0 - PROGRESS: at 41.42% examples, 118759 words/s, in_qsize 8, out_qsize 2\n",
      "2022-09-16 10:55:01,639 [INFO] EPOCH 0 - PROGRESS: at 42.62% examples, 119085 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:02,639 [INFO] EPOCH 0 - PROGRESS: at 43.84% examples, 119424 words/s, in_qsize 8, out_qsize 2\n",
      "2022-09-16 10:55:03,639 [INFO] EPOCH 0 - PROGRESS: at 45.07% examples, 119781 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:04,638 [INFO] EPOCH 0 - PROGRESS: at 46.29% examples, 120095 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:05,639 [INFO] EPOCH 0 - PROGRESS: at 47.52% examples, 120422 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:06,639 [INFO] EPOCH 0 - PROGRESS: at 48.73% examples, 120677 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:07,639 [INFO] EPOCH 0 - PROGRESS: at 49.91% examples, 120843 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:08,639 [INFO] EPOCH 0 - PROGRESS: at 51.10% examples, 121057 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:55:09,639 [INFO] EPOCH 0 - PROGRESS: at 52.33% examples, 121316 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:10,640 [INFO] EPOCH 0 - PROGRESS: at 53.48% examples, 121399 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:55:11,640 [INFO] EPOCH 0 - PROGRESS: at 54.68% examples, 121600 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:12,639 [INFO] EPOCH 0 - PROGRESS: at 55.91% examples, 121840 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:13,640 [INFO] EPOCH 0 - PROGRESS: at 57.15% examples, 122101 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:55:14,640 [INFO] EPOCH 0 - PROGRESS: at 58.36% examples, 122298 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:15,640 [INFO] EPOCH 0 - PROGRESS: at 59.57% examples, 122477 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-16 10:55:16,640 [INFO] EPOCH 0 - PROGRESS: at 60.81% examples, 122703 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:55:17,640 [INFO] EPOCH 0 - PROGRESS: at 62.05% examples, 122939 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:55:18,640 [INFO] EPOCH 0 - PROGRESS: at 63.30% examples, 123163 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-16 10:55:19,641 [INFO] EPOCH 0 - PROGRESS: at 64.53% examples, 123353 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:55:20,640 [INFO] EPOCH 0 - PROGRESS: at 65.74% examples, 123499 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:21,641 [INFO] EPOCH 0 - PROGRESS: at 66.97% examples, 123692 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:22,641 [INFO] EPOCH 0 - PROGRESS: at 68.21% examples, 123881 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:55:23,641 [INFO] EPOCH 0 - PROGRESS: at 69.45% examples, 124051 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:24,641 [INFO] EPOCH 0 - PROGRESS: at 70.68% examples, 124227 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:25,641 [INFO] EPOCH 0 - PROGRESS: at 71.93% examples, 124402 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:26,642 [INFO] EPOCH 0 - PROGRESS: at 73.12% examples, 124490 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:27,641 [INFO] EPOCH 0 - PROGRESS: at 74.28% examples, 124529 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:28,642 [INFO] EPOCH 0 - PROGRESS: at 75.19% examples, 124144 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-16 10:55:29,643 [INFO] EPOCH 0 - PROGRESS: at 75.80% examples, 123270 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:30,643 [INFO] EPOCH 0 - PROGRESS: at 76.58% examples, 122715 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:31,643 [INFO] EPOCH 0 - PROGRESS: at 77.68% examples, 122679 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:32,643 [INFO] EPOCH 0 - PROGRESS: at 78.86% examples, 122753 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:33,643 [INFO] EPOCH 0 - PROGRESS: at 80.06% examples, 122873 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:55:34,643 [INFO] EPOCH 0 - PROGRESS: at 81.29% examples, 123018 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:35,643 [INFO] EPOCH 0 - PROGRESS: at 82.53% examples, 123191 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:36,644 [INFO] EPOCH 0 - PROGRESS: at 83.77% examples, 123347 words/s, in_qsize 7, out_qsize 1\n",
      "2022-09-16 10:55:37,643 [INFO] EPOCH 0 - PROGRESS: at 84.98% examples, 123458 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:38,643 [INFO] EPOCH 0 - PROGRESS: at 86.08% examples, 123414 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:55:39,644 [INFO] EPOCH 0 - PROGRESS: at 86.69% examples, 122681 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:40,644 [INFO] EPOCH 0 - PROGRESS: at 87.52% examples, 122271 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:41,644 [INFO] EPOCH 0 - PROGRESS: at 88.61% examples, 122218 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:55:42,644 [INFO] EPOCH 0 - PROGRESS: at 89.80% examples, 122310 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:43,645 [INFO] EPOCH 0 - PROGRESS: at 90.99% examples, 122406 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:44,645 [INFO] EPOCH 0 - PROGRESS: at 92.22% examples, 122544 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:45,645 [INFO] EPOCH 0 - PROGRESS: at 93.46% examples, 122693 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:46,645 [INFO] EPOCH 0 - PROGRESS: at 94.66% examples, 122792 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:47,645 [INFO] EPOCH 0 - PROGRESS: at 95.90% examples, 122937 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:48,645 [INFO] EPOCH 0 - PROGRESS: at 97.11% examples, 123044 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:55:49,644 [INFO] EPOCH 0 - PROGRESS: at 98.35% examples, 123177 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:55:50,645 [INFO] EPOCH 0 - PROGRESS: at 99.58% examples, 123308 words/s, in_qsize 6, out_qsize 2\n",
      "2022-09-16 10:55:50,985 [INFO] EPOCH 0: training on 10898000 raw words (10898000 effective words) took 88.4s, 123347 effective words/s\n",
      "2022-09-16 10:55:51,989 [INFO] EPOCH 1 - PROGRESS: at 1.22% examples, 133209 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:52,989 [INFO] EPOCH 1 - PROGRESS: at 2.41% examples, 131127 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-16 10:55:53,989 [INFO] EPOCH 1 - PROGRESS: at 3.62% examples, 131418 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:54,989 [INFO] EPOCH 1 - PROGRESS: at 4.85% examples, 132252 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-16 10:55:55,989 [INFO] EPOCH 1 - PROGRESS: at 6.04% examples, 131524 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:56,989 [INFO] EPOCH 1 - PROGRESS: at 7.20% examples, 130813 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:57,990 [INFO] EPOCH 1 - PROGRESS: at 8.39% examples, 130663 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:58,990 [INFO] EPOCH 1 - PROGRESS: at 9.57% examples, 130347 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:55:59,989 [INFO] EPOCH 1 - PROGRESS: at 10.77% examples, 130390 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:00,990 [INFO] EPOCH 1 - PROGRESS: at 11.96% examples, 130313 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:01,990 [INFO] EPOCH 1 - PROGRESS: at 13.17% examples, 130496 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:56:02,990 [INFO] EPOCH 1 - PROGRESS: at 14.41% examples, 130884 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:03,990 [INFO] EPOCH 1 - PROGRESS: at 15.66% examples, 131230 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:04,990 [INFO] EPOCH 1 - PROGRESS: at 16.89% examples, 131493 words/s, in_qsize 8, out_qsize 2\n",
      "2022-09-16 10:56:05,991 [INFO] EPOCH 1 - PROGRESS: at 18.13% examples, 131739 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:06,991 [INFO] EPOCH 1 - PROGRESS: at 19.38% examples, 131999 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:56:07,991 [INFO] EPOCH 1 - PROGRESS: at 20.63% examples, 132208 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:08,991 [INFO] EPOCH 1 - PROGRESS: at 21.87% examples, 132385 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:09,991 [INFO] EPOCH 1 - PROGRESS: at 23.12% examples, 132584 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:10,991 [INFO] EPOCH 1 - PROGRESS: at 24.37% examples, 132759 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:11,991 [INFO] EPOCH 1 - PROGRESS: at 25.62% examples, 132917 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:12,992 [INFO] EPOCH 1 - PROGRESS: at 26.85% examples, 132991 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-16 10:56:13,992 [INFO] EPOCH 1 - PROGRESS: at 28.10% examples, 133118 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:14,992 [INFO] EPOCH 1 - PROGRESS: at 29.34% examples, 133210 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:56:15,992 [INFO] EPOCH 1 - PROGRESS: at 30.58% examples, 133292 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:16,992 [INFO] EPOCH 1 - PROGRESS: at 31.82% examples, 133360 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:17,993 [INFO] EPOCH 1 - PROGRESS: at 33.06% examples, 133439 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:18,992 [INFO] EPOCH 1 - PROGRESS: at 34.28% examples, 133422 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:19,993 [INFO] EPOCH 1 - PROGRESS: at 35.52% examples, 133482 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:56:20,992 [INFO] EPOCH 1 - PROGRESS: at 36.77% examples, 133557 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:56:21,992 [INFO] EPOCH 1 - PROGRESS: at 38.00% examples, 133557 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:56:22,992 [INFO] EPOCH 1 - PROGRESS: at 39.24% examples, 133605 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:23,992 [INFO] EPOCH 1 - PROGRESS: at 40.48% examples, 133656 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:24,992 [INFO] EPOCH 1 - PROGRESS: at 41.72% examples, 133710 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:25,993 [INFO] EPOCH 1 - PROGRESS: at 42.96% examples, 133750 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:26,993 [INFO] EPOCH 1 - PROGRESS: at 44.19% examples, 133760 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:27,993 [INFO] EPOCH 1 - PROGRESS: at 45.44% examples, 133816 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-16 10:56:28,993 [INFO] EPOCH 1 - PROGRESS: at 46.68% examples, 133866 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:29,994 [INFO] EPOCH 1 - PROGRESS: at 47.93% examples, 133921 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:30,993 [INFO] EPOCH 1 - PROGRESS: at 49.17% examples, 133954 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:31,994 [INFO] EPOCH 1 - PROGRESS: at 50.42% examples, 134005 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:32,993 [INFO] EPOCH 1 - PROGRESS: at 51.67% examples, 134058 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-16 10:56:33,993 [INFO] EPOCH 1 - PROGRESS: at 52.92% examples, 134110 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:34,993 [INFO] EPOCH 1 - PROGRESS: at 54.17% examples, 134156 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:56:35,993 [INFO] EPOCH 1 - PROGRESS: at 55.43% examples, 134217 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:36,994 [INFO] EPOCH 1 - PROGRESS: at 56.68% examples, 134269 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:56:37,994 [INFO] EPOCH 1 - PROGRESS: at 57.93% examples, 134313 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:38,995 [INFO] EPOCH 1 - PROGRESS: at 59.18% examples, 134338 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:56:39,994 [INFO] EPOCH 1 - PROGRESS: at 60.42% examples, 134367 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:40,994 [INFO] EPOCH 1 - PROGRESS: at 61.66% examples, 134387 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:56:41,994 [INFO] EPOCH 1 - PROGRESS: at 62.91% examples, 134410 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:42,994 [INFO] EPOCH 1 - PROGRESS: at 64.15% examples, 134425 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:43,995 [INFO] EPOCH 1 - PROGRESS: at 65.38% examples, 134430 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:44,994 [INFO] EPOCH 1 - PROGRESS: at 66.60% examples, 134394 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:45,995 [INFO] EPOCH 1 - PROGRESS: at 67.84% examples, 134402 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:46,995 [INFO] EPOCH 1 - PROGRESS: at 69.08% examples, 134410 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:56:47,995 [INFO] EPOCH 1 - PROGRESS: at 70.28% examples, 134363 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:48,995 [INFO] EPOCH 1 - PROGRESS: at 71.52% examples, 134374 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:49,995 [INFO] EPOCH 1 - PROGRESS: at 72.77% examples, 134397 words/s, in_qsize 7, out_qsize 1\n",
      "2022-09-16 10:56:50,995 [INFO] EPOCH 1 - PROGRESS: at 74.01% examples, 134420 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:51,995 [INFO] EPOCH 1 - PROGRESS: at 75.26% examples, 134447 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:52,996 [INFO] EPOCH 1 - PROGRESS: at 76.50% examples, 134461 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:53,996 [INFO] EPOCH 1 - PROGRESS: at 77.75% examples, 134478 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:54,995 [INFO] EPOCH 1 - PROGRESS: at 78.99% examples, 134497 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:55,995 [INFO] EPOCH 1 - PROGRESS: at 80.23% examples, 134508 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:56,995 [INFO] EPOCH 1 - PROGRESS: at 81.46% examples, 134499 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:56:57,996 [INFO] EPOCH 1 - PROGRESS: at 82.71% examples, 134514 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:58,996 [INFO] EPOCH 1 - PROGRESS: at 83.95% examples, 134531 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:56:59,996 [INFO] EPOCH 1 - PROGRESS: at 85.13% examples, 134444 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:57:00,996 [INFO] EPOCH 1 - PROGRESS: at 86.33% examples, 134396 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:01,996 [INFO] EPOCH 1 - PROGRESS: at 87.55% examples, 134373 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:02,996 [INFO] EPOCH 1 - PROGRESS: at 88.79% examples, 134382 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:03,996 [INFO] EPOCH 1 - PROGRESS: at 90.04% examples, 134407 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:04,997 [INFO] EPOCH 1 - PROGRESS: at 91.28% examples, 134408 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:05,996 [INFO] EPOCH 1 - PROGRESS: at 92.50% examples, 134397 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:06,997 [INFO] EPOCH 1 - PROGRESS: at 93.75% examples, 134419 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:07,997 [INFO] EPOCH 1 - PROGRESS: at 94.99% examples, 134432 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:08,997 [INFO] EPOCH 1 - PROGRESS: at 96.23% examples, 134442 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:09,997 [INFO] EPOCH 1 - PROGRESS: at 97.49% examples, 134471 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:10,997 [INFO] EPOCH 1 - PROGRESS: at 98.74% examples, 134490 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:57:11,997 [INFO] EPOCH 1 - PROGRESS: at 99.97% examples, 134495 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:12,018 [INFO] EPOCH 1: training on 10898000 raw words (10898000 effective words) took 81.0s, 134494 effective words/s\n",
      "2022-09-16 10:57:13,021 [INFO] EPOCH 2 - PROGRESS: at 1.23% examples, 133897 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:14,021 [INFO] EPOCH 2 - PROGRESS: at 2.47% examples, 134324 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:57:15,022 [INFO] EPOCH 2 - PROGRESS: at 3.66% examples, 133009 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:16,022 [INFO] EPOCH 2 - PROGRESS: at 4.87% examples, 132717 words/s, in_qsize 6, out_qsize 2\n",
      "2022-09-16 10:57:17,022 [INFO] EPOCH 2 - PROGRESS: at 6.07% examples, 132373 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-16 10:57:18,021 [INFO] EPOCH 2 - PROGRESS: at 7.29% examples, 132400 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:19,021 [INFO] EPOCH 2 - PROGRESS: at 8.49% examples, 132158 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:20,022 [INFO] EPOCH 2 - PROGRESS: at 9.69% examples, 131968 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:21,022 [INFO] EPOCH 2 - PROGRESS: at 10.88% examples, 131751 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:22,022 [INFO] EPOCH 2 - PROGRESS: at 12.10% examples, 131866 words/s, in_qsize 6, out_qsize 0\n",
      "2022-09-16 10:57:23,022 [INFO] EPOCH 2 - PROGRESS: at 13.34% examples, 132109 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:24,022 [INFO] EPOCH 2 - PROGRESS: at 14.54% examples, 132003 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:25,023 [INFO] EPOCH 2 - PROGRESS: at 15.76% examples, 132120 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:26,022 [INFO] EPOCH 2 - PROGRESS: at 16.95% examples, 131911 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:27,023 [INFO] EPOCH 2 - PROGRESS: at 18.17% examples, 131968 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:28,023 [INFO] EPOCH 2 - PROGRESS: at 19.39% examples, 132046 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:29,023 [INFO] EPOCH 2 - PROGRESS: at 20.58% examples, 131903 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:30,023 [INFO] EPOCH 2 - PROGRESS: at 21.82% examples, 132076 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:31,023 [INFO] EPOCH 2 - PROGRESS: at 23.04% examples, 132127 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:57:32,023 [INFO] EPOCH 2 - PROGRESS: at 24.27% examples, 132221 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:33,023 [INFO] EPOCH 2 - PROGRESS: at 25.50% examples, 132303 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:57:34,024 [INFO] EPOCH 2 - PROGRESS: at 26.72% examples, 132346 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:57:35,024 [INFO] EPOCH 2 - PROGRESS: at 27.95% examples, 132437 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:36,024 [INFO] EPOCH 2 - PROGRESS: at 29.18% examples, 132469 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:37,023 [INFO] EPOCH 2 - PROGRESS: at 30.37% examples, 132380 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:38,024 [INFO] EPOCH 2 - PROGRESS: at 31.60% examples, 132455 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:57:39,024 [INFO] EPOCH 2 - PROGRESS: at 32.83% examples, 132502 words/s, in_qsize 8, out_qsize 2\n",
      "2022-09-16 10:57:40,025 [INFO] EPOCH 2 - PROGRESS: at 34.06% examples, 132533 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:41,024 [INFO] EPOCH 2 - PROGRESS: at 35.29% examples, 132599 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:57:42,025 [INFO] EPOCH 2 - PROGRESS: at 36.53% examples, 132687 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:43,025 [INFO] EPOCH 2 - PROGRESS: at 37.77% examples, 132752 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:44,025 [INFO] EPOCH 2 - PROGRESS: at 38.99% examples, 132783 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:45,025 [INFO] EPOCH 2 - PROGRESS: at 40.20% examples, 132731 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:46,025 [INFO] EPOCH 2 - PROGRESS: at 41.43% examples, 132792 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:47,025 [INFO] EPOCH 2 - PROGRESS: at 42.67% examples, 132841 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:48,026 [INFO] EPOCH 2 - PROGRESS: at 43.90% examples, 132867 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:57:49,026 [INFO] EPOCH 2 - PROGRESS: at 45.13% examples, 132920 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:50,026 [INFO] EPOCH 2 - PROGRESS: at 46.37% examples, 132966 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:51,025 [INFO] EPOCH 2 - PROGRESS: at 47.59% examples, 132966 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:52,026 [INFO] EPOCH 2 - PROGRESS: at 48.82% examples, 133003 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:57:53,026 [INFO] EPOCH 2 - PROGRESS: at 50.04% examples, 132988 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:54,026 [INFO] EPOCH 2 - PROGRESS: at 51.27% examples, 133010 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:55,026 [INFO] EPOCH 2 - PROGRESS: at 52.48% examples, 132996 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:57:56,026 [INFO] EPOCH 2 - PROGRESS: at 53.66% examples, 132898 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:57,026 [INFO] EPOCH 2 - PROGRESS: at 54.84% examples, 132806 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:58,027 [INFO] EPOCH 2 - PROGRESS: at 55.72% examples, 131985 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:57:59,027 [INFO] EPOCH 2 - PROGRESS: at 56.36% examples, 130667 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:00,026 [INFO] EPOCH 2 - PROGRESS: at 57.26% examples, 129984 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:01,027 [INFO] EPOCH 2 - PROGRESS: at 58.44% examples, 129962 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-16 10:58:02,027 [INFO] EPOCH 2 - PROGRESS: at 59.67% examples, 130043 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:03,027 [INFO] EPOCH 2 - PROGRESS: at 60.90% examples, 130127 words/s, in_qsize 8, out_qsize 2\n",
      "2022-09-16 10:58:04,027 [INFO] EPOCH 2 - PROGRESS: at 62.08% examples, 130090 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:05,028 [INFO] EPOCH 2 - PROGRESS: at 63.31% examples, 130169 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:06,028 [INFO] EPOCH 2 - PROGRESS: at 64.54% examples, 130243 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-16 10:58:07,027 [INFO] EPOCH 2 - PROGRESS: at 65.77% examples, 130312 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:58:08,028 [INFO] EPOCH 2 - PROGRESS: at 67.01% examples, 130397 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:58:09,028 [INFO] EPOCH 2 - PROGRESS: at 68.24% examples, 130463 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:58:10,027 [INFO] EPOCH 2 - PROGRESS: at 69.48% examples, 130535 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:58:11,028 [INFO] EPOCH 2 - PROGRESS: at 70.69% examples, 130558 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:58:12,028 [INFO] EPOCH 2 - PROGRESS: at 71.93% examples, 130627 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:13,028 [INFO] EPOCH 2 - PROGRESS: at 73.16% examples, 130692 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:14,028 [INFO] EPOCH 2 - PROGRESS: at 74.39% examples, 130749 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:15,028 [INFO] EPOCH 2 - PROGRESS: at 75.63% examples, 130805 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:16,028 [INFO] EPOCH 2 - PROGRESS: at 76.86% examples, 130865 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:58:17,029 [INFO] EPOCH 2 - PROGRESS: at 78.08% examples, 130890 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-16 10:58:18,029 [INFO] EPOCH 2 - PROGRESS: at 79.31% examples, 130946 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:19,029 [INFO] EPOCH 2 - PROGRESS: at 80.55% examples, 131002 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:20,029 [INFO] EPOCH 2 - PROGRESS: at 81.79% examples, 131061 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-16 10:58:21,029 [INFO] EPOCH 2 - PROGRESS: at 83.03% examples, 131117 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:58:22,029 [INFO] EPOCH 2 - PROGRESS: at 84.24% examples, 131137 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:58:23,029 [INFO] EPOCH 2 - PROGRESS: at 85.46% examples, 131163 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:24,030 [INFO] EPOCH 2 - PROGRESS: at 86.70% examples, 131217 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:58:25,030 [INFO] EPOCH 2 - PROGRESS: at 87.94% examples, 131263 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:58:26,030 [INFO] EPOCH 2 - PROGRESS: at 89.18% examples, 131318 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:27,029 [INFO] EPOCH 2 - PROGRESS: at 90.42% examples, 131366 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:28,030 [INFO] EPOCH 2 - PROGRESS: at 91.66% examples, 131417 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:29,029 [INFO] EPOCH 2 - PROGRESS: at 92.89% examples, 131456 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:30,030 [INFO] EPOCH 2 - PROGRESS: at 94.12% examples, 131493 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:31,029 [INFO] EPOCH 2 - PROGRESS: at 95.35% examples, 131513 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:32,030 [INFO] EPOCH 2 - PROGRESS: at 96.58% examples, 131553 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:58:33,030 [INFO] EPOCH 2 - PROGRESS: at 97.82% examples, 131591 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:34,030 [INFO] EPOCH 2 - PROGRESS: at 99.03% examples, 131598 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:34,819 [INFO] EPOCH 2: training on 10898000 raw words (10898000 effective words) took 82.8s, 131621 effective words/s\n",
      "2022-09-16 10:58:35,822 [INFO] EPOCH 3 - PROGRESS: at 1.25% examples, 136592 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:36,821 [INFO] EPOCH 3 - PROGRESS: at 2.50% examples, 136394 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:37,822 [INFO] EPOCH 3 - PROGRESS: at 3.76% examples, 136612 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:38,822 [INFO] EPOCH 3 - PROGRESS: at 5.00% examples, 136277 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:39,822 [INFO] EPOCH 3 - PROGRESS: at 6.25% examples, 136141 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:40,822 [INFO] EPOCH 3 - PROGRESS: at 7.46% examples, 135500 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:41,822 [INFO] EPOCH 3 - PROGRESS: at 8.69% examples, 135346 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:42,822 [INFO] EPOCH 3 - PROGRESS: at 9.94% examples, 135429 words/s, in_qsize 8, out_qsize 2\n",
      "2022-09-16 10:58:43,823 [INFO] EPOCH 3 - PROGRESS: at 11.15% examples, 134959 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-16 10:58:44,823 [INFO] EPOCH 3 - PROGRESS: at 12.37% examples, 134801 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:58:45,823 [INFO] EPOCH 3 - PROGRESS: at 13.55% examples, 134230 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:46,823 [INFO] EPOCH 3 - PROGRESS: at 14.78% examples, 134256 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:47,824 [INFO] EPOCH 3 - PROGRESS: at 16.00% examples, 134151 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:58:48,824 [INFO] EPOCH 3 - PROGRESS: at 17.23% examples, 134133 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:49,824 [INFO] EPOCH 3 - PROGRESS: at 18.48% examples, 134214 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:50,824 [INFO] EPOCH 3 - PROGRESS: at 19.70% examples, 134146 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:51,825 [INFO] EPOCH 3 - PROGRESS: at 20.94% examples, 134213 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:58:52,824 [INFO] EPOCH 3 - PROGRESS: at 22.19% examples, 134308 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:53,824 [INFO] EPOCH 3 - PROGRESS: at 23.45% examples, 134470 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:54,825 [INFO] EPOCH 3 - PROGRESS: at 24.69% examples, 134537 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:58:55,824 [INFO] EPOCH 3 - PROGRESS: at 25.93% examples, 134552 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:56,825 [INFO] EPOCH 3 - PROGRESS: at 27.18% examples, 134605 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:57,825 [INFO] EPOCH 3 - PROGRESS: at 28.40% examples, 134537 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:58,826 [INFO] EPOCH 3 - PROGRESS: at 29.64% examples, 134572 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:58:59,825 [INFO] EPOCH 3 - PROGRESS: at 30.87% examples, 134544 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:00,825 [INFO] EPOCH 3 - PROGRESS: at 32.06% examples, 134347 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:01,826 [INFO] EPOCH 3 - PROGRESS: at 33.09% examples, 133558 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:59:02,826 [INFO] EPOCH 3 - PROGRESS: at 34.26% examples, 133324 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:03,826 [INFO] EPOCH 3 - PROGRESS: at 35.45% examples, 133208 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:59:04,826 [INFO] EPOCH 3 - PROGRESS: at 36.63% examples, 133058 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:59:05,826 [INFO] EPOCH 3 - PROGRESS: at 37.82% examples, 132931 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:06,826 [INFO] EPOCH 3 - PROGRESS: at 38.99% examples, 132763 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:07,827 [INFO] EPOCH 3 - PROGRESS: at 40.18% examples, 132684 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:08,826 [INFO] EPOCH 3 - PROGRESS: at 41.40% examples, 132684 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:09,827 [INFO] EPOCH 3 - PROGRESS: at 42.63% examples, 132708 words/s, in_qsize 8, out_qsize 2\n",
      "2022-09-16 10:59:10,827 [INFO] EPOCH 3 - PROGRESS: at 43.87% examples, 132790 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:11,827 [INFO] EPOCH 3 - PROGRESS: at 45.11% examples, 132838 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:12,827 [INFO] EPOCH 3 - PROGRESS: at 46.34% examples, 132892 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:13,828 [INFO] EPOCH 3 - PROGRESS: at 47.59% examples, 132963 words/s, in_qsize 8, out_qsize 2\n",
      "2022-09-16 10:59:14,828 [INFO] EPOCH 3 - PROGRESS: at 48.82% examples, 133005 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:15,827 [INFO] EPOCH 3 - PROGRESS: at 50.05% examples, 133025 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:59:16,827 [INFO] EPOCH 3 - PROGRESS: at 51.30% examples, 133084 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:59:17,827 [INFO] EPOCH 3 - PROGRESS: at 52.54% examples, 133147 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:18,828 [INFO] EPOCH 3 - PROGRESS: at 53.78% examples, 133181 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:59:19,828 [INFO] EPOCH 3 - PROGRESS: at 55.02% examples, 133222 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:20,828 [INFO] EPOCH 3 - PROGRESS: at 56.21% examples, 133139 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:59:21,828 [INFO] EPOCH 3 - PROGRESS: at 57.44% examples, 133179 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:59:22,828 [INFO] EPOCH 3 - PROGRESS: at 58.69% examples, 133236 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:23,828 [INFO] EPOCH 3 - PROGRESS: at 59.93% examples, 133269 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:24,828 [INFO] EPOCH 3 - PROGRESS: at 61.17% examples, 133317 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:59:25,829 [INFO] EPOCH 3 - PROGRESS: at 62.40% examples, 133326 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:26,829 [INFO] EPOCH 3 - PROGRESS: at 63.62% examples, 133311 words/s, in_qsize 8, out_qsize 2\n",
      "2022-09-16 10:59:27,829 [INFO] EPOCH 3 - PROGRESS: at 64.86% examples, 133358 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:28,829 [INFO] EPOCH 3 - PROGRESS: at 66.11% examples, 133401 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:29,830 [INFO] EPOCH 3 - PROGRESS: at 67.34% examples, 133409 words/s, in_qsize 6, out_qsize 2\n",
      "2022-09-16 10:59:30,829 [INFO] EPOCH 3 - PROGRESS: at 68.59% examples, 133458 words/s, in_qsize 8, out_qsize 2\n",
      "2022-09-16 10:59:31,829 [INFO] EPOCH 3 - PROGRESS: at 69.83% examples, 133488 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:32,830 [INFO] EPOCH 3 - PROGRESS: at 71.07% examples, 133522 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:33,829 [INFO] EPOCH 3 - PROGRESS: at 72.31% examples, 133550 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:34,830 [INFO] EPOCH 3 - PROGRESS: at 73.55% examples, 133567 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:35,829 [INFO] EPOCH 3 - PROGRESS: at 74.79% examples, 133598 words/s, in_qsize 5, out_qsize 0\n",
      "2022-09-16 10:59:36,830 [INFO] EPOCH 3 - PROGRESS: at 76.04% examples, 133633 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:37,830 [INFO] EPOCH 3 - PROGRESS: at 77.29% examples, 133674 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:38,830 [INFO] EPOCH 3 - PROGRESS: at 78.53% examples, 133697 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:39,830 [INFO] EPOCH 3 - PROGRESS: at 79.77% examples, 133728 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:40,831 [INFO] EPOCH 3 - PROGRESS: at 81.02% examples, 133767 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:41,830 [INFO] EPOCH 3 - PROGRESS: at 82.21% examples, 133704 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:59:42,831 [INFO] EPOCH 3 - PROGRESS: at 83.46% examples, 133732 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:43,830 [INFO] EPOCH 3 - PROGRESS: at 84.69% examples, 133737 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:59:44,831 [INFO] EPOCH 3 - PROGRESS: at 85.89% examples, 133696 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 10:59:45,831 [INFO] EPOCH 3 - PROGRESS: at 87.13% examples, 133722 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:46,831 [INFO] EPOCH 3 - PROGRESS: at 88.38% examples, 133751 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:47,831 [INFO] EPOCH 3 - PROGRESS: at 89.62% examples, 133768 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:48,831 [INFO] EPOCH 3 - PROGRESS: at 90.85% examples, 133782 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:49,831 [INFO] EPOCH 3 - PROGRESS: at 92.10% examples, 133804 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:59:50,831 [INFO] EPOCH 3 - PROGRESS: at 93.34% examples, 133820 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:59:51,831 [INFO] EPOCH 3 - PROGRESS: at 94.58% examples, 133842 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:52,831 [INFO] EPOCH 3 - PROGRESS: at 95.80% examples, 133836 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:53,832 [INFO] EPOCH 3 - PROGRESS: at 97.05% examples, 133857 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 10:59:54,832 [INFO] EPOCH 3 - PROGRESS: at 98.28% examples, 133866 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:55,832 [INFO] EPOCH 3 - PROGRESS: at 99.52% examples, 133881 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:56,220 [INFO] EPOCH 3: training on 10898000 raw words (10898000 effective words) took 81.4s, 133885 effective words/s\n",
      "2022-09-16 10:59:57,223 [INFO] EPOCH 4 - PROGRESS: at 1.24% examples, 134670 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:58,222 [INFO] EPOCH 4 - PROGRESS: at 2.47% examples, 134746 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 10:59:59,222 [INFO] EPOCH 4 - PROGRESS: at 3.71% examples, 134734 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-16 11:00:00,222 [INFO] EPOCH 4 - PROGRESS: at 4.94% examples, 134540 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:01,222 [INFO] EPOCH 4 - PROGRESS: at 6.15% examples, 134131 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:02,223 [INFO] EPOCH 4 - PROGRESS: at 7.33% examples, 133153 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:03,223 [INFO] EPOCH 4 - PROGRESS: at 8.55% examples, 133177 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:04,223 [INFO] EPOCH 4 - PROGRESS: at 9.78% examples, 133274 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:05,223 [INFO] EPOCH 4 - PROGRESS: at 11.00% examples, 133140 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:06,223 [INFO] EPOCH 4 - PROGRESS: at 12.23% examples, 133325 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 11:00:07,224 [INFO] EPOCH 4 - PROGRESS: at 13.47% examples, 133482 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 11:00:08,223 [INFO] EPOCH 4 - PROGRESS: at 14.70% examples, 133453 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:09,223 [INFO] EPOCH 4 - PROGRESS: at 15.91% examples, 133375 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:10,224 [INFO] EPOCH 4 - PROGRESS: at 17.14% examples, 133398 words/s, in_qsize 7, out_qsize 1\n",
      "2022-09-16 11:00:11,224 [INFO] EPOCH 4 - PROGRESS: at 18.38% examples, 133544 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 11:00:12,225 [INFO] EPOCH 4 - PROGRESS: at 19.63% examples, 133668 words/s, in_qsize 5, out_qsize 2\n",
      "2022-09-16 11:00:13,225 [INFO] EPOCH 4 - PROGRESS: at 20.87% examples, 133742 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:14,225 [INFO] EPOCH 4 - PROGRESS: at 22.11% examples, 133838 words/s, in_qsize 6, out_qsize 1\n",
      "2022-09-16 11:00:15,225 [INFO] EPOCH 4 - PROGRESS: at 23.35% examples, 133917 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:16,225 [INFO] EPOCH 4 - PROGRESS: at 24.59% examples, 134001 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:17,225 [INFO] EPOCH 4 - PROGRESS: at 25.83% examples, 134046 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:18,225 [INFO] EPOCH 4 - PROGRESS: at 27.08% examples, 134105 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:19,226 [INFO] EPOCH 4 - PROGRESS: at 28.32% examples, 134155 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:20,225 [INFO] EPOCH 4 - PROGRESS: at 29.53% examples, 134094 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:21,226 [INFO] EPOCH 4 - PROGRESS: at 30.78% examples, 134150 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:22,226 [INFO] EPOCH 4 - PROGRESS: at 32.02% examples, 134175 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:23,226 [INFO] EPOCH 4 - PROGRESS: at 33.26% examples, 134218 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 11:00:24,227 [INFO] EPOCH 4 - PROGRESS: at 34.50% examples, 134264 words/s, in_qsize 7, out_qsize 1\n",
      "2022-09-16 11:00:25,226 [INFO] EPOCH 4 - PROGRESS: at 35.74% examples, 134299 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 11:00:26,227 [INFO] EPOCH 4 - PROGRESS: at 36.99% examples, 134336 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:27,227 [INFO] EPOCH 4 - PROGRESS: at 38.23% examples, 134385 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:28,227 [INFO] EPOCH 4 - PROGRESS: at 39.47% examples, 134406 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:29,226 [INFO] EPOCH 4 - PROGRESS: at 40.71% examples, 134437 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:30,227 [INFO] EPOCH 4 - PROGRESS: at 41.96% examples, 134478 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:31,226 [INFO] EPOCH 4 - PROGRESS: at 43.21% examples, 134512 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:32,227 [INFO] EPOCH 4 - PROGRESS: at 44.44% examples, 134512 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 11:00:33,227 [INFO] EPOCH 4 - PROGRESS: at 45.68% examples, 134539 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:34,227 [INFO] EPOCH 4 - PROGRESS: at 46.93% examples, 134580 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:35,227 [INFO] EPOCH 4 - PROGRESS: at 48.15% examples, 134525 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:36,227 [INFO] EPOCH 4 - PROGRESS: at 49.36% examples, 134457 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:37,228 [INFO] EPOCH 4 - PROGRESS: at 50.60% examples, 134481 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:38,228 [INFO] EPOCH 4 - PROGRESS: at 51.84% examples, 134504 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:39,228 [INFO] EPOCH 4 - PROGRESS: at 53.09% examples, 134529 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:40,228 [INFO] EPOCH 4 - PROGRESS: at 54.33% examples, 134547 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:41,228 [INFO] EPOCH 4 - PROGRESS: at 55.56% examples, 134530 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:42,228 [INFO] EPOCH 4 - PROGRESS: at 56.80% examples, 134545 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:43,228 [INFO] EPOCH 4 - PROGRESS: at 58.04% examples, 134567 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 11:00:44,229 [INFO] EPOCH 4 - PROGRESS: at 59.27% examples, 134548 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:45,229 [INFO] EPOCH 4 - PROGRESS: at 60.50% examples, 134543 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:46,228 [INFO] EPOCH 4 - PROGRESS: at 61.75% examples, 134571 words/s, in_qsize 6, out_qsize 2\n",
      "2022-09-16 11:00:47,229 [INFO] EPOCH 4 - PROGRESS: at 63.00% examples, 134598 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:48,229 [INFO] EPOCH 4 - PROGRESS: at 64.24% examples, 134611 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:49,229 [INFO] EPOCH 4 - PROGRESS: at 65.49% examples, 134640 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 11:00:50,230 [INFO] EPOCH 4 - PROGRESS: at 66.73% examples, 134659 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 11:00:51,229 [INFO] EPOCH 4 - PROGRESS: at 67.98% examples, 134690 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 11:00:52,229 [INFO] EPOCH 4 - PROGRESS: at 69.23% examples, 134703 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:53,230 [INFO] EPOCH 4 - PROGRESS: at 70.47% examples, 134717 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:54,230 [INFO] EPOCH 4 - PROGRESS: at 71.72% examples, 134737 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 11:00:55,230 [INFO] EPOCH 4 - PROGRESS: at 72.96% examples, 134743 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 11:00:56,230 [INFO] EPOCH 4 - PROGRESS: at 74.20% examples, 134756 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:00:57,230 [INFO] EPOCH 4 - PROGRESS: at 75.44% examples, 134769 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 11:00:58,230 [INFO] EPOCH 4 - PROGRESS: at 76.69% examples, 134784 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 11:00:59,230 [INFO] EPOCH 4 - PROGRESS: at 77.92% examples, 134764 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:01:00,231 [INFO] EPOCH 4 - PROGRESS: at 79.13% examples, 134720 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:01:01,230 [INFO] EPOCH 4 - PROGRESS: at 80.37% examples, 134731 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:01:02,231 [INFO] EPOCH 4 - PROGRESS: at 81.60% examples, 134727 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 11:01:03,231 [INFO] EPOCH 4 - PROGRESS: at 82.85% examples, 134742 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:01:04,232 [INFO] EPOCH 4 - PROGRESS: at 84.09% examples, 134754 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 11:01:05,232 [INFO] EPOCH 4 - PROGRESS: at 85.34% examples, 134774 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:01:06,232 [INFO] EPOCH 4 - PROGRESS: at 86.59% examples, 134791 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:01:07,232 [INFO] EPOCH 4 - PROGRESS: at 87.84% examples, 134806 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:01:08,232 [INFO] EPOCH 4 - PROGRESS: at 89.08% examples, 134810 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 11:01:09,232 [INFO] EPOCH 4 - PROGRESS: at 90.32% examples, 134826 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:01:10,232 [INFO] EPOCH 4 - PROGRESS: at 91.57% examples, 134838 words/s, in_qsize 7, out_qsize 0\n",
      "2022-09-16 11:01:11,233 [INFO] EPOCH 4 - PROGRESS: at 92.81% examples, 134848 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 11:01:12,232 [INFO] EPOCH 4 - PROGRESS: at 94.01% examples, 134780 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 11:01:13,233 [INFO] EPOCH 4 - PROGRESS: at 95.20% examples, 134721 words/s, in_qsize 8, out_qsize 1\n",
      "2022-09-16 11:01:14,233 [INFO] EPOCH 4 - PROGRESS: at 96.38% examples, 134646 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:01:15,232 [INFO] EPOCH 4 - PROGRESS: at 97.63% examples, 134662 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:01:16,233 [INFO] EPOCH 4 - PROGRESS: at 98.80% examples, 134567 words/s, in_qsize 8, out_qsize 0\n",
      "2022-09-16 11:01:17,235 [INFO] EPOCH 4 - PROGRESS: at 99.94% examples, 134443 words/s, in_qsize 8, out_qsize 4\n",
      "2022-09-16 11:01:17,337 [INFO] EPOCH 4: training on 10898000 raw words (10898000 effective words) took 81.1s, 134353 effective words/s\n",
      "2022-09-16 11:01:17,338 [INFO] Word2Vec lifecycle event {'msg': 'training on 54490000 raw words (54490000 effective words) took 414.7s, 131394 effective words/s', 'datetime': '2022-09-16T11:01:17.338607', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "2022-09-16 11:01:17,338 [INFO] Word2Vec lifecycle event {'params': 'Word2Vec<vocab=5449, vector_size=878, alpha=0.025>', 'datetime': '2022-09-16T11:01:17.338607', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "# Embed\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)  # Any keywords acceptable by gensim.Word2Vec can be passed, `dimensions` and `workers` are automatically passed (from the Node2Vec constructor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "nodes = list(graph.nodes())\n",
    "emb = []\n",
    "for i in nodes:\n",
    "    emb.append(model.wv[str(i)])\n",
    "\n",
    "import pandas as pd\n",
    "dataframe = pd.DataFrame(nodes,columns=['node'])\n",
    "dataframe['feats'] = emb\n",
    "\n",
    "sub_g = g1\n",
    "feats = sub_g.nodes[tgt_node].data['feat'].cpu().detach().numpy()\n",
    "text_feats= []\n",
    "for i in range(feats.shape[0]):\n",
    "    text_feats.append(feats[i,:])\n",
    "\n",
    "label = sub_g.nodes[tgt_node].data['label'].cpu().detach().numpy()\n",
    "train_mask = sub_g.nodes[tgt_node].data['train_mask'].cpu().detach().numpy()\n",
    "val_mask = sub_g.nodes[tgt_node].data['val_mask'].cpu().detach().numpy()\n",
    "test_mask = sub_g.nodes[tgt_node].data['test_mask'].cpu().detach().numpy()\n",
    "\n",
    "import pandas as pd\n",
    "dataframe['label'] = label.tolist()\n",
    "dataframe['train_mask'] = train_mask.tolist()\n",
    "dataframe['val_mask'] = val_mask.tolist()\n",
    "dataframe['test_mask'] = test_mask.tolist()\n",
    "dataframe['text_feats'] = text_feats\n",
    "\n",
    "dc = nx.degree_centrality(graph)\n",
    "bc = nx.betweenness_centrality(graph)\n",
    "dc_list = []\n",
    "for key in sorted(dc.keys()):\n",
    "    dc_list.append(dc[key])\n",
    "\n",
    "bc_list = []\n",
    "for key in sorted(bc.keys()):\n",
    "    bc_list.append(bc[key])\n",
    "\n",
    "dc_feats = []\n",
    "for i in range(len(dataframe)):\n",
    "    dc_feats.append((dc_list[i]*dataframe['feats'][i]+(1-dc_list[i])*dataframe['text_feats'][i]))\n",
    "\n",
    "bc_feats = []\n",
    "for i in range(len(dataframe)):\n",
    "    bc_feats.append((bc_list[i]*dataframe['feats'][i]+(1-bc_list[i])*dataframe['text_feats'][i]))\n",
    "\n",
    "dataframe['dc_feats'] = dc_feats\n",
    "dataframe['bc_feats'] = bc_feats\n",
    "\n",
    "mix_feats = []\n",
    "for i in range(len(dataframe)):\n",
    "    mix_feats.append(np.hstack((dataframe['feats'][i],dataframe['text_feats'][i])))\n",
    "dataframe['mix_feats'] = mix_feats\n",
    "\n",
    "# Split up the data\n",
    "train = dataframe.query('train_mask == True')\n",
    "val = dataframe.query('val_mask == True')\n",
    "test = dataframe.query('test_mask == True')\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "val.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def getFeatsMatrix(df):\n",
    "    m_temp = np.zeros([len(df['feats'].values),dim])\n",
    "    for i in range(len(df['feats'].values)):\n",
    "        m_temp[i,:] = df['feats'][i]\n",
    "    return m_temp\n",
    "\n",
    "def getTextFeatsMatrix(df):\n",
    "    m_temp = np.zeros([len(df['text_feats'].values),dim])\n",
    "    for i in range(len(df['text_feats'].values)):\n",
    "        m_temp[i,:] = df['text_feats'][i]\n",
    "    return m_temp\n",
    "\n",
    "def getMixFeatsMatrix(df):\n",
    "    m_temp = np.zeros([len(df['mix_feats'].values),2*dim])\n",
    "    for i in range(len(df['mix_feats'].values)):\n",
    "        m_temp[i,:] = df['mix_feats'][i]\n",
    "    return m_temp\n",
    "\n",
    "def getDcFeatsMatrix(df):\n",
    "    m_temp = np.zeros([len(df['dc_feats'].values),dim])\n",
    "    for i in range(len(df['dc_feats'].values)):\n",
    "        m_temp[i,:] = df['dc_feats'][i]\n",
    "    return m_temp\n",
    "\n",
    "def getBcFeatsMatrix(df):\n",
    "    m_temp = np.zeros([len(df['bc_feats'].values),dim])\n",
    "    for i in range(len(df['bc_feats'].values)):\n",
    "        m_temp[i,:] = df['bc_feats'][i]\n",
    "    return m_temp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "def convert_to_data_loader(dataset,m, num_classes):\n",
    "    # convert from list to tensor\n",
    "    input_tensor = torch.from_numpy(m).float()\n",
    "    label_tensor = torch.from_numpy(np.array(dataset['label'])).long()\n",
    "    tensor_dataset = TensorDataset(input_tensor, label_tensor)\n",
    "    loader = DataLoader(tensor_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    return loader\n",
    "\n",
    "num_classes = 2   # number of possible labels in the sentiment analysis task\n",
    "\n",
    "\n",
    "# train_loader = convert_to_data_loader(train,getMixFeatsMatrix(train), num_classes)\n",
    "# dev_loader = convert_to_data_loader(val, getMixFeatsMatrix(val),num_classes)\n",
    "# test_loader = convert_to_data_loader(test,getMixFeatsMatrix(test), num_classes)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 2)\n",
    "\n",
    "\n",
    "        # 构造Dropout方法，在每次训练过程中都随机“掐死”百分之二十的神经元，防止过拟合。\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 确保输入的tensor是展开的单列数据，把每张图片的通道、长度、宽度三个维度都压缩为一列\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # 在训练过程中对隐含层神经元的正向推断使用Dropout方法\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "\n",
    "        # 在输出单元不需要使用Dropout方法\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def train_nn():\n",
    "    # 对上面定义的Classifier类进行实例化\n",
    "    model = Classifier()\n",
    "\n",
    "    # 定义损失函数为负对数损失函数\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    # 优化方法为Adam梯度下降方法，学习率为0.003\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "    # 对训练集的全部数据学习15遍，这个数字越大，训练时间越长\n",
    "    epochs = 20\n",
    "\n",
    "    # 将每次训练的训练误差和测试误差存储在这两个列表里，后面绘制误差变化折线图用\n",
    "    train_losses, test_losses = [], []\n",
    "\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "\n",
    "        # 对训练集中的所有图片都过一遍\n",
    "        for images, labels in train_loader:\n",
    "            # 将优化器中的求导结果都设为0，否则会在每次反向传播之后叠加之前的\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 对64张图片进行推断，计算损失函数，反向传播优化权重，将损失求和\n",
    "            log_ps = model(images)\n",
    "            loss = criterion(log_ps, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # 每次学完一遍数据集，都进行以下测试操作\n",
    "        else:\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            # 测试的时候不需要开自动求导和反向传播\n",
    "            with torch.no_grad():\n",
    "                # 关闭Dropout\n",
    "                model.eval()\n",
    "\n",
    "                # 对测试集中的所有图片都过一遍\n",
    "                for images, labels in dev_loader:\n",
    "                    # 对传入的测试集图片进行正向推断、计算损失，accuracy为测试集一万张图片中模型预测正确率\n",
    "                    log_ps = model(images)\n",
    "                    test_loss += criterion(log_ps, labels)\n",
    "                    ps = torch.exp(log_ps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "\n",
    "                    # 等号右边为每一批64张测试图片中预测正确的占比\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "            # 恢复Dropout\n",
    "            model.train()\n",
    "            # 将训练误差和测试误差存在两个列表里，后面绘制误差变化折线图用\n",
    "            train_losses.append(running_loss/len(train_loader))\n",
    "            test_losses.append(test_loss/len(dev_loader))\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def predict_nn(trained_model, test_loader):\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0  # count the number of correct classification labels\n",
    "\n",
    "    gold_labs = []  # gold labels to return\n",
    "    pred_labs = []  # predicted labels to return\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "        test_output = trained_model(inputs)\n",
    "        predicted_labels = test_output.argmax(1)\n",
    "\n",
    "        gold_labs.extend(labels.tolist())\n",
    "        pred_labs.extend(predicted_labels.tolist())\n",
    "\n",
    "\n",
    "    f1 = f1_score(gold_labs, pred_labs, average='macro')\n",
    "\n",
    "    return f1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (macro average) = 0.6498881431767338\n"
     ]
    }
   ],
   "source": [
    "train_loader = convert_to_data_loader(train,getBcFeatsMatrix(train), num_classes)\n",
    "dev_loader = convert_to_data_loader(val, getBcFeatsMatrix(val),num_classes)\n",
    "test_loader = convert_to_data_loader(test,getBcFeatsMatrix(test), num_classes)\n",
    "\n",
    "\n",
    "model = train_nn()\n",
    "score=predict_nn(model,test_loader)\n",
    "\n",
    "print(f'F1 score (macro average) = {score}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (macro average) = 0.674579795529371\n"
     ]
    }
   ],
   "source": [
    "train_loader = convert_to_data_loader(train,getDcFeatsMatrix(train), num_classes)\n",
    "dev_loader = convert_to_data_loader(val, getDcFeatsMatrix(val),num_classes)\n",
    "test_loader = convert_to_data_loader(test,getDcFeatsMatrix(test), num_classes)\n",
    "\n",
    "model = train_nn()\n",
    "score=predict_nn(model,test_loader)\n",
    "\n",
    "print(f'F1 score (macro average) = {score}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (macro average) = 0.674579795529371\n"
     ]
    }
   ],
   "source": [
    "train_loader = convert_to_data_loader(train,getFeatsMatrix(train), num_classes)\n",
    "dev_loader = convert_to_data_loader(val, getFeatsMatrix(val),num_classes)\n",
    "test_loader = convert_to_data_loader(test,getFeatsMatrix(test), num_classes)\n",
    "\n",
    "model = train_nn()\n",
    "score=predict_nn(model,test_loader)\n",
    "\n",
    "print(f'F1 score (macro average) = {score}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (macro average) = 0.674579795529371\n"
     ]
    }
   ],
   "source": [
    "train_loader = convert_to_data_loader(train,getMixFeatsMatrix(train), num_classes)\n",
    "dev_loader = convert_to_data_loader(val, getMixFeatsMatrix(val),num_classes)\n",
    "test_loader = convert_to_data_loader(test,getMixFeatsMatrix(test), num_classes)\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2*dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 2)\n",
    "\n",
    "\n",
    "        # 构造Dropout方法，在每次训练过程中都随机“掐死”百分之二十的神经元，防止过拟合。\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 确保输入的tensor是展开的单列数据，把每张图片的通道、长度、宽度三个维度都压缩为一列\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # 在训练过程中对隐含层神经元的正向推断使用Dropout方法\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "\n",
    "        # 在输出单元不需要使用Dropout方法\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = train_nn()\n",
    "score=predict_nn(model,test_loader)\n",
    "\n",
    "print(f'F1 score (macro average) = {score}')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}